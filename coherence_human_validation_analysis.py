#!/usr/bin/env python3
"""
Coherenceç»´åº¦Human Validationåˆ†æ
ç›®æ ‡ï¼šå›åº”ç¡•å£«è®ºæ–‡å¯¼å¸ˆçš„å­¦æœ¯è¦æ±‚
- è®¡ç®—auto-human coherence correlation
- é‡æ–°è®¾å®šåŸºäºempirical dataçš„é˜ˆå€¼
- åˆ†æpractical significance
- éªŒè¯method validity
"""

import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import roc_curve, auc
from scipy.stats import spearmanr, pearsonr
import warnings
warnings.filterwarnings('ignore')

class CoherenceHumanValidationAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.human_data = None
        self.auto_data = None
        self.merged_data = None
        self.results = {}
        
    def load_human_data(self, csv_path="/Users/haha/Story/Interview.csv"):
        """åŠ è½½äººå·¥è¯„ä»·æ•°æ®"""
        print("ğŸ“Š åŠ è½½äººå·¥è¯„ä»·æ•°æ®...")
        df = pd.read_csv(csv_path)
        
        # æå–coherenceè¯„åˆ† (æ¯ä¸ªæ•…äº‹4åˆ—coherenceæ•°æ®)
        coherence_data = []
        
        for index, row in df.iterrows():
            participant_id = f"P{index+1:02d}"
            group_id = row['Group_id']
            
            # è·å–4ä¸ªæ•…äº‹çš„é…ç½®ä¿¡æ¯
            stories = [row['Story 1'], row['Story 2'], row['Story 3'], row['Story 4']]
            
            # è·å–coherenceè¯„åˆ† (æ³¨æ„åˆ—åå¯èƒ½æœ‰åç¼€)
            coherence_cols = [
                'Coherence How coherent and logical is the plot development of this story?',
                'Coherence How coherent and logical is the plot development of this story?.1', 
                'Coherence How coherent and logical is the plot development of this story?.2',
                'Coherence How coherent and logical is the plot development of this story?.3'
            ]
            
            for i, (story_config, coherence_col) in enumerate(zip(stories, coherence_cols)):
                if coherence_col in row and pd.notna(row[coherence_col]):
                    coherence_data.append({
                        'participant_id': participant_id,
                        'group_id': group_id,
                        'story_position': i + 1,
                        'story_config': story_config,
                        'human_coherence': row[coherence_col],
                        'is_baseline': story_config == 'Sci baseline'
                    })
        
        self.human_data = pd.DataFrame(coherence_data)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.human_data)} æ¡äººå·¥coherenceè¯„åˆ†")
        print(f"   - åŸºçº¿æ•…äº‹è¯„åˆ†: {sum(self.human_data['is_baseline'])} æ¡")
        print(f"   - å®éªŒæ•…äº‹è¯„åˆ†: {sum(~self.human_data['is_baseline'])} æ¡")
        
        return self.human_data
    
    def load_auto_coherence_data(self):
        """åŠ è½½è‡ªåŠ¨coherenceæ•°æ®"""
        print("\nğŸ¤– åŠ è½½è‡ªåŠ¨coherenceåˆ†æç»“æœ...")
        
        auto_data = []
        
        # 1. åŠ è½½baselineæ•°æ®
        baseline_files = [
            ("/Users/haha/Story/baseline_analysis_results/baseline_s1/hred_coherence_analysis.json", "simple_baseline_s1"),
            ("/Users/haha/Story/baseline_analysis_results/baseline_s2/hred_coherence_analysis.json", "simple_baseline_s2"), 
            ("/Users/haha/Story/baseline_analysis_results/baseline_s3/hred_coherence_analysis.json", "simple_baseline_s3")
        ]
        
        for file_path, config_name in baseline_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    auto_coherence = data['HRED_coherence_evaluation']['average_coherence']
                    total_sentences = data['HRED_coherence_evaluation']['total_sentences']
                    coherence_std = data['detailed_analysis']['basic_statistics']['coherence_std']
                    
                    auto_data.append({
                        'story_config': config_name,
                        'auto_coherence': auto_coherence,
                        'total_sentences': total_sentences,
                        'coherence_std': coherence_std,
                        'is_baseline': True
                    })
            except Exception as e:
                print(f"   è­¦å‘Šï¼šæ— æ³•åŠ è½½ {file_path}: {e}")
        
        # 2. ä»metrics_master_clean.csvåŠ è½½å®éªŒæ•°æ®
        try:
            metrics_df = pd.read_csv("/Users/haha/Story/metrics_master_clean.csv")
            
            for _, row in metrics_df.iterrows():
                if pd.notna(row.get('avg_coherence')):
                    # æ„å»ºstory_configåç§°
                    structure = row.get('structure', '')
                    temperature = row.get('temperature', '')
                    seed = row.get('seed', '')
                    
                    if structure and temperature and seed:
                        story_config = f"{structure}_T{temperature}_s{seed}"
                        
                        auto_data.append({
                            'story_config': story_config,
                            'auto_coherence': row['avg_coherence'],
                            'total_sentences': row.get('total_sentences', None),
                            'coherence_std': None,  # metricsæ–‡ä»¶ä¸­æ²¡æœ‰stdæ•°æ®
                            'is_baseline': False
                        })
        except Exception as e:
            print(f"   è­¦å‘Šï¼šæ— æ³•åŠ è½½metricsæ–‡ä»¶: {e}")
        
        self.auto_data = pd.DataFrame(auto_data)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(self.auto_data)} æ¡è‡ªåŠ¨coherenceåˆ†æç»“æœ")
        
        return self.auto_data
    
    def merge_human_auto_data(self):
        """åˆå¹¶äººå·¥å’Œè‡ªåŠ¨è¯„ä»·æ•°æ®"""
        print("\nğŸ”— åˆå¹¶äººå·¥å’Œè‡ªåŠ¨è¯„ä»·æ•°æ®...")
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print("\nè°ƒè¯•ä¿¡æ¯:")
        print("Human story configs:", self.human_data['story_config'].unique())
        print("Auto story configs:", self.auto_data['story_config'].unique())
        
        # æ›´å¤æ‚çš„mappingç­–ç•¥
        merged_records = []
        
        for _, human_row in self.human_data.iterrows():
            human_config = human_row['story_config']
            
            # 1. å¤„ç†baselineåŒ¹é…
            if human_config == 'Sci baseline':
                # æ‰¾åˆ°ä»»æ„ä¸€ä¸ªbaseline autoç»“æœï¼ˆå› ä¸ºå®ƒä»¬æœ¬è´¨ç›¸åŒï¼‰
                baseline_auto = self.auto_data[self.auto_data['is_baseline']].iloc[0] if len(self.auto_data[self.auto_data['is_baseline']]) > 0 else None
                if baseline_auto is not None:
                    merged_record = human_row.to_dict()
                    merged_record.update({
                        'auto_coherence': baseline_auto['auto_coherence'],
                        'total_sentences': baseline_auto['total_sentences'],
                        'coherence_std': baseline_auto['coherence_std'],
                        'is_baseline_auto': baseline_auto['is_baseline']
                    })
                    merged_records.append(merged_record)
            
            # 2. å¤„ç†å®éªŒé…ç½®åŒ¹é…
            else:
                # ç›´æ¥æŸ¥æ‰¾åŒ¹é…çš„é…ç½®
                matching_auto = self.auto_data[self.auto_data['story_config'] == human_config]
                if len(matching_auto) > 0:
                    auto_row = matching_auto.iloc[0]
                    merged_record = human_row.to_dict()
                    merged_record.update({
                        'auto_coherence': auto_row['auto_coherence'],
                        'total_sentences': auto_row['total_sentences'], 
                        'coherence_std': auto_row['coherence_std'],
                        'is_baseline_auto': auto_row['is_baseline']
                    })
                    merged_records.append(merged_record)
        
        self.merged_data = pd.DataFrame(merged_records)
        print(f"âœ… æˆåŠŸåˆå¹¶ {len(self.merged_data)} æ¡matchedè®°å½•")
        
        if len(self.merged_data) > 0:
            print(f"   - Baselineè®°å½•: {sum(self.merged_data['is_baseline'])} æ¡")  
            print(f"   - å®éªŒè®°å½•: {sum(~self.merged_data['is_baseline'])} æ¡")
            print(f"   - Auto coherenceèŒƒå›´: {self.merged_data['auto_coherence'].min():.3f} - {self.merged_data['auto_coherence'].max():.3f}")
        else:
            print("âŒ è­¦å‘Šï¼šæ²¡æœ‰åŒ¹é…çš„è®°å½•ï¼")
        
        return self.merged_data
    
    def calculate_correlation_analysis(self):
        """è®¡ç®—correlationåˆ†æ - æ ¸å¿ƒå­¦æœ¯éªŒè¯"""
        print("\nğŸ“ˆ è®¡ç®—Human-Auto Coherence Correlation...")
        
        if len(self.merged_data) == 0:
            print("âŒ æ— æ³•è¿›è¡Œcorrelationåˆ†æï¼šæ²¡æœ‰åŒ¹é…æ•°æ®")
            return None
        
        human_scores = self.merged_data['human_coherence']
        auto_scores = self.merged_data['auto_coherence']
        
        # æ£€æŸ¥auto scoresæ˜¯å¦ä¸ºconstant
        if auto_scores.std() == 0:
            print("âš ï¸ è­¦å‘Šï¼šæ‰€æœ‰auto coherenceåˆ†æ•°ç›¸åŒï¼Œæ— æ³•è®¡ç®—correlation")
            pearson_r, pearson_p = np.nan, np.nan
            spearman_r, spearman_p = np.nan, np.nan
        else:
            # 1. Pearson correlation
            pearson_r, pearson_p = pearsonr(human_scores, auto_scores)
            
            # 2. Spearman correlation (æ›´robust)
            spearman_r, spearman_p = spearmanr(human_scores, auto_scores)
        
        # 3. è®¡ç®—ä¸åŒå­ç»„çš„correlation
        baseline_mask = self.merged_data['is_baseline']
        
        if sum(baseline_mask) > 2:
            baseline_pearson_r, baseline_pearson_p = pearsonr(
                human_scores[baseline_mask], auto_scores[baseline_mask]
            )
        else:
            baseline_pearson_r, baseline_pearson_p = np.nan, np.nan
        
        if sum(~baseline_mask) > 2:
            exp_pearson_r, exp_pearson_p = pearsonr(
                human_scores[~baseline_mask], auto_scores[~baseline_mask]
            )
        else:
            exp_pearson_r, exp_pearson_p = np.nan, np.nan
        
        correlation_results = {
            'overall_correlation': {
                'pearson_r': pearson_r,
                'pearson_p': pearson_p,
                'spearman_r': spearman_r,
                'spearman_p': spearman_p,
                'sample_size': len(human_scores),
                'significant': pearson_p < 0.05,
                'strong_correlation': abs(pearson_r) > 0.5
            },
            'baseline_correlation': {
                'pearson_r': baseline_pearson_r,
                'pearson_p': baseline_pearson_p,
                'sample_size': sum(baseline_mask)
            },
            'experimental_correlation': {
                'pearson_r': exp_pearson_r,
                'pearson_p': exp_pearson_p,
                'sample_size': sum(~baseline_mask)
            }
        }
        
        self.results['correlation_analysis'] = correlation_results
        
        # æ‰“å°ç»“æœ
        print(f"ğŸ“Š Correlation Analysis Results:")
        print(f"   Overall Pearson r = {pearson_r:.3f} (p = {pearson_p:.3f})")
        print(f"   Overall Spearman r = {spearman_r:.3f} (p = {spearman_p:.3f})")
        print(f"   Sample size: {len(human_scores)}")
        
        # å­¦æœ¯åˆ¤æ–­
        if pearson_p < 0.05:
            if abs(pearson_r) > 0.7:
                print("   âœ… å¼ºæ˜¾è‘—ç›¸å…³ (å­¦æœ¯æ ‡å‡†: EXCELLENT)")
            elif abs(pearson_r) > 0.5:
                print("   âœ… ä¸­ç­‰æ˜¾è‘—ç›¸å…³ (å­¦æœ¯æ ‡å‡†: ACCEPTABLE)")
            elif abs(pearson_r) > 0.3:
                print("   âš ï¸ å¼±æ˜¾è‘—ç›¸å…³ (å­¦æœ¯æ ‡å‡†: MARGINAL)")
            else:
                print("   âŒ æ˜¾è‘—ä½†ç›¸å…³æ€§å¾ˆå¼± (å­¦æœ¯æ ‡å‡†: INSUFFICIENT)")
        else:
            print("   âŒ æ— æ˜¾è‘—ç›¸å…³ (å­¦æœ¯æ ‡å‡†: FAILED)")
        
        return correlation_results
    
    def empirical_threshold_setting(self):
        """åŸºäºempirical dataé‡æ–°è®¾å®šé˜ˆå€¼"""
        print("\nğŸ¯ åŸºäºHuman Dataè®¾å®šEmpirical Thresholds...")
        
        if len(self.merged_data) == 0:
            return None
        
        human_scores = self.merged_data['human_coherence']
        auto_scores = self.merged_data['auto_coherence']
        
        # 1. åŸºäºhuman score quartiles
        human_quartiles = np.percentile(human_scores, [25, 50, 75])
        
        # 2. åŸºäºhuman scoreå¯¹åº”çš„auto scoreåˆ†å¸ƒ
        high_human = auto_scores[human_scores >= human_quartiles[2]]  # top 25%
        medium_human = auto_scores[(human_scores >= human_quartiles[0]) & (human_scores < human_quartiles[2])]
        low_human = auto_scores[human_scores < human_quartiles[0]]  # bottom 25%
        
        # 3. è®¾å®šnew thresholds
        if len(high_human) > 0 and len(low_human) > 0:
            excellent_threshold = np.percentile(high_human, 25)  # é«˜äººå·¥è¯„åˆ†ç»„çš„25%åˆ†ä½æ•°
            poor_threshold = np.percentile(low_human, 75)        # ä½äººå·¥è¯„åˆ†ç»„çš„75%åˆ†ä½æ•°
            good_threshold = (excellent_threshold + poor_threshold) / 2
        else:
            # fallbackåˆ°ç»Ÿè®¡å­¦æ–¹æ³•
            excellent_threshold = np.percentile(auto_scores, 75)
            good_threshold = np.percentile(auto_scores, 50)  
            poor_threshold = np.percentile(auto_scores, 25)
        
        threshold_results = {
            'empirical_thresholds': {
                'excellent': excellent_threshold,
                'good': good_threshold,
                'poor': poor_threshold
            },
            'old_thresholds': {
                'excellent': 0.7,
                'good': 0.5,
                'poor': 0.3
            },
            'human_quartiles': {
                'Q1': human_quartiles[0],
                'Q2': human_quartiles[1], 
                'Q3': human_quartiles[2]
            },
            'threshold_method': 'human_score_based_quartiles'
        }
        
        self.results['threshold_analysis'] = threshold_results
        
        print(f"ğŸ“Š Empirical Thresholds (vs Old Arbitrary):")
        print(f"   Excellent: {excellent_threshold:.3f} (was: 0.700)")
        print(f"   Good:      {good_threshold:.3f} (was: 0.500)")
        print(f"   Poor:      {poor_threshold:.3f} (was: 0.300)")
        
        return threshold_results
    
    def practical_significance_analysis(self):
        """åˆ†æpractical significance"""
        print("\nğŸ’¡ Practical Significance Analysis...")
        
        if len(self.merged_data) == 0:
            return None
        
        human_scores = self.merged_data['human_coherence']
        auto_scores = self.merged_data['auto_coherence']
        
        # 1. Effect sizeåˆ†æ
        # è®¡ç®—äººå·¥è¯„åˆ†é«˜åˆ†ç»„vsä½åˆ†ç»„çš„auto scoreå·®å¼‚
        high_human_mask = human_scores >= np.percentile(human_scores, 75)
        low_human_mask = human_scores <= np.percentile(human_scores, 25)
        
        high_auto_scores = auto_scores[high_human_mask]
        low_auto_scores = auto_scores[low_human_mask]
        
        if len(high_auto_scores) > 0 and len(low_auto_scores) > 0:
            # Cohen's d effect size
            pooled_std = np.sqrt(((len(high_auto_scores)-1)*np.var(high_auto_scores) + 
                                (len(low_auto_scores)-1)*np.var(low_auto_scores)) / 
                               (len(high_auto_scores)+len(low_auto_scores)-2))
            cohens_d = (np.mean(high_auto_scores) - np.mean(low_auto_scores)) / pooled_std
            
            # å®é™…å·®å¼‚
            practical_diff = np.mean(high_auto_scores) - np.mean(low_auto_scores)
            relative_diff = practical_diff / np.mean(low_auto_scores) * 100
        else:
            cohens_d = np.nan
            practical_diff = np.nan 
            relative_diff = np.nan
        
        # 2. åˆ†æ0.404è¿™ä¸ªåˆ†æ•°çš„æ„ä¹‰
        auto_mean = np.mean(auto_scores)
        human_mean = np.mean(human_scores)
        
        # æ‰¾åˆ°auto_coherence=0.404å¯¹åº”çš„human scoreæœŸæœ›å€¼
        if len(self.merged_data) > 3:
            from scipy.interpolate import interp1d
            try:
                # ä½¿ç”¨æ’å€¼ä¼°è®¡0.404å¯¹åº”çš„human score
                sorted_indices = np.argsort(auto_scores)
                sorted_auto = auto_scores.iloc[sorted_indices]
                sorted_human = human_scores.iloc[sorted_indices]
                
                if auto_mean >= min(sorted_auto) and auto_mean <= max(sorted_auto):
                    interp_func = interp1d(sorted_auto, sorted_human, kind='linear', fill_value='extrapolate')
                    expected_human_score = interp_func(0.404)
                else:
                    expected_human_score = np.nan
            except:
                expected_human_score = np.nan
        else:
            expected_human_score = np.nan
        
        practical_results = {
            'effect_size': {
                'cohens_d': cohens_d,
                'interpretation': self._interpret_cohens_d(cohens_d)
            },
            'practical_difference': {
                'absolute_diff': practical_diff,
                'relative_diff_percent': relative_diff
            },
            'score_interpretation': {
                'ai_average_auto_coherence': 0.404,
                'baseline_average_auto_coherence': 0.259,
                'expected_human_score_for_0404': expected_human_score,
                'human_score_range': f"{np.min(human_scores):.1f} - {np.max(human_scores):.1f}",
                'human_mean': human_mean
            }
        }
        
        self.results['practical_significance'] = practical_results
        
        print(f"ğŸ“Š Practical Significance Results:")
        if not np.isnan(cohens_d):
            print(f"   Cohen's d = {cohens_d:.3f} ({self._interpret_cohens_d(cohens_d)})")
        if not np.isnan(expected_human_score):
            print(f"   Auto score 0.404 â†’ Expected human score: {expected_human_score:.1f}/10")
        print(f"   Human score range: {np.min(human_scores):.1f} - {np.max(human_scores):.1f}")
        print(f"   Human mean: {human_mean:.1f}")
        
        return practical_results
    
    def _interpret_cohens_d(self, d):
        """è§£é‡ŠCohen's d effect size"""
        if np.isnan(d):
            return "æ— æ³•è®¡ç®—"
        elif abs(d) >= 0.8:
            return "å¤§æ•ˆåº”"
        elif abs(d) >= 0.5:
            return "ä¸­ç­‰æ•ˆåº”"
        elif abs(d) >= 0.2:
            return "å°æ•ˆåº”"
        else:
            return "å¾®å¼±æ•ˆåº”"
    
    def create_validation_visualizations(self):
        """åˆ›å»ºvalidationå¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“ˆ åˆ›å»ºHuman Validationå¯è§†åŒ–...")
        
        if len(self.merged_data) == 0:
            print("âŒ æ— æ•°æ®å¯è§†åŒ–")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Scatter plot: Human vs Auto coherence
        ax1 = axes[0, 0]
        human_scores = self.merged_data['human_coherence']
        auto_scores = self.merged_data['auto_coherence']
        
        ax1.scatter(auto_scores, human_scores, alpha=0.7, s=60)
        
        # æ·»åŠ regression line
        z = np.polyfit(auto_scores, human_scores, 1)
        p = np.poly1d(z)
        ax1.plot(auto_scores, p(auto_scores), "r--", alpha=0.8)
        
        # æ·»åŠ correlationä¿¡æ¯
        if 'correlation_analysis' in self.results:
            r = self.results['correlation_analysis']['overall_correlation']['pearson_r']
            p_val = self.results['correlation_analysis']['overall_correlation']['pearson_p']
            ax1.text(0.05, 0.95, f'r = {r:.3f}\np = {p_val:.3f}', 
                    transform=ax1.transAxes, bbox=dict(boxstyle="round", facecolor='white', alpha=0.8),
                    verticalalignment='top')
        
        ax1.set_xlabel('Auto Coherence Score')
        ax1.set_ylabel('Human Coherence Score (1-10)')
        ax1.set_title('Human vs Automated Coherence Correlation')
        ax1.grid(True, alpha=0.3)
        
        # 2. Distribution comparison
        ax2 = axes[0, 1]
        ax2.hist(human_scores, alpha=0.5, label='Human Scores', bins=10, density=True)
        
        # Normalize auto scores to 1-10 scale for comparison
        if auto_scores.max() != auto_scores.min():
            auto_normalized = (auto_scores - auto_scores.min()) / (auto_scores.max() - auto_scores.min()) * 9 + 1
            ax2.hist(auto_normalized, alpha=0.5, label='Auto Scores (normalized)', bins=10, density=True)
        else:
            # å¦‚æœæ‰€æœ‰auto scoresç›¸åŒï¼Œæ˜¾ç¤ºä¸ºå•ä¸€å€¼
            ax2.axvline(x=5, color='orange', alpha=0.7, linewidth=3, label=f'Auto Score (constant: {auto_scores.iloc[0]:.3f})')
        
        ax2.set_xlabel('Coherence Score')
        ax2.set_ylabel('Density')
        ax2.set_title('Score Distribution Comparison')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Threshold comparison
        ax3 = axes[1, 0]
        if 'threshold_analysis' in self.results:
            old_thresholds = [0.3, 0.5, 0.7]
            new_thresholds = [
                self.results['threshold_analysis']['empirical_thresholds']['poor'],
                self.results['threshold_analysis']['empirical_thresholds']['good'],
                self.results['threshold_analysis']['empirical_thresholds']['excellent']
            ]
            
            x = ['Poor/Good', 'Good/Excellent', 'Excellent+']
            x_pos = np.arange(len(x))
            
            width = 0.35
            ax3.bar(x_pos - width/2, old_thresholds, width, label='Old (Arbitrary)', alpha=0.7)
            ax3.bar(x_pos + width/2, new_thresholds, width, label='New (Empirical)', alpha=0.7)
            
            ax3.set_xlabel('Threshold Categories')
            ax3.set_ylabel('Auto Coherence Score')
            ax3.set_title('Threshold Comparison: Arbitrary vs Empirical')
            ax3.set_xticks(x_pos)
            ax3.set_xticklabels(x)
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        # 4. Residual analysis
        ax4 = axes[1, 1]
        if len(auto_scores) > 0 and len(human_scores) > 0:
            # Calculate predicted human scores
            z = np.polyfit(auto_scores, human_scores, 1)
            predicted_human = z[0] * auto_scores + z[1]
            residuals = human_scores - predicted_human
            
            ax4.scatter(predicted_human, residuals, alpha=0.7)
            ax4.axhline(y=0, color='red', linestyle='--', alpha=0.8)
            ax4.set_xlabel('Predicted Human Score')
            ax4.set_ylabel('Residuals')
            ax4.set_title('Residual Analysis')
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('/Users/haha/Story/coherence_human_validation_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: coherence_human_validation_analysis.png")
    
    def generate_academic_report(self):
        """ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š"""
        print("\nğŸ“ ç”Ÿæˆå­¦æœ¯æŠ¥å‘Š...")
        
        report = {
            'analysis_metadata': {
                'analysis_date': pd.Timestamp.now().isoformat(),
                'purpose': 'Human validation of automated coherence metric for Master thesis',
                'human_evaluators': len(self.human_data['participant_id'].unique()) if self.human_data is not None else 0,
                'total_evaluations': len(self.merged_data) if self.merged_data is not None else 0
            },
            'validation_results': self.results
        }
        
        # ä¿å­˜detailed results
        with open('/Users/haha/Story/coherence_human_validation_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        # ç”Ÿæˆç®€åŒ–çš„å­¦æœ¯æ€»ç»“
        self._generate_academic_summary()
        
        print("âœ… å­¦æœ¯æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print("   - è¯¦ç»†æŠ¥å‘Š: coherence_human_validation_report.json")
        print("   - å­¦æœ¯æ€»ç»“: coherence_academic_validation_summary.md")
    
    def _generate_academic_summary(self):
        """ç”Ÿæˆå­¦æœ¯æ€»ç»“"""
        if len(self.results) == 0:
            return
        
        summary = """# Coherence Metric Human Validation - Academic Summary

## ğŸ¯ ç ”ç©¶ç›®çš„
éªŒè¯è‡ªåŠ¨åŒ–coherenceæŒ‡æ ‡ä¸äººå·¥è¯„ä»·çš„ä¸€è‡´æ€§ï¼Œä»¥æ»¡è¶³ç¡•å£«è®ºæ–‡å­¦æœ¯æ ‡å‡†è¦æ±‚ã€‚

## ğŸ“Š éªŒè¯æ–¹æ³•
- **Human Evaluation**: 20åè¯„ä»·è€…ï¼Œ80æ¡coherenceè¯„åˆ† (1-10åˆ†)
- **Automated Metric**: HRED-based sentence-transformer coherence analysis
- **Question**: "How coherent and logical is the plot development of this story?"

## ğŸ“ˆ å…³é”®å‘ç°

### 1. Correlation Analysis
"""
        
        if 'correlation_analysis' in self.results:
            corr = self.results['correlation_analysis']['overall_correlation']
            summary += f"""
- **Pearson r = {corr['pearson_r']:.3f}** (p = {corr['pearson_p']:.3f})
- **Spearman r = {corr['spearman_r']:.3f}** (p = {corr['spearman_p']:.3f})
- **Sample size**: {corr['sample_size']}
- **Statistical significance**: {'âœ… YES' if corr['significant'] else 'âŒ NO'}
- **Academic assessment**: {'âœ… ACCEPTABLE' if corr.get('strong_correlation', False) else 'âš ï¸ NEEDS IMPROVEMENT'}
"""
        
        if 'threshold_analysis' in self.results:
            thresh = self.results['threshold_analysis']
            summary += f"""
### 2. Empirical Threshold Setting
**Old (Arbitrary) vs New (Empirical)**:
- Poor/Good boundary: 0.300 â†’ {thresh['empirical_thresholds']['poor']:.3f}
- Good/Excellent boundary: 0.500 â†’ {thresh['empirical_thresholds']['good']:.3f}  
- Excellence threshold: 0.700 â†’ {thresh['empirical_thresholds']['excellent']:.3f}

**Method**: Based on human score quartiles and corresponding auto scores
"""
        
        if 'practical_significance' in self.results:
            practical = self.results['practical_significance']
            summary += f"""
### 3. Practical Significance
"""
            if 'effect_size' in practical:
                summary += f"- **Effect size**: Cohen's d = {practical['effect_size']['cohens_d']:.3f} ({practical['effect_size']['interpretation']})\n"
            
            if 'score_interpretation' in practical:
                score_interp = practical['score_interpretation']
                summary += f"""- **AI Story Average (0.404)** corresponds to human score: {score_interp.get('expected_human_score_for_0404', 'N/A'):.1f}/10
- **Human score range**: {score_interp['human_score_range']}
- **Baseline vs AI improvement**: 0.259 â†’ 0.404 (+56%)
"""
        
        summary += """
## ğŸ“ å­¦æœ¯ç»“è®º

### Method Validity
"""
        
        # æ ¹æ®correlationç»“æœåˆ¤æ–­
        if 'correlation_analysis' in self.results:
            r = self.results['correlation_analysis']['overall_correlation']['pearson_r']
            p = self.results['correlation_analysis']['overall_correlation']['pearson_p']
            
            if p < 0.05 and abs(r) > 0.5:
                summary += "âœ… **VALIDATED**: Automated coherence shows acceptable correlation with human judgment\n"
            elif p < 0.05 and abs(r) > 0.3:
                summary += "âš ï¸ **MARGINAL**: Automated coherence shows weak but significant correlation\n" 
            else:
                summary += "âŒ **INSUFFICIENT**: Automated coherence lacks sufficient human validation\n"
        
        summary += """
### Threshold Justification
âœ… **IMPROVED**: Thresholds now based on empirical human data rather than arbitrary cutoffs

### Limitations Acknowledged
- âš ï¸ Method focuses on adjacent sentence similarity, may miss long-range coherence
- âš ï¸ Semantic similarity â‰  logical consistency  
- âš ï¸ Genre-specific effects not fully explored

### Master Thesis Readiness
"""
        
        # ç»¼åˆåˆ¤æ–­
        validation_score = 0
        if 'correlation_analysis' in self.results:
            if self.results['correlation_analysis']['overall_correlation'].get('significant', False):
                validation_score += 1
            if self.results['correlation_analysis']['overall_correlation'].get('strong_correlation', False):
                validation_score += 1
        
        if validation_score >= 2:
            summary += "âœ… **READY**: Coherence metric meets academic standards for Master thesis\n"
        elif validation_score == 1:
            summary += "âš ï¸ **CONDITIONAL**: Requires additional discussion of limitations\n"
        else:
            summary += "âŒ **NOT READY**: Requires substantial improvement or replacement\n"
        
        summary += """
## ğŸ” Recommended Next Steps

1. **If correlation is acceptable (r>0.5)**: Proceed with empirical thresholds
2. **If correlation is marginal (0.3<r<0.5)**: Add limitation discussion, consider complementary metrics
3. **If correlation is insufficient (r<0.3)**: Consider method revision or replacement

## ğŸ“š Academic Defense Points

**Expected Questions & Answers**:
- Q: "Why 0.404 is good coherence?" 
  A: "Based on human validation, corresponds to X/10 human rating"
- Q: "Adjacent sentence limitation?"
  A: "Acknowledged limitation, future work should include discourse markers"  
- Q: "Human agreement validity?"
  A: "Correlation analysis shows r=X.XX, statistically significant"
"""
        
        with open('/Users/haha/Story/coherence_academic_validation_summary.md', 'w', encoding='utf-8') as f:
            f.write(summary)
    
    def run_complete_validation(self):
        """è¿è¡Œå®Œæ•´çš„human validationåˆ†æ"""
        print("ğŸ“ å¼€å§‹Coherence Human Validation Analysis")
        print("=" * 60)
        
        # Step 1: Load data
        self.load_human_data()
        self.load_auto_coherence_data()
        
        # Step 2: Merge and validate
        merged = self.merge_human_auto_data()
        if len(merged) == 0:
            print("âŒ åˆ†æå¤±è´¥ï¼šæ— æ³•åŒ¹é…humanå’Œautoæ•°æ®")
            return None
        
        # Step 3: Core academic validations
        self.calculate_correlation_analysis()
        self.empirical_threshold_setting()
        self.practical_significance_analysis()
        
        # Step 4: Visualizations and reporting
        self.create_validation_visualizations()
        self.generate_academic_report()
        
        print("=" * 60)
        print("âœ… Human Validation Analysis Complete!")
        
        return self.results


if __name__ == "__main__":
    analyzer = CoherenceHumanValidationAnalyzer()
    results = analyzer.run_complete_validation()
