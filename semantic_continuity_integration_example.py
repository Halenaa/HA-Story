"""
è¯­ä¹‰è¿ç»­æ€§ç³»ç»Ÿé›†æˆç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„ç›¸å¯¹æ¯”è¾ƒç³»ç»Ÿæ›¿ä»£ä¸»è§‚é˜ˆå€¼
"""

import os
import sys
import json
import pandas as pd
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent.absolute()
project_root = current_dir.parent if 'src' in str(current_dir) else current_dir
sys.path.append(str(project_root))

from src.utils.semantic_continuity_relative_comparison import (
    SemanticContinuityRelativeComparison,
    create_comparison_system_from_existing_data
)
from src.analysis.hred_coherence_evaluator import HREDSemanticContinuityEvaluator


def integrate_relative_comparison_into_evaluation():
    """
    å°†ç›¸å¯¹æ¯”è¾ƒç³»ç»Ÿé›†æˆåˆ°ç°æœ‰è¯„ä¼°æµç¨‹ä¸­çš„ç¤ºä¾‹
    """
    print("ğŸ”§ è¯­ä¹‰è¿ç»­æ€§è¯„ä¼°ç³»ç»Ÿé›†æˆç¤ºä¾‹")
    print("=" * 50)
    
    # 1. åˆ›å»ºç›¸å¯¹æ¯”è¾ƒç³»ç»Ÿ
    comparison_system = SemanticContinuityRelativeComparison()
    
    # 2. ä»ç°æœ‰CSVæ•°æ®ä¸­åŠ è½½åŸºå‡†åˆ†æ•°
    csv_files = [
        "/Users/haha/Story/metrics_master_clean.csv",
        # å¯ä»¥æ·»åŠ æ›´å¤šåŸºå‡†æ•°æ®æ–‡ä»¶
    ]
    
    try:
        # å°è¯•ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®
        df = pd.read_csv("/Users/haha/Story/metrics_master_clean.csv")
        
        # æ£€æŸ¥æ–°çš„åˆ—åæ˜¯å¦å­˜åœ¨
        if 'avg_semantic_continuity' in df.columns:
            baseline_scores = df['avg_semantic_continuity'].dropna().tolist()
            comparison_system.add_baseline_data(baseline_scores, "existing_stories")
            print(f"âœ… ä»CSVåŠ è½½äº† {len(baseline_scores)} ä¸ªåŸºå‡†åˆ†æ•°")
        else:
            print("âš ï¸  CSVæ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'avg_semantic_continuity' åˆ—")
            print(f"å¯ç”¨åˆ—: {list(df.columns)}")
            # ä¸ºæ¼”ç¤ºç›®çš„åˆ›å»ºç¤ºä¾‹æ•°æ®
            baseline_scores = [0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70]
            comparison_system.add_baseline_data(baseline_scores, "demo_baseline")
            print("âœ… ä½¿ç”¨æ¼”ç¤ºåŸºå‡†æ•°æ®")
    
    except Exception as e:
        print(f"âš ï¸  æ— æ³•åŠ è½½CSVæ•°æ®: {e}")
        # åˆ›å»ºæ¼”ç¤ºæ•°æ®
        baseline_scores = [0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70]
        comparison_system.add_baseline_data(baseline_scores, "demo_baseline")
        print("âœ… ä½¿ç”¨æ¼”ç¤ºåŸºå‡†æ•°æ®")
    
    # 3. å±•ç¤ºç›¸å¯¹æ¯”è¾ƒåŠŸèƒ½
    test_scores = [0.30, 0.45, 0.55, 0.65, 0.75]
    dataset_name = "existing_stories" if 'avg_semantic_continuity' in df.columns else "demo_baseline"
    
    print(f"\nğŸ“Š è¯­ä¹‰è¿ç»­æ€§ç›¸å¯¹æ¯”è¾ƒç»“æœ:")
    print("-" * 50)
    
    for score in test_scores:
        result = comparison_system.compare_to_baseline(score, dataset_name)
        print(f"åˆ†æ•°: {score:.3f} | {result['comparison_description']} | {result['position_description']}")
    
    # 4. ç”Ÿæˆåˆ†å¸ƒæ€»ç»“
    print(f"\nğŸ“ˆ åŸºå‡†æ•°æ®åˆ†å¸ƒæ€»ç»“:")
    print("-" * 50)
    summary = comparison_system.generate_distribution_summary(dataset_name)
    
    print(f"æ ·æœ¬æ•°é‡: {summary['sample_count']}")
    print(f"å¹³å‡å€¼: {summary['distribution_summary']['mean']}")
    print(f"æ ‡å‡†å·®: {summary['distribution_summary']['std']}")
    print(f"èŒƒå›´: {summary['distribution_summary']['min']} - {summary['distribution_summary']['max']}")
    
    print("\nç™¾åˆ†ä½é˜ˆå€¼:")
    for percentile, value in summary['percentile_thresholds'].items():
        print(f"  {percentile}: {value}")
    
    print(f"\nâš ï¸  é‡è¦è¯´æ˜: {summary['measurement_limitation']}")
    
    return comparison_system


def demonstrate_semantic_continuity_evaluation(story_text_sample):
    """
    æ¼”ç¤ºæ–°çš„è¯­ä¹‰è¿ç»­æ€§è¯„ä¼°æ–¹æ³•ï¼ˆä¸ä½¿ç”¨ä¸»è§‚é˜ˆå€¼ï¼‰
    """
    print(f"\nğŸ” è¯­ä¹‰è¿ç»­æ€§è¯„ä¼°æ¼”ç¤º")
    print("-" * 50)
    
    # æ¨¡æ‹Ÿå¥å­ï¼ˆå®é™…ä½¿ç”¨æ—¶ä¼šä»æ•…äº‹ä¸­æå–ï¼‰
    sentences = [
        "å°çº¢å¸½å‡†å¤‡å»çœ‹æœ›ç”Ÿç—…çš„å¤–å©†ã€‚",
        "å¥¹åœ¨æ£®æ—ä¸­é‡åˆ°äº†ä¸€åªç‹¼ã€‚", 
        "ç‹¼è¯¢é—®å°çº¢å¸½è¦å»å“ªé‡Œã€‚",
        "å°çº¢å¸½å‘Šè¯‰äº†ç‹¼å¤–å©†ä½çš„åœ°æ–¹ã€‚",
        "ç‹¼æƒ³å‡ºäº†ä¸€ä¸ªé‚ªæ¶çš„è®¡åˆ’ã€‚"
    ]
    
    print("ç¤ºä¾‹å¥å­åºåˆ—:")
    for i, sentence in enumerate(sentences, 1):
        print(f"  {i}. {sentence}")
    
    # ä½¿ç”¨è¯­ä¹‰è¿ç»­æ€§è¯„ä¼°å™¨
    try:
        evaluator = HREDSemanticContinuityEvaluator(model_name='all-MiniLM-L6-v2')  # ä½¿ç”¨æ›´è½»é‡çš„æ¨¡å‹è¿›è¡Œæ¼”ç¤º
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„æ•…äº‹æ•°æ®æ ¼å¼
        story_data = [{"plot": " ".join(sentences)}]
        
        # è¿›è¡Œè¯„ä¼°
        result = evaluator.evaluate_story_semantic_continuity(story_data, include_details=False)
        
        continuity_score = result['HRED_semantic_continuity_evaluation']['average_semantic_continuity']
        print(f"\nè¯­ä¹‰è¿ç»­æ€§åˆ†æ•°: {continuity_score:.4f}")
        print(f"æµ‹é‡è¯´æ˜: {result['HRED_semantic_continuity_evaluation']['objective_description']['explanation']}")
        print(f"å±€é™æ€§è¯´æ˜: {result['HRED_semantic_continuity_evaluation']['objective_description']['limitation']}")
        
        # ä½¿ç”¨ç›¸å¯¹æ¯”è¾ƒç³»ç»Ÿ
        comparison_system = integrate_relative_comparison_into_evaluation()
        dataset_name = list(comparison_system.reference_data.keys())[0]
        comparison_result = comparison_system.compare_to_baseline(continuity_score, dataset_name)
        
        print(f"\nç›¸å¯¹æ¯”è¾ƒç»“æœ:")
        print(f"  {comparison_result['comparison_description']}")
        print(f"  æ°´å¹³è¯„ä»·: {comparison_result['position_description']}")
        print(f"  ç™¾åˆ†ä½æ’å: {comparison_result['percentile_rank']}%")
        
    except ImportError:
        print("âš ï¸  sentence-transformersæœªå®‰è£…ï¼Œæ— æ³•è¿è¡Œè¯­ä¹‰è¿ç»­æ€§è¯„ä¼°æ¼”ç¤º")
    except Exception as e:
        print(f"âš ï¸  è¯„ä¼°æ¼”ç¤ºå¤±è´¥: {e}")


if __name__ == "__main__":
    # è¿è¡Œé›†æˆç¤ºä¾‹
    comparison_system = integrate_relative_comparison_into_evaluation()
    
    # è¿è¡Œè¯„ä¼°æ¼”ç¤º
    sample_story = "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ•…äº‹æ–‡æœ¬"
    demonstrate_semantic_continuity_evaluation(sample_story)
    
    print(f"\nâœ… è¯­ä¹‰è¿ç»­æ€§ç³»ç»Ÿé›†æˆå®Œæˆ!")
    print("\nä¸»è¦æ”¹è¿›:")
    print("  âŒ åˆ é™¤äº†ä¸»è§‚é˜ˆå€¼ (0.7='ä¼˜ç§€' ç­‰)")
    print("  âœ… å®ç°äº†ç›¸å¯¹æ¯”è¾ƒç³»ç»Ÿ")
    print("  âœ… è¯šå®è¯´æ˜äº†æµ‹é‡å±€é™æ€§")
    print("  âœ… æä¾›äº†å®¢è§‚çš„ç™¾åˆ†ä½æ’å")
