#!/usr/bin/env python3
"""
ä½¿ç”¨ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒçš„PPLè®¡ç®—æ–¹å¼é‡æ–°è®¡ç®—baseline PPL
ç¡®ä¿å¯¹æ¯”çš„å…¬å¹³æ€§å’Œä¸€è‡´æ€§

åŸºäº batch_analyze_fluency.py çš„é€»è¾‘ï¼Œä½¿ç”¨ç›¸åŒçš„ï¼š
- FluencyAnalyzer
- roberta-large æ¨¡å‹  
- ç›¸åŒçš„å‚æ•°è®¾ç½®
- ç›¸åŒçš„è®¡ç®—æµç¨‹
"""

import os
import json
import sys
from pathlib import Path
from typing import Dict, List
import pandas as pd
from datetime import datetime

# æ·»åŠ srcè·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from analysis.fluency_analyzer import FluencyAnalyzer

class BaselinePPLRecalculator:
    """ä½¿ç”¨ä¸å®éªŒæ ·æœ¬ç›¸åŒæ–¹æ³•çš„baseline PPLé‡æ–°è®¡ç®—å™¨"""
    
    def __init__(self, model_name: str = "roberta-large"):
        """
        åˆå§‹åŒ–é‡æ–°è®¡ç®—å™¨
        
        Args:
            model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œå¿…é¡»ä¸å®éªŒæ ·æœ¬ä¸€è‡´
        """
        self.model_name = model_name
        print(f"ğŸ”„ åˆå§‹åŒ–baseline PPLé‡æ–°è®¡ç®—å™¨")
        print(f"   ä½¿ç”¨æ¨¡å‹: {model_name} (ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒ)")
        
        # baselineæ–‡ä»¶é…ç½®
        self.baseline_files = {
            'baseline_s1': '/Users/haha/Story/baseline_s1.md',
            'baseline_s2': '/Users/haha/Story/baseline_s2.md', 
            'baseline_s3': '/Users/haha/Story/baseline_s3.md'
        }
        
        # è¾“å‡ºç›®å½•
        self.output_dir = '/Users/haha/Story/baseline_ppl_recalculation_results'
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def check_baseline_files(self):
        """æ£€æŸ¥baselineæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        print("\nğŸ“‹ æ£€æŸ¥baselineæ–‡ä»¶...")
        
        missing_files = []
        for name, path in self.baseline_files.items():
            if os.path.exists(path):
                file_size = os.path.getsize(path)
                word_count = len(open(path, 'r', encoding='utf-8').read().split())
                print(f"   âœ… {name}: {file_size:,} bytes, ~{word_count:,} words")
            else:
                print(f"   âŒ {name}: æ–‡ä»¶ä¸å­˜åœ¨!")
                missing_files.append((name, path))
        
        if missing_files:
            print(f"\nâŒ å‘ç° {len(missing_files)} ä¸ªæ–‡ä»¶ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥!")
            return False
        
        print(f"âœ… æ‰€æœ‰ {len(self.baseline_files)} ä¸ªbaselineæ–‡ä»¶æ£€æŸ¥é€šè¿‡!")
        return True
    
    def read_story_content(self, file_path: str) -> str:
        """è¯»å–æ•…äº‹å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return content.strip()
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    def save_individual_result(self, result: Dict, baseline_name: str):
        """ä¿å­˜å•ä¸ªbaselineçš„ç»“æœ"""
        output_file = os.path.join(self.output_dir, f"{baseline_name}_unified_ppl_result.json")
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"   ğŸ’¾ å•ä¸ªç»“æœä¿å­˜: {output_file}")
        except Exception as e:
            print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
    
    def recalculate_all_baselines(self):
        """é‡æ–°è®¡ç®—æ‰€æœ‰baselineçš„PPL"""
        print(f"\nğŸš€ å¼€å§‹é‡æ–°è®¡ç®—baseline PPL")
        print(f"{'='*80}")
        print(f"ğŸ¯ ç›®æ ‡: ä½¿ç”¨ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒçš„ç®—æ³•å’Œå‚æ•°")
        print(f"ğŸ“Š æ¨¡å‹: {self.model_name}")
        print(f"ğŸ”§ ç®—æ³•: FluencyAnalyzer (roberta-large + Masked LM pseudo-PPL)")
        print(f"{'='*80}")
        
        # åˆå§‹åŒ–åˆ†æå™¨ï¼ˆä¸å®éªŒæ ·æœ¬ä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ–¹å¼ï¼‰
        print("\nğŸ¤– åˆå§‹åŒ–æµç•…åº¦åˆ†æå™¨...")
        analyzer = FluencyAnalyzer(model_name=self.model_name)
        print(f"   âœ… åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = []
        comparison_data = []
        
        # è¯»å–ç°æœ‰çš„baseline PPLæ•°æ®ä½œä¸ºå¯¹æ¯”
        original_ppls = self.load_original_baseline_ppls()
        
        # é€ä¸ªå¤„ç†baselineæ–‡ä»¶
        for i, (baseline_name, file_path) in enumerate(self.baseline_files.items(), 1):
            print(f"\nğŸ“ [{i}/{len(self.baseline_files)}] é‡æ–°è®¡ç®—: {baseline_name}")
            print(f"   ğŸ“‚ æ–‡ä»¶: {file_path}")
            
            # è¯»å–æ•…äº‹å†…å®¹
            story_content = self.read_story_content(file_path)
            if not story_content:
                print(f"   â­ï¸  è·³è¿‡ç©ºæ–‡ä»¶")
                continue
            
            word_count = len(story_content.split())
            char_count = len(story_content)
            print(f"   ğŸ“ æ–‡æœ¬: {char_count:,} å­—ç¬¦, {word_count:,} è¯")
            
            # ä½¿ç”¨ä¸å®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒçš„æ–¹æ³•åˆ†ææµç•…åº¦
            try:
                start_time = datetime.now()
                print(f"   ğŸ”„ å¼€å§‹PPLè®¡ç®—...")
                
                # å…³é”®ï¼šä½¿ç”¨ä¸å®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒçš„analyze_fluencyæ–¹æ³•
                result = analyzer.analyze_fluency(story_content)
                
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                
                # æ·»åŠ é¢å¤–ä¿¡æ¯ï¼ˆä¸å®éªŒæ ·æœ¬ä¿æŒä¸€è‡´çš„æ ¼å¼ï¼‰
                result.update({
                    'baseline_name': baseline_name,
                    'story_file_path': file_path,
                    'word_count': word_count,
                    'char_count': char_count,
                    'calculation_duration_seconds': duration,
                    'recalculation_timestamp': datetime.now().isoformat(),
                    'method_note': 'Same as 54 experimental samples: FluencyAnalyzer + roberta-large'
                })
                
                # ä¿å­˜å•ä¸ªç»“æœ
                self.save_individual_result(result, baseline_name)
                
                # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                all_results.append(result)
                
                # å¯¹æ¯”æ•°æ®
                original_ppl = original_ppls.get(baseline_name, 'N/A')
                new_ppl = result['pseudo_ppl']
                
                comparison_data.append({
                    'baseline_name': baseline_name,
                    'original_ppl': original_ppl,
                    'new_ppl_unified': new_ppl,
                    'difference': abs(new_ppl - original_ppl) if original_ppl != 'N/A' else 'N/A',
                    'word_count': word_count
                })
                
                print(f"   âœ… å®Œæˆ - PPL: {new_ppl:.3f}, é”™è¯¯ç‡: {result['err_per_100w']:.2f}%, è€—æ—¶: {duration:.1f}ç§’")
                if original_ppl != 'N/A':
                    diff = abs(new_ppl - original_ppl)
                    print(f"   ğŸ“Š å¯¹æ¯” - åŸPPL: {original_ppl:.3f}, æ–°PPL: {new_ppl:.3f}, å·®å¼‚: {diff:.3f}")
                
            except Exception as e:
                print(f"   âŒ è®¡ç®—å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # å½’ä¸€åŒ–åˆ†æ•°ï¼ˆä¸å®éªŒæ ·æœ¬ä½¿ç”¨ç›¸åŒæ–¹æ³•ï¼‰
        if all_results:
            print(f"\nğŸ”¢ åº”ç”¨å½’ä¸€åŒ–åˆ†æ•°ï¼ˆä¸å®éªŒæ ·æœ¬ç›¸åŒæ–¹æ³•ï¼‰...")
            try:
                all_results = analyzer.normalize_scores(all_results)
                print(f"   âœ… å½’ä¸€åŒ–å®Œæˆ")
            except Exception as e:
                print(f"   âš ï¸  å½’ä¸€åŒ–å¤±è´¥: {e}")
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        self.save_summary_results(all_results, comparison_data)
        
        print(f"\nğŸ‰ Baseline PPLé‡æ–°è®¡ç®—å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(all_results)}/{len(self.baseline_files)} ä¸ªæ–‡ä»¶")
        
        return all_results, comparison_data
    
    def load_original_baseline_ppls(self) -> Dict:
        """åŠ è½½åŸå§‹çš„baseline PPLæ•°æ®ç”¨äºå¯¹æ¯”"""
        original_ppls = {}
        
        # ä»ç°æœ‰çš„JSONæ–‡ä»¶ä¸­è¯»å–
        ppl_files = {
            'baseline_s1': '/Users/haha/Story/baseline_s1_fluency_result.json',
            'baseline_s2': '/Users/haha/Story/baseline_s2_fluency_result.json',
            'baseline_s3': '/Users/haha/Story/baseline_s3_fluency_result.json'
        }
        
        for name, file_path in ppl_files.items():
            try:
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    original_ppls[name] = data.get('pseudo_ppl', 'N/A')
                    print(f"   ğŸ“„ åŠ è½½åŸå§‹PPL {name}: {original_ppls[name]}")
            except Exception as e:
                print(f"   âš ï¸  æ— æ³•åŠ è½½åŸå§‹PPL {name}: {e}")
                original_ppls[name] = 'N/A'
        
        return original_ppls
    
    def save_summary_results(self, results: List[Dict], comparison_data: List[Dict]):
        """ä¿å­˜æ±‡æ€»ç»“æœå’Œå¯¹æ¯”æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ä¿å­˜è¯¦ç»†ç»“æœ
        summary_file = os.path.join(self.output_dir, f'baseline_ppl_unified_summary_{timestamp}.json')
        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'method': 'Same as 54 experimental samples',
                    'model': self.model_name,
                    'timestamp': timestamp,
                    'results': results
                }, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“„ è¯¦ç»†æ±‡æ€»ä¿å­˜: {summary_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜è¯¦ç»†æ±‡æ€»å¤±è´¥: {e}")
        
        # 2. ä¿å­˜å¯¹æ¯”CSV
        comparison_file = os.path.join(self.output_dir, f'baseline_ppl_comparison_{timestamp}.csv')
        try:
            df = pd.DataFrame(comparison_data)
            df.to_csv(comparison_file, index=False)
            print(f"ğŸ“Š å¯¹æ¯”CSVä¿å­˜: {comparison_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¯¹æ¯”CSVå¤±è´¥: {e}")
        
        # 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report(results, comparison_data, timestamp)
    
    def generate_comparison_report(self, results: List[Dict], comparison_data: List[Dict], timestamp: str):
        """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š"""
        report_file = os.path.join(self.output_dir, f'baseline_ppl_unification_report_{timestamp}.md')
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        new_ppls = [r['pseudo_ppl'] for r in results if r['pseudo_ppl'] != float('inf')]
        
        lines = [
            "# ğŸ”„ Baseline PPLç»Ÿä¸€é‡æ–°è®¡ç®—æŠ¥å‘Š",
            f"\n**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**è®¡ç®—æ–¹æ³•**: ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ä¸€è‡´",
            f"**ä½¿ç”¨æ¨¡å‹**: {self.model_name}",
            
            "\n## ğŸ¯ ç›®æ ‡",
            "\nç¡®ä¿baselineå’Œ54ä¸ªå®éªŒæ ·æœ¬ä½¿ç”¨**å®Œå…¨ç›¸åŒ**çš„PPLè®¡ç®—æ–¹æ³•ï¼š",
            "- ç›¸åŒçš„æ¨¡å‹: `roberta-large`",
            "- ç›¸åŒçš„ç®—æ³•: `FluencyAnalyzer.analyze_fluency()`", 
            "- ç›¸åŒçš„å‚æ•°: `subsample_rate=4`, è‡ªé€‚åº”å­é‡‡æ ·",
            "- ç›¸åŒçš„å¤„ç†æµç¨‹: åˆ†å—ã€æ©ç ã€å½’ä¸€åŒ–",
            
            "\n## ğŸ“Š é‡æ–°è®¡ç®—ç»“æœ",
            "\n### æ–°çš„ç»Ÿä¸€PPLå€¼:"
        ]
        
        for i, result in enumerate(results):
            name = result['baseline_name']
            ppl = result['pseudo_ppl']
            err_rate = result['err_per_100w']
            word_count = result['word_count']
            duration = result.get('calculation_duration_seconds', 0)
            
            lines.append(f"- **{name}**: PPL = {ppl:.3f}, é”™è¯¯ç‡ = {err_rate:.2f}%, {word_count:,} è¯ ({duration:.1f}ç§’)")
        
        # ç»Ÿè®¡æ‘˜è¦
        if new_ppls:
            avg_ppl = sum(new_ppls) / len(new_ppls)
            std_ppl = (sum((x - avg_ppl) ** 2 for x in new_ppls) / len(new_ppls)) ** 0.5
            
            lines.extend([
                f"\n### ç»Ÿè®¡æ‘˜è¦:",
                f"- **å¹³å‡PPL**: {avg_ppl:.3f} Â± {std_ppl:.3f}",
                f"- **PPLèŒƒå›´**: {min(new_ppls):.3f} - {max(new_ppls):.3f}",
                f"- **æ ·æœ¬æ•°**: {len(new_ppls)}"
            ])
        
        # å¯¹æ¯”è¡¨æ ¼
        lines.extend([
            "\n## ğŸ“ˆ æ–°æ—§å¯¹æ¯”",
            "\n| Baseline | åŸPPL | æ–°PPL (ç»Ÿä¸€) | å·®å¼‚ | è¯æ•° |",
            "|----------|-------|-------------|------|------|"
        ])
        
        for comp in comparison_data:
            name = comp['baseline_name']
            orig = comp['original_ppl']
            new = comp['new_ppl_unified']
            diff = comp['difference']
            words = comp['word_count']
            
            orig_str = f"{orig:.3f}" if orig != 'N/A' else 'N/A'
            diff_str = f"{diff:.3f}" if diff != 'N/A' else 'N/A'
            
            lines.append(f"| {name} | {orig_str} | {new:.3f} | {diff_str} | {words:,} |")
        
        # ç»“è®ºå’Œå»ºè®®
        lines.extend([
            "\n## âœ… ç»“è®º",
            "\n### æˆåŠŸå®Œæˆç»Ÿä¸€:",
            "1. âœ… æ‰€æœ‰baselineç°åœ¨ä½¿ç”¨ä¸54ä¸ªå®éªŒæ ·æœ¬ç›¸åŒçš„PPLè®¡ç®—æ–¹æ³•",
            "2. âœ… ä½¿ç”¨ç›¸åŒçš„`roberta-large`æ¨¡å‹å’Œ`FluencyAnalyzer`",
            "3. âœ… åº”ç”¨ç›¸åŒçš„å‚æ•°è®¾ç½®å’Œå¤„ç†æµç¨‹",
            "4. âœ… æ•°æ®å¯¹æ¯”ç°åœ¨å…·æœ‰å®Œå…¨çš„å…¬å¹³æ€§",
            
            "\n### ä¸‹ä¸€æ­¥è¡ŒåŠ¨:",
            "1. **æ›´æ–°CSVæ–‡ä»¶**: å°†æ–°çš„ç»Ÿä¸€PPLå€¼æ›´æ–°åˆ°`metrics_master_clean.csv`",
            "2. **é‡æ–°åˆ†æ**: åŸºäºç»Ÿä¸€çš„PPLæ•°æ®é‡æ–°è¿›è¡Œfluencyç»´åº¦åˆ†æ",
            "3. **éªŒè¯ç»“æœ**: ç¡®ä¿æ–°çš„PPLå€¼åˆç†ä¸”ä¸å®éªŒæ ·æœ¬åœ¨åŒä¸€é‡çº§",
            "4. **å½’æ¡£å¤‡ä»½**: ä¿ç•™åŸå§‹æ•°æ®ä½œä¸ºå‚è€ƒ",
            
            f"\n---\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
        ])
        
        # ä¿å­˜æŠ¥å‘Š
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines))
            print(f"ğŸ“‹ å¯¹æ¯”æŠ¥å‘Šä¿å­˜: {report_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ Baseline PPLç»Ÿä¸€é‡æ–°è®¡ç®—ç³»ç»Ÿ")
    print("=" * 80)
    print("ç›®æ ‡: ä½¿ç”¨ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒçš„PPLç®—æ³•é‡æ–°è®¡ç®—baseline")
    print("æ–¹æ³•: FluencyAnalyzer + roberta-large (å®Œå…¨ä¸€è‡´)")
    print("=" * 80)
    
    # åˆå§‹åŒ–é‡æ–°è®¡ç®—å™¨
    try:
        recalculator = BaselinePPLRecalculator(model_name="roberta-large")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥æ–‡ä»¶
    if not recalculator.check_baseline_files():
        return False
    
    # è¯¢é—®ç”¨æˆ·ç¡®è®¤
    try:
        print(f"\nğŸ¤” ç¡®è®¤å¼€å§‹é‡æ–°è®¡ç®—å—?")
        print("   è¿™å°†ä½¿ç”¨ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒçš„æ–¹æ³•é‡æ–°è®¡ç®—æ‰€æœ‰baseline PPL")
        confirm = input("   è¾“å…¥ 'y' ç»§ç»­: ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return False
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return False
    
    # æ‰§è¡Œé‡æ–°è®¡ç®—
    try:
        start_time = datetime.now()
        results, comparison_data = recalculator.recalculate_all_baselines()
        end_time = datetime.now()
        
        total_time = (end_time - start_time).total_seconds()
        
        print(f"\n{'='*80}")
        print("ğŸ‰ é‡æ–°è®¡ç®—å®Œæˆ!")
        print(f"{'='*80}")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f} ç§’")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(results)} ä¸ªbaselineæ–‡ä»¶")
        print(f"ğŸ“ ç»“æœç›®å½•: {recalculator.output_dir}")
        
        if results:
            ppls = [r['pseudo_ppl'] for r in results if r['pseudo_ppl'] != float('inf')]
            if ppls:
                avg_ppl = sum(ppls) / len(ppls)
                print(f"\nğŸ“ˆ æ–°çš„ç»Ÿä¸€baselineå¹³å‡PPL: {avg_ppl:.3f}")
                print("âœ… ç°åœ¨baselineå’Œ54ä¸ªå®éªŒæ ·æœ¬ä½¿ç”¨å®Œå…¨ç›¸åŒçš„PPLç®—æ³•!")
        
        return True
        
    except Exception as e:
        print(f"âŒ é‡æ–°è®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
