#!/usr/bin/env python3
"""
PPLè®¡ç®—ä¸€è‡´æ€§éªŒè¯è„šæœ¬ - GPUä¼˜åŒ–ç‰ˆæœ¬
ä¸“é—¨ä¸ºGPUç¯å¢ƒä¼˜åŒ–ï¼ŒåŠ é€ŸPPLè®¡ç®—

è¿è¡Œæ–¹æ³•:
1. ç¡®ä¿æœ‰CUDAç¯å¢ƒ: nvidia-smi
2. å®‰è£…ä¾èµ–: pip install torch transformers pandas numpy
3. è¿è¡Œè„šæœ¬: python verify_ppl_calculation_gpu.py
"""

import pandas as pd
import numpy as np
import torch
import json
import os
from pathlib import Path
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForMaskedLM, GPT2LMHeadModel, GPT2Tokenizer
import math
from typing import Dict, List, Tuple
import warnings
import gc
warnings.filterwarnings('ignore')

class GPUOptimizedPPLCalculator:
    """GPUä¼˜åŒ–çš„PPLè®¡ç®—å™¨"""
    
    def __init__(self, method='bert_masked_lm', batch_size=32):
        """
        åˆå§‹åŒ–GPUä¼˜åŒ–PPLè®¡ç®—å™¨
        
        Args:
            method: 'bert_masked_lm' æˆ– 'gpt2_autoregressive'
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ŒGPUå†…å­˜å…è®¸çš„æƒ…å†µä¸‹å¯ä»¥è°ƒå¤§
        """
        self.method = method
        self.batch_size = batch_size
        
        # æ£€æŸ¥GPU
        if not torch.cuda.is_available():
            print("âŒ æœªæ£€æµ‹åˆ°CUDAæ”¯æŒï¼")
            print("è¯·ç¡®ä¿ï¼š")
            print("1. æœ‰NVIDIA GPU")
            print("2. å®‰è£…äº†CUDA")
            print("3. å®‰è£…äº†æ”¯æŒCUDAçš„PyTorch")
            raise RuntimeError("éœ€è¦GPUæ”¯æŒ")
        
        self.device = torch.device('cuda')
        gpu_name = torch.cuda.get_device_name()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"ğŸš€ GPUç¯å¢ƒæ£€æµ‹æˆåŠŸï¼")
        print(f"   GPU: {gpu_name}")
        print(f"   æ˜¾å­˜: {gpu_memory:.1f} GB")
        print(f"   æ–¹æ³•: {method}")
        print(f"   æ‰¹å¤§å°: {batch_size}")
        
        # åŠ è½½æ¨¡å‹
        if method == 'bert_masked_lm':
            self.model_name = 'bert-base-uncased'
            print(f"ğŸ”„ åŠ è½½BERTæ¨¡å‹...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForMaskedLM.from_pretrained(self.model_name)
        elif method == 'gpt2_autoregressive':
            self.model_name = 'gpt2'
            print(f"ğŸ”„ åŠ è½½GPT-2æ¨¡å‹...")
            self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)
            self.model = GPT2LMHeadModel.from_pretrained(self.model_name)
            self.tokenizer.pad_token = self.tokenizer.eos_token
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
        
        self.model.to(self.device)
        self.model.eval()
        
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆå¹¶ç§»è‡³GPU")
        print(f"   GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
    
    def calculate_bert_pseudo_ppl_gpu(self, text: str, subsample_rate: int = 4) -> float:
        """
        GPUä¼˜åŒ–çš„BERT pseudo-PPLè®¡ç®—
        ä½¿ç”¨æ‰¹å¤„ç†åŠ é€Ÿè®¡ç®—
        """
        if not text or not text.strip():
            return float('inf')
        
        # åˆ†è¯
        tokens = self.tokenizer.tokenize(text)
        if len(tokens) < 2:
            return float('inf')
        
        # é™åˆ¶æœ€å¤§é•¿åº¦ä»¥é€‚åº”GPUå†…å­˜
        max_tokens = 480  # ä¸ºç‰¹æ®Štokenç•™ç©ºé—´
        if len(tokens) > max_tokens:
            print(f"   æ–‡æœ¬è¿‡é•¿({len(tokens)} tokens)ï¼Œæˆªå–å‰{max_tokens}ä¸ªtokens")
            tokens = tokens[:max_tokens]
        
        total_neg_log_likelihood = 0.0
        valid_predictions = 0
        
        # ç¡®ä¿å­é‡‡æ ·ç‡åˆç†
        actual_subsample_rate = max(1, min(subsample_rate, len(tokens)))
        positions_to_mask = list(range(0, len(tokens), actual_subsample_rate))
        
        print(f"   å¤„ç† {len(tokens)} tokensï¼Œ{len(positions_to_mask)} ä¸ªæ©ç ä½ç½®")
        
        # æ‰¹å¤„ç†
        batches = [positions_to_mask[i:i+self.batch_size] 
                  for i in range(0, len(positions_to_mask), self.batch_size)]
        
        for batch_idx, batch_positions in enumerate(batches):
            try:
                batch_inputs = []
                batch_original_tokens = []
                batch_mask_positions = []
                
                for pos in batch_positions:
                    if pos >= len(tokens):
                        continue
                    
                    original_token = tokens[pos]
                    
                    # è·³è¿‡ç‰¹æ®Štoken
                    if original_token in ['[CLS]', '[SEP]', '[PAD]']:
                        continue
                    
                    # åˆ›å»ºmaskedç‰ˆæœ¬
                    masked_tokens = ['[CLS]'] + tokens[:pos] + ['[MASK]'] + tokens[pos+1:] + ['[SEP]']
                    
                    # æˆªæ–­ä»¥é€‚åº”æ¨¡å‹é™åˆ¶
                    if len(masked_tokens) > 512:
                        masked_tokens = masked_tokens[:512]
                    
                    input_text = ' '.join(masked_tokens)
                    batch_inputs.append(input_text)
                    batch_original_tokens.append(original_token)
                    
                    # æ‰¾åˆ°[MASK]åœ¨input_idsä¸­çš„ä½ç½®
                    temp_encoded = self.tokenizer(input_text, return_tensors='pt')
                    mask_pos = (temp_encoded['input_ids'] == self.tokenizer.mask_token_id).nonzero(as_tuple=True)[1]
                    batch_mask_positions.append(mask_pos[0].item() if len(mask_pos) > 0 else -1)
                
                if not batch_inputs:
                    continue
                
                # æ‰¹é‡ç¼–ç 
                batch_encoded = self.tokenizer(
                    batch_inputs, 
                    padding=True, 
                    truncation=True,
                    max_length=512,
                    return_tensors='pt'
                )
                
                # ç§»è‡³GPU
                batch_encoded = {k: v.to(self.device) for k, v in batch_encoded.items()}
                
                with torch.no_grad():
                    outputs = self.model(**batch_encoded)
                    logits = outputs.logits  # [batch_size, seq_len, vocab_size]
                
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„logæ¦‚ç‡
                for i, (original_token, mask_pos) in enumerate(zip(batch_original_tokens, batch_mask_positions)):
                    if mask_pos == -1:
                        continue
                    
                    try:
                        original_token_id = self.tokenizer.convert_tokens_to_ids(original_token)
                        log_probs = torch.log_softmax(logits[i, mask_pos], dim=-1)
                        token_log_prob = log_probs[original_token_id].item()
                        
                        total_neg_log_likelihood += (-token_log_prob)
                        valid_predictions += 1
                        
                    except Exception as e:
                        print(f"      è­¦å‘Š: æ‰¹æ¬¡{batch_idx}æ ·æœ¬{i}å¤„ç†å‡ºé”™: {e}")
                        continue
                
                # æ¸…ç†GPUå†…å­˜
                del batch_encoded, outputs, logits
                torch.cuda.empty_cache()
                
                if (batch_idx + 1) % 10 == 0:
                    print(f"   å¤„ç†è¿›åº¦: {batch_idx + 1}/{len(batches)} æ‰¹æ¬¡")
                
            except Exception as e:
                print(f"   æ‰¹æ¬¡{batch_idx}å¤„ç†å¤±è´¥: {e}")
                continue
        
        if valid_predictions == 0:
            return float('inf')
        
        # è®¡ç®—pseudo-PPL
        avg_neg_log_likelihood = total_neg_log_likelihood / valid_predictions
        pseudo_ppl = math.exp(avg_neg_log_likelihood)
        
        print(f"   âœ… è®¡ç®—å®Œæˆ: {valid_predictions} ä¸ªæœ‰æ•ˆé¢„æµ‹ï¼Œpseudo-PPL = {pseudo_ppl:.2f}")
        return pseudo_ppl
    
    def calculate_gpt2_ppl_gpu(self, text: str) -> float:
        """
        GPUä¼˜åŒ–çš„GPT-2å›°æƒ‘åº¦è®¡ç®—
        """
        if not text or not text.strip():
            return float('inf')
        
        # åˆ†å—å¤„ç†é•¿æ–‡æœ¬
        max_length = 512
        words = text.split()
        
        if len(words) > 400:  # å¤§æ¦‚å¯¹åº”512 tokens
            # åˆ†å—è®¡ç®—
            chunk_size = 400
            chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
            
            total_loss = 0.0
            total_chunks = 0
            
            print(f"   é•¿æ–‡æœ¬åˆ†ä¸º{len(chunks)}å—å¤„ç†")
            
            for chunk in chunks:
                inputs = self.tokenizer(chunk, return_tensors='pt', max_length=max_length, truncation=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.model(**inputs, labels=inputs['input_ids'])
                    loss = outputs.loss
                    total_loss += loss.item()
                    total_chunks += 1
                
                # æ¸…ç†å†…å­˜
                del inputs, outputs
                torch.cuda.empty_cache()
            
            avg_loss = total_loss / total_chunks
            perplexity = math.exp(avg_loss)
            
        else:
            # å•æ¬¡è®¡ç®—
            inputs = self.tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs, labels=inputs['input_ids'])
                loss = outputs.loss
                perplexity = torch.exp(loss).item()
            
            # æ¸…ç†å†…å­˜
            del inputs, outputs
            torch.cuda.empty_cache()
        
        print(f"   âœ… GPT-2 PPLè®¡ç®—å®Œæˆ: {perplexity:.2f}")
        return perplexity
    
    def calculate_ppl(self, text: str, **kwargs) -> float:
        """ç»Ÿä¸€PPLè®¡ç®—æ¥å£"""
        if self.method == 'bert_masked_lm':
            return self.calculate_bert_pseudo_ppl_gpu(text, **kwargs)
        elif self.method == 'gpt2_autoregressive':
            return self.calculate_gpt2_ppl_gpu(text)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {self.method}")

def load_baseline_stories() -> Dict[str, str]:
    """åŠ è½½baselineæ•…äº‹"""
    baseline_files = {
        'baseline_s1': '/Users/haha/Story/baseline_s1.md',
        'baseline_s2': '/Users/haha/Story/baseline_s2.md', 
        'baseline_s3': '/Users/haha/Story/baseline_s3.md'
    }
    
    stories = {}
    for name, file_path in baseline_files.items():
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            stories[name] = content
            word_count = len(content.split())
            print(f"âœ… åŠ è½½ {name}: {word_count} è¯")
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    return stories

def get_sample_experimental_stories(limit=3) -> Dict[str, str]:
    """è·å–å°‘é‡å®éªŒæ•…äº‹æ ·æœ¬è¿›è¡Œå¿«é€ŸéªŒè¯"""
    try:
        df = pd.read_csv('/Users/haha/Story/metrics_master_clean.csv')
        experimental_stories = {}
        
        # ç­›é€‰ébaselineæ•°æ®ï¼Œåªå–å°‘é‡æ ·æœ¬
        exp_df = df[df['is_baseline'] != 1].head(limit)
        
        for _, row in exp_df.iterrows():
            story_id = row['story_id']
            # å°è¯•ä»æ•°æ®è¾“å‡ºç›®å½•æ‰¾åˆ°å¯¹åº”æ–‡ä»¶
            possible_paths = [
                f"/Users/haha/Story/data/output/regression_test/{row.get('original_config_name', '')}/final_story.md",
                f"/Users/haha/Story/data/output/horror_test/{row.get('original_config_name', '')}/final_story.md", 
                f"/Users/haha/Story/data/output/romantic_test/{row.get('original_config_name', '')}/final_story.md"
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    with open(path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    experimental_stories[story_id] = content
                    word_count = len(content.split())
                    print(f"âœ… åŠ è½½å®éªŒæ•…äº‹ {story_id}: {word_count} è¯")
                    break
        
        return experimental_stories
        
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½å®éªŒæ•…äº‹: {e}")
        return {}

def quick_gpu_verification():
    """å¿«é€ŸGPUéªŒè¯ - åªæµ‹è¯•å…³é”®æ ·æœ¬"""
    print("ğŸš€ å¿«é€ŸGPU PPLéªŒè¯")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“– åŠ è½½æ•°æ®...")
    baseline_stories = load_baseline_stories()
    experimental_stories = get_sample_experimental_stories(limit=2)  # åªå–2ä¸ªå®éªŒæ ·æœ¬
    
    if not baseline_stories:
        print("âŒ æ— æ³•åŠ è½½baselineæ•°æ®ï¼Œåœæ­¢éªŒè¯")
        return
    
    # åªæµ‹è¯•BERTæ–¹æ³• (ä¸»è¦æ–¹æ³•)
    print(f"\nğŸ§® ä½¿ç”¨BERT masked LMæ–¹æ³•è¿›è¡ŒéªŒè¯")
    print("-" * 50)
    
    calculator = GPUOptimizedPPLCalculator(method='bert_masked_lm', batch_size=16)
    results = {'baseline': {}, 'experimental': {}}
    
    # è®¡ç®—baseline PPL
    print("\nğŸ“Š è®¡ç®—baseline PPL...")
    baseline_ppls = []
    for story_name, content in baseline_stories.items():
        print(f"\nğŸ”„ å¤„ç† {story_name}...")
        
        start_time = datetime.now()
        ppl = calculator.calculate_ppl(content, subsample_rate=4)
        end_time = datetime.now()
        
        duration = (end_time - start_time).total_seconds()
        results['baseline'][story_name] = ppl
        
        print(f"   âœ… ç»“æœ: PPL = {ppl:.2f} (è€—æ—¶: {duration:.1f}ç§’)")
        
        if ppl != float('inf'):
            baseline_ppls.append(ppl)
    
    # è®¡ç®—å®éªŒæ•…äº‹PPL
    experimental_ppls = []
    if experimental_stories:
        print("\nğŸ“Š è®¡ç®—å®éªŒæ•…äº‹PPL...")
        for story_id, content in experimental_stories.items():
            print(f"\nğŸ”„ å¤„ç† {story_id}...")
            
            start_time = datetime.now()
            ppl = calculator.calculate_ppl(content, subsample_rate=4)
            end_time = datetime.now()
            
            duration = (end_time - start_time).total_seconds()
            results['experimental'][story_id] = ppl
            
            print(f"   âœ… ç»“æœ: PPL = {ppl:.2f} (è€—æ—¶: {duration:.1f}ç§’)")
            
            if ppl != float('inf'):
                experimental_ppls.append(ppl)
    
    # ç«‹å³æ˜¾ç¤ºå…³é”®ç»“æœ
    print("\n" + "="*80)
    print("ğŸ¯ å…³é”®å‘ç°")
    print("="*80)
    
    if baseline_ppls:
        baseline_avg = np.mean(baseline_ppls)
        baseline_std = np.std(baseline_ppls)
        print(f"\nğŸ“Š Baseline PPLç»Ÿè®¡:")
        print(f"   å¹³å‡å€¼: {baseline_avg:.2f} Â± {baseline_std:.2f}")
        print(f"   èŒƒå›´: {min(baseline_ppls):.2f} - {max(baseline_ppls):.2f}")
        
        # å’Œä¹‹å‰æŠ¥å‘Šçš„11.5å¯¹æ¯”
        print(f"\nâ— å¯¹æ¯”ä¹‹å‰çš„æŠ¥å‘Š:")
        print(f"   ä¹‹å‰æŠ¥å‘Šçš„baseline PPL: 11.5")
        print(f"   é‡æ–°è®¡ç®—çš„baseline PPL: {baseline_avg:.2f}")
        
        if abs(baseline_avg - 11.5) > 2:
            print(f"   âš ï¸  å·®å¼‚æ˜¾è‘—: {abs(baseline_avg - 11.5):.1f}ç‚¹å·®å¼‚")
        else:
            print(f"   âœ… å·®å¼‚åˆç†: {abs(baseline_avg - 11.5):.1f}ç‚¹å·®å¼‚")
    
    if experimental_ppls:
        exp_avg = np.mean(experimental_ppls)
        exp_std = np.std(experimental_ppls)
        print(f"\nğŸ“Š å®éªŒæ ·æœ¬PPLç»Ÿè®¡:")
        print(f"   å¹³å‡å€¼: {exp_avg:.2f} Â± {exp_std:.2f}")
        print(f"   èŒƒå›´: {min(experimental_ppls):.2f} - {max(experimental_ppls):.2f}")
        
        # å’Œä¹‹å‰æŠ¥å‘Šçš„2.6å¯¹æ¯”
        print(f"\nâ— å¯¹æ¯”ä¹‹å‰çš„æŠ¥å‘Š:")
        print(f"   ä¹‹å‰æŠ¥å‘Šçš„å®éªŒPPL: 2.6")
        print(f"   é‡æ–°è®¡ç®—çš„å®éªŒPPL: {exp_avg:.2f}")
        
        if abs(exp_avg - 2.6) > 1:
            print(f"   âš ï¸  å·®å¼‚æ˜¾è‘—: {abs(exp_avg - 2.6):.1f}ç‚¹å·®å¼‚")
        else:
            print(f"   âœ… å·®å¼‚åˆç†: {abs(exp_avg - 2.6):.1f}ç‚¹å·®å¼‚")
    
    # è®¡ç®—æ¯”å€¼
    if baseline_ppls and experimental_ppls:
        ratio = baseline_avg / exp_avg
        print(f"\nğŸ“ˆ PPLæ¯”å€¼åˆ†æ:")
        print(f"   baseline/experimental = {ratio:.1f}")
        print(f"   ä¹‹å‰æŠ¥å‘Šçš„æ¯”å€¼: {11.5/2.6:.1f}")
        
        if abs(ratio - 11.5/2.6) > 1:
            print(f"   âš ï¸  æ¯”å€¼å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥")
        else:
            print(f"   âœ… æ¯”å€¼ç›¸å¯¹ä¸€è‡´")
    
    # ä¿å­˜å¿«é€ŸéªŒè¯ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    results_file = f'/Users/haha/Story/gpu_ppl_quick_verification_{timestamp}.json'
    
    full_results = {
        'method': 'bert_masked_lm',
        'timestamp': timestamp,
        'baseline_stats': {
            'mean': baseline_avg if baseline_ppls else None,
            'std': baseline_std if baseline_ppls else None,
            'values': results['baseline']
        },
        'experimental_stats': {
            'mean': exp_avg if experimental_ppls else None,
            'std': exp_std if experimental_ppls else None,
            'values': results['experimental']
        },
        'comparison': {
            'baseline_reported': 11.5,
            'experimental_reported': 2.6,
            'ratio_reported': 11.5/2.6,
            'ratio_calculated': ratio if (baseline_ppls and experimental_ppls) else None
        }
    }
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(full_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ å¿«é€ŸéªŒè¯ç»“æœä¿å­˜åˆ°: {results_file}")
    
    return full_results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ GPUä¼˜åŒ–PPLè®¡ç®—éªŒè¯")
    print("å¿«é€ŸéªŒè¯fluencyç»´åº¦æ•°æ®ä¸€è‡´æ€§é—®é¢˜")
    print("="*80)
    
    # æ£€æŸ¥GPU
    try:
        gpu_info = torch.cuda.get_device_name() if torch.cuda.is_available() else "æ— GPU"
        print(f"ğŸ–¥ï¸  ç¡¬ä»¶ç¯å¢ƒ: {gpu_info}")
        
        if not torch.cuda.is_available():
            print("\nâŒ éœ€è¦GPUç¯å¢ƒ!")
            print("è¯·åœ¨æœ‰NVIDIA GPUçš„æœºå™¨ä¸Šè¿è¡Œ")
            print("æˆ–ä½¿ç”¨CPUç‰ˆæœ¬: verify_ppl_calculation.py")
            return
    except:
        print("âŒ GPUç¯å¢ƒæ£€æŸ¥å¤±è´¥")
        return
    
    try:
        # è¿è¡Œå¿«é€ŸéªŒè¯
        results = quick_gpu_verification()
        
        if results:
            print("\nğŸ‰ GPUå¿«é€ŸéªŒè¯å®Œæˆï¼")
            print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
            print("1. å¦‚æœå‘ç°æ˜¾è‘—å·®å¼‚ï¼Œéœ€è¦ç»Ÿä¸€PPLè®¡ç®—æ–¹æ³•")
            print("2. é‡æ–°è®¡ç®—æ‰€æœ‰æ•…äº‹çš„PPLæ•°æ®")
            print("3. æ›´æ–°metrics_master_clean.csv")
        else:
            print("\nâŒ éªŒè¯å¤±è´¥")
            
    except Exception as e:
        print(f"\nğŸ’¥ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"\nğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")

if __name__ == "__main__":
    main()
