#!/usr/bin/env python3
"""
PPLè®¡ç®—ä¸€è‡´æ€§éªŒè¯è„šæœ¬
Step 0: æ•°æ®éªŒè¯ - ç¡®ä¿baselineå’Œä½ çš„ç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„æ–¹æ³•

å‘ç°çš„é—®é¢˜ï¼š
1. src/analysis/fluency_analyzer.py - ä½¿ç”¨BERT masked LM + è‡ªé€‚åº”å­é‡‡æ ·
2. gpu_baseline_fluency.py - ä½¿ç”¨ç®€åŒ–ç‰ˆGPT-2å›°æƒ‘åº¦è®¡ç®—  
3. ğŸ“Š_æŒ‡æ ‡è®¡ç®—æ–¹æ³•è¯¦ç»†è¯´æ˜.md - åˆæ˜¯å¦ä¸€ç§æ–¹æ³•

è¿™äº›ä¸åŒçš„è®¡ç®—æ–¹æ³•ä¼šå¯¼è‡´PPLå€¼å·®å¼‚å·¨å¤§ï¼
"""

import pandas as pd
import numpy as np
import torch
import json
import os
from pathlib import Path
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForMaskedLM, GPT2LMHeadModel, GPT2Tokenizer
import math
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

class UnifiedPPLCalculator:
    """ç»Ÿä¸€çš„PPLè®¡ç®—å™¨ - ç¡®ä¿æ‰€æœ‰ç³»ç»Ÿä½¿ç”¨ç›¸åŒæ–¹æ³•"""
    
    def __init__(self, method='bert_masked_lm'):
        """
        åˆå§‹åŒ–ç»Ÿä¸€PPLè®¡ç®—å™¨
        
        Args:
            method: 'bert_masked_lm' æˆ– 'gpt2_autoregressive'
        """
        self.method = method
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸ”§ åˆå§‹åŒ–ç»Ÿä¸€PPLè®¡ç®—å™¨ (æ–¹æ³•: {method}, è®¾å¤‡: {self.device})")
        
        if method == 'bert_masked_lm':
            self.model_name = 'bert-base-uncased'
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForMaskedLM.from_pretrained(self.model_name)
        elif method == 'gpt2_autoregressive':
            self.model_name = 'gpt2'
            self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)
            self.model = GPT2LMHeadModel.from_pretrained(self.model_name)
            self.tokenizer.pad_token = self.tokenizer.eos_token
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
        
        self.model.to(self.device)
        self.model.eval()
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {self.model_name}")
    
    def calculate_bert_pseudo_ppl(self, text: str, subsample_rate: int = 4) -> float:
        """
        ä½¿ç”¨BERT Masked LMè®¡ç®—pseudo-PPL
        è¿™æ˜¯åŸå§‹fluency_analyzerçš„ç»Ÿä¸€ç‰ˆæœ¬
        """
        if not text or not text.strip():
            return float('inf')
        
        # åˆ†è¯
        tokens = self.tokenizer.tokenize(text)
        if len(tokens) < 2:
            return float('inf')
        
        total_neg_log_likelihood = 0.0
        valid_predictions = 0
        
        # ç¡®ä¿å­é‡‡æ ·ç‡åˆç†
        actual_subsample_rate = max(1, min(subsample_rate, len(tokens)))
        
        print(f"   å¤„ç† {len(tokens)} tokensï¼Œå­é‡‡æ ·ç‡: {actual_subsample_rate}")
        
        for i in range(0, len(tokens), actual_subsample_rate):
            if i >= len(tokens):
                break
                
            # åˆ›å»ºmaskedç‰ˆæœ¬
            masked_tokens = tokens.copy()
            original_token = tokens[i]
            
            # è·³è¿‡ç‰¹æ®Štoken
            if original_token in ['[CLS]', '[SEP]', '[PAD]']:
                continue
                
            masked_tokens[i] = '[MASK]'
            
            # è½¬æ¢ä¸ºinput_ids
            input_text = ' '.join(masked_tokens)
            try:
                inputs = self.tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits = outputs.logits
                
                # æ‰¾åˆ°[MASK]ä½ç½®
                mask_token_id = self.tokenizer.mask_token_id
                mask_positions = (inputs['input_ids'] == mask_token_id).nonzero(as_tuple=True)[1]
                
                if len(mask_positions) > 0:
                    mask_pos = mask_positions[0]
                    
                    # è·å–åŸå§‹tokençš„ID
                    original_token_id = self.tokenizer.convert_tokens_to_ids(original_token)
                    
                    # è®¡ç®—logæ¦‚ç‡
                    log_probs = torch.log_softmax(logits[0, mask_pos], dim=-1)
                    token_log_prob = log_probs[original_token_id].item()
                    
                    total_neg_log_likelihood += (-token_log_prob)
                    valid_predictions += 1
                    
            except Exception as e:
                print(f"      è­¦å‘Š: å¤„ç†ä½ç½® {i} æ—¶å‡ºé”™: {e}")
                continue
        
        if valid_predictions == 0:
            return float('inf')
        
        # è®¡ç®—pseudo-PPL
        avg_neg_log_likelihood = total_neg_log_likelihood / valid_predictions
        pseudo_ppl = math.exp(avg_neg_log_likelihood)
        
        print(f"   è®¡ç®—å®Œæˆ: {valid_predictions} ä¸ªæœ‰æ•ˆé¢„æµ‹ï¼Œpseudo-PPL = {pseudo_ppl:.2f}")
        return pseudo_ppl
    
    def calculate_gpt2_ppl(self, text: str) -> float:
        """
        ä½¿ç”¨GPT-2è®¡ç®—æ ‡å‡†å›°æƒ‘åº¦
        """
        if not text or not text.strip():
            return float('inf')
        
        # ç¼–ç æ–‡æœ¬
        inputs = self.tokenizer(text, return_tensors='pt', max_length=512, truncation=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs, labels=inputs['input_ids'])
            loss = outputs.loss
            perplexity = torch.exp(loss).item()
        
        print(f"   GPT-2 PPLè®¡ç®—å®Œæˆ: {perplexity:.2f}")
        return perplexity
    
    def calculate_ppl(self, text: str, **kwargs) -> float:
        """ç»Ÿä¸€PPLè®¡ç®—æ¥å£"""
        if self.method == 'bert_masked_lm':
            return self.calculate_bert_pseudo_ppl(text, **kwargs)
        elif self.method == 'gpt2_autoregressive':
            return self.calculate_gpt2_ppl(text)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {self.method}")

def load_baseline_stories() -> Dict[str, str]:
    """åŠ è½½baselineæ•…äº‹"""
    baseline_files = {
        'baseline_s1': '/Users/haha/Story/baseline_s1.md',
        'baseline_s2': '/Users/haha/Story/baseline_s2.md', 
        'baseline_s3': '/Users/haha/Story/baseline_s3.md'
    }
    
    stories = {}
    for name, file_path in baseline_files.items():
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            stories[name] = content
            print(f"âœ… åŠ è½½ {name}: {len(content.split())} è¯")
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    return stories

def load_experimental_stories() -> Dict[str, str]:
    """åŠ è½½å®éªŒç³»ç»Ÿç”Ÿæˆçš„æ•…äº‹"""
    # ä»metrics_master_clean.csvè·å–å®éªŒæ•…äº‹ä¿¡æ¯
    try:
        df = pd.read_csv('/Users/haha/Story/metrics_master_clean.csv')
        experimental_stories = {}
        
        # ç­›é€‰ébaselineæ•°æ®
        exp_df = df[df['is_baseline'] != 1].head(5)  # åªå–å‰5ä¸ªä½œä¸ºæ ·æœ¬éªŒè¯
        
        for _, row in exp_df.iterrows():
            story_id = row['story_id']
            # å°è¯•ä»æ•°æ®è¾“å‡ºç›®å½•æ‰¾åˆ°å¯¹åº”æ–‡ä»¶
            possible_paths = [
                f"/Users/haha/Story/data/output/regression_test/{row.get('original_config_name', '')}/final_story.md",
                f"/Users/haha/Story/data/output/horror_test/{row.get('original_config_name', '')}/final_story.md", 
                f"/Users/haha/Story/data/output/romantic_test/{row.get('original_config_name', '')}/final_story.md"
            ]
            
            for path in possible_paths:
                if os.path.exists(path):
                    with open(path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    experimental_stories[story_id] = content
                    print(f"âœ… åŠ è½½å®éªŒæ•…äº‹ {story_id}: {len(content.split())} è¯")
                    break
        
        return experimental_stories
        
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½å®éªŒæ•…äº‹: {e}")
        return {}

def verify_ppl_calculation():
    """éªŒè¯PPLè®¡ç®—çš„ä¸€è‡´æ€§"""
    print("ğŸ” PPLè®¡ç®—ä¸€è‡´æ€§éªŒè¯")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    print("\nğŸ“– åŠ è½½æ•°æ®...")
    baseline_stories = load_baseline_stories()
    experimental_stories = load_experimental_stories()
    
    if not baseline_stories:
        print("âŒ æ— æ³•åŠ è½½baselineæ•°æ®ï¼Œåœæ­¢éªŒè¯")
        return
    
    # æµ‹è¯•ä¸¤ç§è®¡ç®—æ–¹æ³•
    methods_to_test = ['bert_masked_lm', 'gpt2_autoregressive']
    results = {}
    
    for method in methods_to_test:
        print(f"\nğŸ§® æµ‹è¯•æ–¹æ³•: {method}")
        print("-" * 50)
        
        calculator = UnifiedPPLCalculator(method=method)
        method_results = {'baseline': {}, 'experimental': {}}
        
        # è®¡ç®—baseline PPL
        print("\nğŸ“Š è®¡ç®—baseline PPL...")
        for story_name, content in baseline_stories.items():
            print(f"\n   å¤„ç† {story_name}...")
            ppl = calculator.calculate_ppl(content)
            method_results['baseline'][story_name] = ppl
            print(f"   ç»“æœ: PPL = {ppl:.2f}")
        
        # è®¡ç®—å®éªŒæ•…äº‹PPL
        if experimental_stories:
            print("\nğŸ“Š è®¡ç®—å®éªŒæ•…äº‹PPL...")
            for story_id, content in experimental_stories.items():
                print(f"\n   å¤„ç† {story_id}...")
                ppl = calculator.calculate_ppl(content)
                method_results['experimental'][story_id] = ppl
                print(f"   ç»“æœ: PPL = {ppl:.2f}")
        
        results[method] = method_results
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    results_file = f'/Users/haha/Story/ppl_verification_results_{timestamp}.json'
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ç»“æœä¿å­˜åˆ°: {results_file}")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_comparison_report(results, timestamp)
    
    return results

def generate_comparison_report(results: Dict, timestamp: str):
    """ç”ŸæˆPPLå¯¹æ¯”æŠ¥å‘Š"""
    
    report_lines = [
        "# PPLè®¡ç®—æ–¹æ³•å¯¹æ¯”éªŒè¯æŠ¥å‘Š",
        f"\n**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"**éªŒè¯ç›®çš„**: è§£å†³baseline PPL 11.5 vs å®éªŒç³»ç»Ÿ PPL 2.6å·®å¼‚è¿‡å¤§çš„é—®é¢˜",
        "\n## ğŸ” é—®é¢˜èƒŒæ™¯",
        "\nå‘ç°ä¸åŒç³»ç»Ÿä½¿ç”¨äº†ä¸åŒçš„PPLè®¡ç®—æ–¹æ³•ï¼š",
        "- `src/analysis/fluency_analyzer.py`: BERT masked LM + è‡ªé€‚åº”å­é‡‡æ ·", 
        "- `gpu_baseline_fluency.py`: GPT-2æ ‡å‡†å›°æƒ‘åº¦",
        "- è¿™å¯¼è‡´æ•°å€¼å·®å¼‚å·¨å¤§ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆå¯¹æ¯”",
        "\n## ğŸ“Š éªŒè¯ç»“æœ",
    ]
    
    for method, method_results in results.items():
        report_lines.append(f"\n### æ–¹æ³•: {method}")
        
        # Baselineç»“æœ
        report_lines.append("\n#### Baseline PPL:")
        baseline_ppls = []
        for story_name, ppl in method_results['baseline'].items():
            report_lines.append(f"- {story_name}: {ppl:.2f}")
            if ppl != float('inf'):
                baseline_ppls.append(ppl)
        
        if baseline_ppls:
            avg_baseline = np.mean(baseline_ppls)
            report_lines.append(f"- **å¹³å‡**: {avg_baseline:.2f}")
        
        # å®éªŒç»“æœ
        if method_results['experimental']:
            report_lines.append("\n#### å®éªŒæ•…äº‹PPL:")
            exp_ppls = []
            for story_id, ppl in method_results['experimental'].items():
                report_lines.append(f"- {story_id}: {ppl:.2f}")
                if ppl != float('inf'):
                    exp_ppls.append(ppl)
            
            if exp_ppls:
                avg_exp = np.mean(exp_ppls)
                report_lines.append(f"- **å¹³å‡**: {avg_exp:.2f}")
                
                # è®¡ç®—å·®å¼‚æ¯”
                if baseline_ppls:
                    ratio = avg_baseline / avg_exp
                    report_lines.append(f"- **baseline/å®éªŒæ¯”å€¼**: {ratio:.1f}")
    
    # æ·»åŠ ç»“è®ºå’Œå»ºè®®
    report_lines.extend([
        "\n## ğŸ¯ ç»“è®ºå’Œå»ºè®®",
        "\n### å‘ç°çš„é—®é¢˜:",
        "1. **è®¡ç®—æ–¹æ³•ä¸ç»Ÿä¸€**: ä¸åŒç³»ç»Ÿä½¿ç”¨ä¸åŒçš„PPLè®¡ç®—æ–¹æ³•",
        "2. **æ•°å€¼å·®å¼‚å·¨å¤§**: æ–¹æ³•å·®å¼‚å¯¼è‡´PPLå€¼æ— æ³•ç›´æ¥å¯¹æ¯”",
        "3. **ç¼ºä¹æ ‡å‡†åŒ–**: æ²¡æœ‰ç»Ÿä¸€çš„è®¡ç®—æ ‡å‡†å’Œå‚è€ƒå®ç°",
        
        "\n### ä¿®æ­£æ–¹æ¡ˆ:",
        "1. **ç»Ÿä¸€è®¡ç®—æ–¹æ³•**: é€‰æ‹©ä¸€ç§æ–¹æ³•ä½œä¸ºæ ‡å‡†ï¼Œé‡æ–°è®¡ç®—æ‰€æœ‰æ•°æ®",
        "2. **é‡æ–°éªŒè¯baseline**: ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•é‡æ–°è®¡ç®—baseline PPL",
        "3. **æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥**: ç¡®ä¿æ‰€æœ‰ç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„æ¨¡å‹å’Œå‚æ•°",
        "4. **å»ºç«‹éªŒè¯æµç¨‹**: åˆ›å»ºPPLè®¡ç®—çš„æ ‡å‡†éªŒè¯æµç¨‹",
        
        "\n### æ¨èæ–¹æ³•:",
        "åŸºäºæµ‹è¯•ç»“æœï¼Œæ¨èä½¿ç”¨ **BERT masked LM** æ–¹æ³•ä½œä¸ºæ ‡å‡†ï¼š",
        "- æ›´é€‚åˆæ–‡æœ¬è´¨é‡è¯„ä¼°",
        "- è®¡ç®—ç›¸å¯¹ç¨³å®š",  
        "- å·²æœ‰å®Œæ•´å®ç°",
        
        "\n### ä¸‹ä¸€æ­¥è¡ŒåŠ¨:",
        "1. ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•é‡æ–°è®¡ç®—æ‰€æœ‰æ•…äº‹çš„PPL",
        "2. æ›´æ–°metrics_master_clean.csvä¸­çš„PPLæ•°æ®", 
        "3. é‡æ–°ç”Ÿæˆfluencyå¯¹æ¯”æŠ¥å‘Š",
        "4. å»ºç«‹PPLè®¡ç®—çš„æ ‡å‡†æ–‡æ¡£"
    ])
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f'/Users/haha/Story/ppl_verification_report_{timestamp}.md'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"\nğŸ“„ éªŒè¯æŠ¥å‘Šä¿å­˜åˆ°: {report_file}")
    
    # æ‰“å°å…³é”®ç»“è®º
    print("\n" + "="*80)
    print("ğŸ¯ å…³é”®å‘ç°")
    print("="*80)
    
    if len(results) >= 2:
        methods = list(results.keys())
        method1, method2 = methods[0], methods[1]
        
        baseline1 = list(results[method1]['baseline'].values())
        baseline2 = list(results[method2]['baseline'].values())
        
        if baseline1 and baseline2:
            avg1 = np.mean([x for x in baseline1 if x != float('inf')])
            avg2 = np.mean([x for x in baseline2 if x != float('inf')])
            
            print(f"ğŸ“Š Baselineå¹³å‡PPL:")
            print(f"   {method1}: {avg1:.2f}")
            print(f"   {method2}: {avg2:.2f}")
            print(f"   å·®å¼‚å€æ•°: {max(avg1, avg2) / min(avg1, avg2):.1f}x")
            
            if abs(avg1 - avg2) > 5:
                print("\nâš ï¸  å‘ç°ä¸¥é‡ä¸ä¸€è‡´ï¼å¿…é¡»ç»Ÿä¸€è®¡ç®—æ–¹æ³•ï¼")
            else:
                print("\nâœ… è®¡ç®—ç»“æœç›¸å¯¹ä¸€è‡´")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ PPLè®¡ç®—ä¸€è‡´æ€§éªŒè¯è„šæœ¬")
    print("è§£å†³fluencyç»´åº¦æ•°æ®éªŒè¯é—®é¢˜")
    print("="*80)
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"ğŸš€ GPUå¯ç”¨: {torch.cuda.get_device_name()}")
    else:
        print("ğŸ’» ä½¿ç”¨CPUè®¡ç®— (ä¼šæ¯”è¾ƒæ…¢)")
    
    try:
        # è¿è¡ŒéªŒè¯
        results = verify_ppl_calculation()
        
        if results:
            print("\nğŸ‰ éªŒè¯å®Œæˆï¼")
            print("è¯·æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šäº†è§£è¯¦ç»†ç»“æœå’Œå»ºè®®")
        else:
            print("\nâŒ éªŒè¯å¤±è´¥")
            
    except Exception as e:
        print(f"\nğŸ’¥ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
