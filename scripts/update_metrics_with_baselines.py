#!/usr/bin/env python3
"""
æ›´æ–°metrics_master_clean.csvï¼Œæ•´åˆæ‰€æœ‰baselineåˆ†æç»“æœ
"""

import pandas as pd
import numpy as np
import json
import os
import re
from pathlib import Path

class BaselineMetricsUpdater:
    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®æ›´æ–°å™¨"""
        self.baseline_analysis_dir = '/Users/haha/Story/baseline_analysis_results'
        self.metrics_csv_path = '/Users/haha/Story/metrics_master_clean.csv'
        self.backup_csv_path = '/Users/haha/Story/metrics_master_clean_backup.csv'
        
        # baselineæ˜ å°„ - ä¿®æ­£å‘½åé¿å…ä¸åŸæœ‰baselineæ··æ·†
        self.baseline_mapping = {
            'baseline_s1': {
                'story_id': 'simple_baseline_s1_Tbaseline_sbaseline',
                'original_config_name': 'simple_baseline_s1',
                'test_type': 'simple_baseline_analysis',
                'genre': 'baseline',
                'structure': 'baseline', 
                'temperature': 'baseline',
                'seed': 'baseline',
                'is_baseline': 1
            },
            'baseline_s2': {
                'story_id': 'simple_baseline_s2_Tbaseline_sbaseline', 
                'original_config_name': 'simple_baseline_s2',
                'test_type': 'simple_baseline_analysis',
                'genre': 'baseline',
                'structure': 'baseline',
                'temperature': 'baseline',
                'seed': 'baseline',
                'is_baseline': 1
            },
            'baseline_s3': {
                'story_id': 'simple_baseline_s3_Tbaseline_sbaseline',
                'original_config_name': 'simple_baseline_s3', 
                'test_type': 'simple_baseline_analysis',
                'genre': 'baseline',
                'structure': 'baseline',
                'temperature': 'baseline',
                'seed': 'baseline',
                'is_baseline': 1
            }
        }
    
    def backup_original_csv(self):
        """å¤‡ä»½åŸå§‹CSVæ–‡ä»¶"""
        if os.path.exists(self.metrics_csv_path):
            import shutil
            shutil.copy2(self.metrics_csv_path, self.backup_csv_path)
            print(f"âœ… å·²å¤‡ä»½åŸå§‹CSVåˆ°: {self.backup_csv_path}")
    
    def load_baseline_analysis_results(self, baseline_name):
        """åŠ è½½å•ä¸ªbaselineçš„åˆ†æç»“æœ"""
        baseline_dir = f"{self.baseline_analysis_dir}/{baseline_name}"
        
        if not os.path.exists(baseline_dir):
            print(f"âŒ æœªæ‰¾åˆ°baselineåˆ†æç»“æœ: {baseline_dir}")
            return None
        
        results = {}
        
        # 1. åŠ è½½ç»¼åˆåˆ†æç»“æœ
        comprehensive_file = f"{baseline_dir}/comprehensive_analysis.json"
        if os.path.exists(comprehensive_file):
            with open(comprehensive_file, 'r', encoding='utf-8') as f:
                results['comprehensive'] = json.load(f)
        
        # 2. åŠ è½½å¤šæ ·æ€§ç»“æœ
        diversity_dir = f"diversity_results_baseline_{baseline_name}"
        if os.path.exists(diversity_dir):
            # æŸ¥æ‰¾ç»“æœæ–‡ä»¶
            individual_file = f"{diversity_dir}/individual_results.json"
            group_file = f"{diversity_dir}/group_results.json"
            
            if os.path.exists(individual_file):
                with open(individual_file, 'r', encoding='utf-8') as f:
                    results['diversity_individual'] = json.load(f)
            
            if os.path.exists(group_file):
                with open(group_file, 'r', encoding='utf-8') as f:
                    results['diversity_group'] = json.load(f)
        
        return results
    
    def extract_metrics_from_analysis(self, baseline_name, analysis_results):
        """ä»åˆ†æç»“æœä¸­æå–æŒ‡æ ‡"""
        config = self.baseline_mapping[baseline_name]
        metrics = config.copy()  # åŸºç¡€é…ç½®
        
        if not analysis_results:
            return metrics
        
        try:
            # 1. åŸºç¡€æ–‡æœ¬ç‰¹å¾
            if 'comprehensive' in analysis_results and 'text_features' in analysis_results['comprehensive']:
                text_features = analysis_results['comprehensive']['text_features']
                metrics.update({
                    'total_words': text_features.get('total_words', 0),
                    'total_sentences': text_features.get('total_sentences', 0),
                    'chapter_count': text_features.get('chapter_count', 0)
                })
            
            # 2. å¤šæ ·æ€§æŒ‡æ ‡
            if 'comprehensive' in analysis_results and 'diversity' in analysis_results['comprehensive']:
                diversity_data = analysis_results['comprehensive']['diversity']
                
                # ä¸ªä½“å¤šæ ·æ€§æ•°æ®
                if 'individual' in diversity_data and diversity_data['individual']:
                    first_key = list(diversity_data['individual'].keys())[0]
                    div_result = diversity_data['individual'][first_key]
                    
                    metrics.update({
                        'distinct_avg': div_result.get('distinct_avg', np.nan),
                        'distinct_score': div_result.get('distinct_score', np.nan)
                    })
                
                # ç»„å¤šæ ·æ€§æ•°æ®
                if 'group' in diversity_data and diversity_data['group']:
                    first_key = list(diversity_data['group'].keys())[0]
                    group_result = diversity_data['group'][first_key]
                    
                    metrics.update({
                        'diversity_group_score': group_result.get('diversity_score', np.nan),
                        'self_bleu_group': group_result.get('self_bleu_group', np.nan),
                        'one_minus_self_bleu': 1 - group_result.get('self_bleu_group', 0) if group_result.get('self_bleu_group') is not None else np.nan,
                        'alpha_value': group_result.get('alpha', np.nan),
                        'alpha_genre': group_result.get('alpha', np.nan)  # é€šå¸¸ç›¸åŒ
                    })
            
            # 3. æµç•…æ€§æŒ‡æ ‡
            if 'comprehensive' in analysis_results and 'fluency' in analysis_results['comprehensive']:
                fluency = analysis_results['comprehensive']['fluency']
                metrics.update({
                    'pseudo_ppl': fluency.get('pseudo_ppl', np.nan),
                    'err_per_100w': fluency.get('err_per_100w', np.nan),
                    'error_count': fluency.get('error_count', 0),
                    'fluency_word_count': fluency.get('word_count', 0)
                })
            
            # 4. è¿è´¯æ€§æŒ‡æ ‡
            if 'comprehensive' in analysis_results and 'coherence' in analysis_results['comprehensive']:
                coherence = analysis_results['comprehensive']['coherence']
                
                # å°è¯•ä»detailed_analysisä¸­æå–
                if 'detailed_analysis' in coherence and 'basic_statistics' in coherence['detailed_analysis']:
                    stats = coherence['detailed_analysis']['basic_statistics']
                    
                    metrics.update({
                        'avg_coherence': stats.get('average_coherence', np.nan),
                        'coherence_std': stats.get('coherence_std', np.nan),
                        'coherence_median': stats.get('coherence_median', np.nan),
                        'coherence_max': stats.get('max_coherence', np.nan),
                        'coherence_min': stats.get('min_coherence', np.nan)
                    })
                
                # ä»HRED_coherence_evaluationä¸­æå–å¥å­æ•°
                if 'HRED_coherence_evaluation' in coherence:
                    hred = coherence['HRED_coherence_evaluation']
                    metrics.update({
                        'coherence_sentence_count': hred.get('total_sentences', 0)
                    })
                
                # è®¡ç®—é«˜ä½è¿è´¯æ€§æ®µè½æ•°
                if 'detailed_analysis' in coherence:
                    detailed = coherence['detailed_analysis']
                    
                    # ä½è¿è´¯æ€§æ–­ç‚¹æ•°
                    low_coherence_count = len(detailed.get('coherence_breakpoints', []))
                    metrics['low_coherence_points'] = low_coherence_count
                    
                    # é«˜è¿è´¯æ€§æ®µè½æ•°
                    high_coherence_count = len(detailed.get('high_coherence_segments', []))
                    metrics['high_coherence_segments'] = high_coherence_count
            
            # 5. æƒ…æ„Ÿåˆ†ææŒ‡æ ‡
            if 'comprehensive' in analysis_results and 'emotion' in analysis_results['comprehensive']:
                emotion = analysis_results['comprehensive']['emotion']
                
                # RoBERTaæŒ‡æ ‡ (ä»primary_analysis)
                if 'primary_analysis' in emotion:
                    roberta = emotion['primary_analysis']
                    
                    # è®¡ç®—å¹³å‡åˆ†æ•°
                    if 'scores' in roberta and roberta['scores']:
                        roberta_scores = roberta['scores']
                        roberta_avg = sum(roberta_scores) / len(roberta_scores)
                        roberta_std = np.std(roberta_scores)
                        
                        metrics.update({
                            'roberta_avg_score': roberta_avg,
                            'roberta_std': roberta_std
                        })
                        
                        # ç« èŠ‚åˆ†æ•°å­—ç¬¦ä¸²
                        scores_str = ','.join([str(round(s, 4)) for s in roberta_scores])
                        metrics['roberta_scores_str'] = scores_str
                    
                    # Reaganåˆ†ç±»
                    if 'reagan_classification' in roberta:
                        reagan = roberta['reagan_classification']
                        metrics['reagan_classification'] = reagan.get('best_match', '')
                
                # LabMTæŒ‡æ ‡ (ä»validation_analysis)
                if 'validation_analysis' in emotion:
                    labmt = emotion['validation_analysis']
                    
                    if 'scores' in labmt and labmt['scores']:
                        labmt_scores = labmt['scores']
                        labmt_std = np.std(labmt_scores)
                        metrics['labmt_std'] = labmt_std
                        
                        # ç« èŠ‚åˆ†æ•°å­—ç¬¦ä¸²
                        scores_str = ','.join([str(round(s, 4)) for s in labmt_scores])
                        metrics['labmt_scores_str'] = scores_str
                
                # ç›¸å…³æ€§å’Œä¸€è‡´æ€§æŒ‡æ ‡
                if 'correlation_analysis' in emotion:
                    corr = emotion['correlation_analysis']
                    
                    # Pearsonç›¸å…³æ€§
                    if 'pearson_correlation' in corr:
                        pearson = corr['pearson_correlation']
                        if isinstance(pearson, dict) and 'r' in pearson:
                            metrics['correlation_coefficient'] = pearson['r']
                        else:
                            metrics['correlation_coefficient'] = pearson
                    
                    # æ–¹å‘ä¸€è‡´æ€§
                    metrics['direction_consistency'] = corr.get('direction_consistency', False)
                    
                    # åˆ†ç±»ä¸€è‡´æ€§ï¼ˆé€šè¿‡æ¯”è¾ƒprimaryå’Œvalidationçš„åˆ†ç±»ï¼‰
                    primary_class = ''
                    validation_class = ''
                    if 'primary_analysis' in emotion and 'reagan_classification' in emotion['primary_analysis']:
                        primary_class = emotion['primary_analysis']['reagan_classification'].get('best_match', '')
                    if 'validation_analysis' in emotion and 'reagan_classification' in emotion['validation_analysis']:
                        validation_class = emotion['validation_analysis']['reagan_classification'].get('best_match', '')
                    
                    metrics['classification_agreement'] = (primary_class == validation_class) if primary_class and validation_class else False
                
                # ä¸»è¦åˆ†æ­§æ•°ï¼ˆæš‚æ—¶è®¾ä¸º0ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è®¡ç®—ï¼‰
                metrics['major_disagreements'] = 0
            
            # 6. ç»“æ„åˆ†ææŒ‡æ ‡
            if 'comprehensive' in analysis_results and 'structure' in analysis_results['comprehensive']:
                structure = analysis_results['comprehensive']['structure']
                
                # TPè¦†ç›–ç‡ (ä»Papalampidi_detailed_results)
                if 'Papalampidi_detailed_results' in structure:
                    papa = structure['Papalampidi_detailed_results']
                    if 'turning_points' in papa:
                        tp_count = len(papa['turning_points'])
                        metrics.update({
                            'tp_coverage': f'{tp_count}/5',
                            'tp_m': tp_count,
                            'tp_n': 5,
                            'tp_completion_rate': tp_count / 5
                        })
                
                # Liå‡½æ•°å¤šæ ·æ€§ (ä»Li_detailed_results)
                if 'Li_detailed_results' in structure:
                    li_data = structure['Li_detailed_results']
                    if li_data:
                        unique_functions = len(set(li_data.values()))
                        metrics['li_function_diversity'] = unique_functions
                
                # äº‹ä»¶ç»Ÿè®¡ (ä»event_list)
                if 'event_list' in structure:
                    metrics['total_events'] = len(structure['event_list'])
            
            # 7. è®¾ç½®é»˜è®¤å€¼ï¼ˆå¦‚æœæ²¡æœ‰å®é™…æ•°æ®ï¼‰
            default_values = {
                'diversity_score_seed': np.nan,
                'wall_time_sec': np.nan,
                'peak_mem_mb': np.nan,
                'tokens_total': np.nan,
                'cost_usd': np.nan,
                'emotion_correlation': np.nan
            }
            
            for key, default_val in default_values.items():
                if key not in metrics:
                    metrics[key] = default_val
            
            return metrics
            
        except Exception as e:
            print(f"âŒ æå–{baseline_name}æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return config  # è¿”å›åŸºç¡€é…ç½®
    
    def update_metrics_csv(self):
        """æ›´æ–°metrics CSVæ–‡ä»¶"""
        print("ğŸ”„ å¼€å§‹æ›´æ–°metrics_master_clean.csv...")
        
        # 1. å¤‡ä»½åŸæ–‡ä»¶
        self.backup_original_csv()
        
        # 2. åŠ è½½ç°æœ‰æ•°æ®
        if os.path.exists(self.metrics_csv_path):
            df_existing = pd.read_csv(self.metrics_csv_path)
            print(f"ğŸ“Š åŠ è½½ç°æœ‰æ•°æ®: {len(df_existing)} è¡Œ")
            
            # ä¿ç•™åŸæœ‰æ•°æ®ï¼Œåªæ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„story_id
            baseline_story_ids = [config['story_id'] for config in self.baseline_mapping.values()]
            existing_baseline_count = len(df_existing[df_existing['story_id'].isin(baseline_story_ids)])
            if existing_baseline_count > 0:
                print(f"âš ï¸  å‘ç° {existing_baseline_count} ä¸ªå·²å­˜åœ¨çš„baselineè®°å½•ï¼Œå°†è¿½åŠ æ–°æ•°æ®")
            else:
                print("âœ… æ²¡æœ‰å‘ç°é‡å¤çš„baselineè®°å½•")
        else:
            df_existing = pd.DataFrame()
            print("ğŸ“Š åˆ›å»ºæ–°çš„DataFrame")
        
        # 3. å¤„ç†æ¯ä¸ªbaseline
        new_rows = []
        
        for baseline_name in self.baseline_mapping.keys():
            print(f"\nğŸ“ å¤„ç† {baseline_name}...")
            
            # åŠ è½½åˆ†æç»“æœ
            analysis_results = self.load_baseline_analysis_results(baseline_name)
            
            # æå–æŒ‡æ ‡
            metrics = self.extract_metrics_from_analysis(baseline_name, analysis_results)
            new_rows.append(metrics)
            
            print(f"   âœ… æŒ‡æ ‡æå–å®Œæˆ: {len([k for k, v in metrics.items() if v is not None and v != ''])} ä¸ªæœ‰æ•ˆæŒ‡æ ‡")
        
        # 4. åˆ›å»ºæ–°çš„DataFrame
        df_new = pd.DataFrame(new_rows)
        
        # 5. åˆå¹¶æ•°æ®
        if len(df_existing) > 0:
            df_final = pd.concat([df_existing, df_new], ignore_index=True)
        else:
            df_final = df_new
        
        # 6. é‡æ–°æ’åºåˆ—ï¼ˆä¿æŒä¸åŸCSVç›¸åŒçš„åˆ—é¡ºåºï¼‰
        if os.path.exists(self.metrics_csv_path):
            original_df = pd.read_csv(self.metrics_csv_path)
            original_columns = original_df.columns.tolist()
            
            # ç¡®ä¿æ‰€æœ‰æ–°åˆ—éƒ½åŒ…å«åœ¨å†…
            all_columns = original_columns + [col for col in df_final.columns if col not in original_columns]
            df_final = df_final.reindex(columns=all_columns)
        
        # 7. ä¿å­˜æ›´æ–°åçš„CSV
        df_final.to_csv(self.metrics_csv_path, index=False)
        
        print(f"\nğŸ‰ metrics_master_clean.csv æ›´æ–°å®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆæ•°æ®: {len(df_final)} è¡Œ x {len(df_final.columns)} åˆ—")
        print(f"ğŸ“ˆ æ–°å¢baseline: {len(new_rows)} ä¸ª")
        print(f"ğŸ’¾ æ–‡ä»¶ä¿å­˜åˆ°: {self.metrics_csv_path}")
        print(f"ğŸ—‚ï¸  å¤‡ä»½æ–‡ä»¶: {self.backup_csv_path}")
        
        return df_final

def main():
    """ä¸»å‡½æ•°"""
    updater = BaselineMetricsUpdater()
    df = updater.update_metrics_csv()
    
    # æ˜¾ç¤ºæ–°å¢çš„baselineæ•°æ®æ¦‚è§ˆ
    print("\nğŸ“‹ æ–°å¢baselineæ•°æ®æ¦‚è§ˆ:")
    print("-" * 60)
    baseline_story_ids = [config['story_id'] for config in updater.baseline_mapping.values()]
    baseline_data = df[df['story_id'].isin(baseline_story_ids)]
    
    for _, row in baseline_data.iterrows():
        print(f"â€¢ {row['story_id']}")
        print(f"  - æ€»è¯æ•°: {row.get('total_words', 'N/A')}")
        print(f"  - ç« èŠ‚æ•°: {row.get('chapter_count', 'N/A')}")
        print(f"  - æµç•…æ€§PPL: {row.get('pseudo_ppl', 'N/A')}")
        print(f"  - å¹³å‡è¿è´¯æ€§: {row.get('avg_coherence', 'N/A')}")
        print()
    
    return df

if __name__ == "__main__":
    main()
