"""
æ€§èƒ½æ•°æ®å¯è§†åŒ–å’Œè¶‹åŠ¿åˆ†æå·¥å…·
ç”¨äºç”Ÿæˆæ€§èƒ½æŠ¥å‘Šçš„å›¾è¡¨ã€è¶‹åŠ¿åˆ†æå’Œå¤æ‚åº¦æ›²çº¿æ‹Ÿåˆ
"""

import os
import json
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.optimize import curve_fit
from scipy.stats import pearsonr
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import datetime
from pathlib import Path

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class PerformanceVisualizer:
    """æ€§èƒ½æ•°æ®å¯è§†åŒ–å™¨"""
    
    def __init__(self, output_dir: str = "output"):
        self.output_dir = output_dir
        self.reports = []
        
    def load_performance_reports(self, data_dir: str = "data/output") -> int:
        """åŠ è½½æ‰€æœ‰æ€§èƒ½æŠ¥å‘Š"""
        self.reports = []
        
        if not os.path.exists(data_dir):
            print(f"âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return 0
            
        report_count = 0
        for root, dirs, files in os.walk(data_dir):
            for file in files:
                if file.startswith("performance_analysis_") and file.endswith(".json"):
                    filepath = os.path.join(root, file)
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            report = json.load(f)
                        self.reports.append({
                            'data': report,
                            'filepath': filepath,
                            'filename': file
                        })
                        report_count += 1
                    except Exception as e:
                        print(f"âŒ åŠ è½½æŠ¥å‘Šå¤±è´¥ {file}: {e}")
        
        print(f"âœ… æˆåŠŸåŠ è½½ {report_count} ä¸ªæ€§èƒ½æŠ¥å‘Š")
        return report_count
    
    def extract_data_for_analysis(self) -> pd.DataFrame:
        """æå–ç”¨äºåˆ†æçš„ç»“æ„åŒ–æ•°æ®"""
        data_rows = []
        
        for report in self.reports:
            report_data = report['data']
            
            # åŸºæœ¬ä¿¡æ¯
            metadata = report_data.get('metadata', {})
            text_features = report_data.get('text_features', {})
            complexity = report_data.get('complexity_analysis', {})
            stage_performance = report_data.get('stage_performance', {})
            
            row = {
                'task_name': metadata.get('task_name', ''),
                'timestamp': metadata.get('analysis_timestamp', ''),
                'total_time': metadata.get('total_execution_time', 0),
                
                # æ–‡æœ¬ç‰¹å¾
                'word_count': text_features.get('total_word_count', 0),
                'char_count': text_features.get('total_char_count', 0),
                'sentence_count': text_features.get('total_sentence_count', 0),
                'chapter_count': text_features.get('chapter_count', 0),
                'avg_chapter_length': text_features.get('avg_chapter_length', 0),
                'avg_sentence_length': text_features.get('avg_sentence_length', 0),
                
                # æ•ˆç‡æŒ‡æ ‡
                'words_per_second': complexity.get('efficiency_metrics', {}).get('words_per_second', 0),
                'chars_per_second': complexity.get('efficiency_metrics', {}).get('chars_per_second', 0),
                'time_per_word': complexity.get('time_per_word', 0),
                'time_per_char': complexity.get('time_per_char', 0),
                
                # å¤æ‚åº¦æŒ‡æ ‡
                'linear_indicator': complexity.get('complexity_indicators', {}).get('linear_indicator', 0),
                'sqrt_indicator': complexity.get('complexity_indicators', {}).get('sqrt_n_indicator', 0),
                'quadratic_indicator': complexity.get('complexity_indicators', {}).get('quadratic_indicator', 0),
                
                # å†…å­˜ç›¸å…³æŒ‡æ ‡
                'peak_memory_mb': metadata.get('peak_memory_usage_mb', 0),
                'memory_per_character': 0,
                
                # APIæˆæœ¬ç›¸å…³æŒ‡æ ‡
                'total_api_cost': metadata.get('total_api_cost', 0),
                'total_tokens': metadata.get('total_tokens', 0),
                'cost_per_word': 0,
                'cost_per_token': 0,
                
                # è§’è‰²ç›¸å…³æŒ‡æ ‡
                'character_count': text_features.get('character_features', {}).get('character_count', 0),
                'character_complexity_score': text_features.get('character_features', {}).get('character_complexity_score', 0),
                
                # å„é˜¶æ®µæ—¶é—´
                **{f'{stage}_time': duration for stage, duration in stage_performance.get('stage_times', {}).items()}
            }
            
            data_rows.append(row)
        
        df = pd.DataFrame(data_rows)
        
        # è®¡ç®—æ´¾ç”ŸæŒ‡æ ‡
        if not df.empty:
            # è®¡ç®—æ¯è§’è‰²å†…å­˜å¼€é”€
            df['memory_per_character'] = df.apply(
                lambda r: r['peak_memory_mb'] / r['character_count'] if r['character_count'] > 0 else 0, 
                axis=1
            )
            
            # è®¡ç®—æˆæœ¬æ•ˆç‡æŒ‡æ ‡
            df['cost_per_word'] = df.apply(
                lambda r: r['total_api_cost'] / r['word_count'] if r['word_count'] > 0 else 0,
                axis=1
            )
            
            df['cost_per_token'] = df.apply(
                lambda r: r['total_api_cost'] / r['total_tokens'] if r['total_tokens'] > 0 else 0,
                axis=1
            )
        
        return df
    
    def plot_time_complexity_analysis(self, save_path: str = None) -> str:
        """ç»˜åˆ¶æ—¶é—´å¤æ‚åº¦åˆ†æå›¾"""
        df = self.extract_data_for_analysis()
        
        if df.empty:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨æ•°æ®è¿›è¡Œå¤æ‚åº¦åˆ†æ")
            return None
            
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        df = df[(df['word_count'] > 0) & (df['total_time'] > 0)].copy()
        
        if len(df) < 2:
            print("âš ï¸ æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¤æ‚åº¦åˆ†æ")
            return None
            
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. æ€»æ—¶é—´ vs å­—æ•°
        x = df['word_count'].values
        y = df['total_time'].values
        
        ax1.scatter(x, y, alpha=0.6, s=60)
        ax1.set_xlabel('å­—æ•°')
        ax1.set_ylabel('æ€»æ—¶é—´ (ç§’)')
        ax1.set_title('æ—¶é—´å¤æ‚åº¦åˆ†æ: æ€»æ—¶é—´ vs å­—æ•°')
        
        # å°è¯•æ‹Ÿåˆä¸åŒå¤æ‚åº¦æ¨¡å‹
        if len(x) >= 3:
            self._fit_complexity_curves(ax1, x, y)
        
        # 2. æ•ˆç‡è¶‹åŠ¿
        if 'timestamp' in df.columns:
            df_sorted = df.sort_values('timestamp')
            ax2.plot(range(len(df_sorted)), df_sorted['words_per_second'], marker='o')
            ax2.set_xlabel('æ‰§è¡Œé¡ºåº')
            ax2.set_ylabel('ç”Ÿæˆæ•ˆç‡ (å­—/ç§’)')
            ax2.set_title('æ•ˆç‡è¶‹åŠ¿åˆ†æ')
            ax2.tick_params(axis='x', rotation=45)
        
        # 3. å„é˜¶æ®µæ—¶é—´åˆ†å¸ƒ
        stage_columns = [col for col in df.columns if col.endswith('_time')]
        if stage_columns:
            stage_data = df[stage_columns].mean()
            stage_names = [col.replace('_time', '').replace('_', ' ').title() for col in stage_columns]
            
            ax3.pie(stage_data.values, labels=stage_names, autopct='%1.1f%%')
            ax3.set_title('å¹³å‡å„é˜¶æ®µæ—¶é—´åˆ†å¸ƒ')
        
        # 4. å¤æ‚åº¦æŒ‡æ ‡å¯¹æ¯”
        indicators = ['linear_indicator', 'sqrt_indicator', 'quadratic_indicator']
        available_indicators = [ind for ind in indicators if ind in df.columns and df[ind].sum() > 0]
        
        if available_indicators:
            indicator_data = df[available_indicators].mean()
            indicator_names = [ind.replace('_indicator', '').replace('_', ' ').title() for ind in available_indicators]
            
            ax4.bar(indicator_names, indicator_data.values)
            ax4.set_ylabel('æŒ‡æ ‡å€¼')
            ax4.set_title('å¤æ‚åº¦æŒ‡æ ‡å¯¹æ¯”')
            ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        if save_path is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"{self.output_dir}/complexity_analysis_{timestamp}.png"
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š å¤æ‚åº¦åˆ†æå›¾å·²ä¿å­˜: {save_path}")
        return save_path
    
    def _fit_complexity_curves(self, ax, x, y):
        """æ‹Ÿåˆä¸åŒå¤æ‚åº¦æ›²çº¿"""
        x_fit = np.linspace(min(x), max(x), 100)
        
        # çº¿æ€§æ‹Ÿåˆ: T(n) = an + b
        try:
            linear_coeffs = np.polyfit(x, y, 1)
            linear_fit = np.poly1d(linear_coeffs)
            ax.plot(x_fit, linear_fit(x_fit), '--', label=f'çº¿æ€§: T(n)={linear_coeffs[0]:.2e}n+{linear_coeffs[1]:.2f}', alpha=0.7)
        except:
            pass
        
        # å¯¹æ•°çº¿æ€§æ‹Ÿåˆ: T(n) = a*n*log(n) + b
        try:
            def nlogn_func(x_val, a, b):
                return a * x_val * np.log(x_val + 1) + b
            
            popt, _ = curve_fit(nlogn_func, x, y, maxfev=1000)
            y_fit = nlogn_func(x_fit, *popt)
            ax.plot(x_fit, y_fit, ':', label=f'N*Log(N): T(n)={popt[0]:.2e}*n*log(n)+{popt[1]:.2f}', alpha=0.7)
        except:
            pass
        
        # äºŒæ¬¡æ‹Ÿåˆ: T(n) = anÂ² + bn + c
        try:
            if len(x) >= 3:
                quad_coeffs = np.polyfit(x, y, 2)
                quad_fit = np.poly1d(quad_coeffs)
                ax.plot(x_fit, quad_fit(x_fit), '-.', label=f'äºŒæ¬¡: T(n)={quad_coeffs[0]:.2e}nÂ²+...', alpha=0.7)
        except:
            pass
        
        ax.legend()
    
    def plot_performance_trends(self, save_path: str = None) -> str:
        """ç»˜åˆ¶æ€§èƒ½è¶‹åŠ¿å›¾"""
        df = self.extract_data_for_analysis()
        
        if df.empty:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨æ•°æ®")
            return None
            
        # æŒ‰æ—¶é—´æ’åº
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æ‰§è¡Œæ—¶é—´è¶‹åŠ¿
        ax1.plot(df.index, df['total_time'], marker='o', linewidth=2, markersize=6)
        ax1.set_xlabel('æ‰§è¡Œé¡ºåº')
        ax1.set_ylabel('æ€»æ—¶é—´ (ç§’)')
        ax1.set_title('æ‰§è¡Œæ—¶é—´è¶‹åŠ¿')
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        if len(df) >= 2:
            z = np.polyfit(df.index, df['total_time'], 1)
            p = np.poly1d(z)
            ax1.plot(df.index, p(df.index), "--", alpha=0.7, color='red', 
                    label=f'è¶‹åŠ¿: {"ä¸Šå‡" if z[0] > 0 else "ä¸‹é™"}')
            ax1.legend()
        
        # 2. ç”Ÿæˆæ•ˆç‡è¶‹åŠ¿
        ax2.plot(df.index, df['words_per_second'], marker='s', linewidth=2, markersize=6, color='green')
        ax2.set_xlabel('æ‰§è¡Œé¡ºåº')
        ax2.set_ylabel('æ•ˆç‡ (å­—/ç§’)')
        ax2.set_title('ç”Ÿæˆæ•ˆç‡è¶‹åŠ¿')
        ax2.grid(True, alpha=0.3)
        
        # 3. å­—æ•° vs æ—¶é—´æ•£ç‚¹å›¾
        scatter = ax3.scatter(df['word_count'], df['total_time'], 
                            c=df.index, cmap='viridis', s=80, alpha=0.7)
        ax3.set_xlabel('å­—æ•°')
        ax3.set_ylabel('æ€»æ—¶é—´ (ç§’)')
        ax3.set_title('å­—æ•° vs æ—¶é—´ (é¢œè‰²è¡¨ç¤ºæ—¶é—´é¡ºåº)')
        plt.colorbar(scatter, ax=ax3, label='æ‰§è¡Œé¡ºåº')
        
        # 4. å„é˜¶æ®µæ—¶é—´ç®±çº¿å›¾
        stage_columns = [col for col in df.columns if col.endswith('_time') and df[col].sum() > 0]
        if stage_columns:
            stage_data = [df[col].dropna() for col in stage_columns]
            stage_names = [col.replace('_time', '').replace('_', '\n') for col in stage_columns]
            
            box_plot = ax4.boxplot(stage_data, labels=stage_names, patch_artist=True)
            ax4.set_ylabel('æ—¶é—´ (ç§’)')
            ax4.set_title('å„é˜¶æ®µæ—¶é—´åˆ†å¸ƒ')
            
            # ç¾åŒ–ç®±çº¿å›¾
            colors = plt.cm.Set3(np.linspace(0, 1, len(box_plot['boxes'])))
            for patch, color in zip(box_plot['boxes'], colors):
                patch.set_facecolor(color)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        if save_path is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"{self.output_dir}/performance_trends_{timestamp}.png"
            
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿å›¾å·²ä¿å­˜: {save_path}")
        return save_path
    
    def plot_memory_complexity_analysis(self, save_path: str = None) -> str:
        """ç»˜åˆ¶å†…å­˜å¤æ‚åº¦åˆ†æå›¾"""
        df = self.extract_data_for_analysis()
        
        if df.empty or df['peak_memory_mb'].sum() == 0:
            print("âš ï¸ æ²¡æœ‰å†…å­˜æ•°æ®è¿›è¡Œåˆ†æ")
            return None
            
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        df = df[(df['character_count'] > 0) & (df['peak_memory_mb'] > 0)].copy()
        
        if len(df) < 2:
            print("âš ï¸ å†…å­˜æ•°æ®ç‚¹ä¸è¶³")
            return None
            
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. å†…å­˜ä½¿ç”¨ vs è§’è‰²æ•°é‡
        x = df['character_count'].values
        y = df['peak_memory_mb'].values
        
        ax1.scatter(x, y, alpha=0.6, s=60, c='red')
        ax1.set_xlabel('è§’è‰²æ•°é‡')
        ax1.set_ylabel('å³°å€¼å†…å­˜ (MB)')
        ax1.set_title('å†…å­˜å¤æ‚åº¦åˆ†æ: å†…å­˜ä½¿ç”¨ vs è§’è‰²æ•°é‡')
        
        # æ‹Ÿåˆçº¿æ€§å…³ç³»
        if len(x) >= 2:
            try:
                linear_coeffs = np.polyfit(x, y, 1)
                x_fit = np.linspace(min(x), max(x), 100)
                linear_fit = np.poly1d(linear_coeffs)
                ax1.plot(x_fit, linear_fit(x_fit), '--', 
                        label=f'çº¿æ€§æ‹Ÿåˆ: M = {linear_coeffs[0]:.2f}*C + {linear_coeffs[1]:.2f}',
                        color='darkred', alpha=0.8)
                ax1.legend()
            except:
                pass
        
        # 2. æ¯è§’è‰²å†…å­˜å¼€é”€è¶‹åŠ¿
        if 'timestamp' in df.columns:
            df_sorted = df.sort_values('timestamp')
            ax2.plot(range(len(df_sorted)), df_sorted['memory_per_character'], 
                    marker='o', color='orange')
            ax2.set_xlabel('æ‰§è¡Œé¡ºåº')
            ax2.set_ylabel('å†…å­˜/è§’è‰² (MB)')
            ax2.set_title('æ¯è§’è‰²å†…å­˜å¼€é”€è¶‹åŠ¿')
        
        # 3. å†…å­˜ vs è§’è‰²å¤æ‚åº¦
        if 'character_complexity_score' in df.columns and df['character_complexity_score'].sum() > 0:
            scatter = ax3.scatter(df['character_complexity_score'], df['peak_memory_mb'], 
                                c=df['character_count'], cmap='plasma', s=80, alpha=0.7)
            ax3.set_xlabel('è§’è‰²å¤æ‚åº¦è¯„åˆ†')
            ax3.set_ylabel('å³°å€¼å†…å­˜ (MB)')
            ax3.set_title('å†…å­˜ vs è§’è‰²å¤æ‚åº¦ (é¢œè‰²è¡¨ç¤ºè§’è‰²æ•°é‡)')
            plt.colorbar(scatter, ax=ax3, label='è§’è‰²æ•°é‡')
        
        # 4. å†…å­˜æ•ˆç‡åˆ†å¸ƒ
        if len(df['memory_per_character']) > 1:
            ax4.hist(df['memory_per_character'], bins=max(len(df)//3, 3), 
                    alpha=0.7, color='lightcoral', edgecolor='black')
            ax4.set_xlabel('å†…å­˜/è§’è‰² (MB)')
            ax4.set_ylabel('é¢‘æ¬¡')
            ax4.set_title('æ¯è§’è‰²å†…å­˜å¼€é”€åˆ†å¸ƒ')
            ax4.axvline(df['memory_per_character'].mean(), color='red', linestyle='--',
                       label=f'å¹³å‡å€¼: {df["memory_per_character"].mean():.2f} MB')
            ax4.legend()
        
        plt.tight_layout()
        
        if save_path is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"{self.output_dir}/memory_complexity_{timestamp}.png"
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ§  å†…å­˜å¤æ‚åº¦åˆ†æå›¾å·²ä¿å­˜: {save_path}")
        return save_path
    
    def plot_api_cost_analysis(self, save_path: str = None) -> str:
        """ç»˜åˆ¶APIæˆæœ¬åˆ†æå›¾"""
        df = self.extract_data_for_analysis()
        
        if df.empty or df['total_api_cost'].sum() == 0:
            print("âš ï¸ æ²¡æœ‰APIæˆæœ¬æ•°æ®è¿›è¡Œåˆ†æ")
            return None
            
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        df = df[df['total_api_cost'] > 0].copy()
        
        if len(df) < 2:
            print("âš ï¸ APIæˆæœ¬æ•°æ®ç‚¹ä¸è¶³")
            return None
            
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. APIæˆæœ¬ vs æ–‡æœ¬é•¿åº¦
        x = df['word_count'].values
        y = df['total_api_cost'].values
        
        ax1.scatter(x, y, alpha=0.6, s=60, c='green')
        ax1.set_xlabel('ç”Ÿæˆå­—æ•°')
        ax1.set_ylabel('APIæˆæœ¬ ($)')
        ax1.set_title('APIæˆæœ¬ vs æ–‡æœ¬é•¿åº¦')
        
        # æ‹Ÿåˆçº¿æ€§å…³ç³»
        if len(x) >= 2:
            try:
                linear_coeffs = np.polyfit(x, y, 1)
                x_fit = np.linspace(min(x), max(x), 100)
                linear_fit = np.poly1d(linear_coeffs)
                ax1.plot(x_fit, linear_fit(x_fit), '--', 
                        label=f'çº¿æ€§æ‹Ÿåˆ: Cost = {linear_coeffs[0]:.6f}*Words + {linear_coeffs[1]:.6f}',
                        color='darkgreen', alpha=0.8)
                ax1.legend()
            except:
                pass
        
        # 2. æˆæœ¬æ•ˆç‡è¶‹åŠ¿
        if 'timestamp' in df.columns:
            df_sorted = df.sort_values('timestamp')
            ax2.plot(range(len(df_sorted)), df_sorted['cost_per_word'], 
                    marker='s', color='blue')
            ax2.set_xlabel('æ‰§è¡Œé¡ºåº')
            ax2.set_ylabel('æˆæœ¬/å­— ($)')
            ax2.set_title('æˆæœ¬æ•ˆç‡è¶‹åŠ¿')
        
        # 3. Tokenä½¿ç”¨ vs æˆæœ¬
        if 'total_tokens' in df.columns:
            scatter = ax3.scatter(df['total_tokens'], df['total_api_cost'],
                                c=df['word_count'], cmap='viridis', s=80, alpha=0.7)
            ax3.set_xlabel('æ€»Tokens')
            ax3.set_ylabel('APIæˆæœ¬ ($)')
            ax3.set_title('Tokenä½¿ç”¨ vs æˆæœ¬ (é¢œè‰²è¡¨ç¤ºå­—æ•°)')
            plt.colorbar(scatter, ax=ax3, label='ç”Ÿæˆå­—æ•°')
        
        # 4. æˆæœ¬æ•ˆç‡åˆ†å¸ƒ
        if len(df['cost_per_word']) > 1:
            ax4.hist(df['cost_per_word'], bins=max(len(df)//3, 3), 
                    alpha=0.7, color='lightblue', edgecolor='black')
            ax4.set_xlabel('æˆæœ¬/å­— ($)')
            ax4.set_ylabel('é¢‘æ¬¡')
            ax4.set_title('å•å­—æˆæœ¬åˆ†å¸ƒ')
            ax4.axvline(df['cost_per_word'].mean(), color='blue', linestyle='--',
                       label=f'å¹³å‡å€¼: ${df["cost_per_word"].mean():.6f}')
            ax4.legend()
        
        plt.tight_layout()
        
        if save_path is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"{self.output_dir}/api_cost_analysis_{timestamp}.png"
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ’° APIæˆæœ¬åˆ†æå›¾å·²ä¿å­˜: {save_path}")
        return save_path
    
    def generate_performance_heatmap(self, save_path: str = None) -> str:
        """ç”Ÿæˆæ€§èƒ½çƒ­åŠ›å›¾"""
        df = self.extract_data_for_analysis()
        
        if df.empty:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨æ•°æ®")
            return None
            
        # é€‰æ‹©æ•°å€¼åˆ—è¿›è¡Œçƒ­åŠ›å›¾åˆ†æ
        numeric_cols = ['total_time', 'word_count', 'char_count', 'sentence_count', 
                       'words_per_second', 'chars_per_second', 'time_per_word']
        
        # åŠ å…¥é˜¶æ®µæ—¶é—´åˆ—
        stage_cols = [col for col in df.columns if col.endswith('_time')]
        numeric_cols.extend(stage_cols)
        
        # è¿‡æ»¤å­˜åœ¨çš„åˆ—
        available_cols = [col for col in numeric_cols if col in df.columns and df[col].sum() != 0]
        
        if len(available_cols) < 2:
            print("âš ï¸ å¯ç”¨æ•°å€¼åˆ—ä¸è¶³")
            return None
            
        # åˆ›å»ºç›¸å…³æ€§çƒ­åŠ›å›¾
        correlation_data = df[available_cols].corr()
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
        
        # 1. ç›¸å…³æ€§çƒ­åŠ›å›¾
        sns.heatmap(correlation_data, annot=True, cmap='coolwarm', center=0,
                   square=True, ax=ax1, cbar_kws={"shrink": .8})
        ax1.set_title('æ€§èƒ½æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾')
        
        # 2. æ ‡å‡†åŒ–åçš„æ€§èƒ½æŒ‡æ ‡çƒ­åŠ›å›¾
        normalized_df = df[available_cols].apply(lambda x: (x - x.min()) / (x.max() - x.min()))
        
        sns.heatmap(normalized_df.T, cmap='viridis', ax=ax2, 
                   cbar_kws={"shrink": .8}, yticklabels=True)
        ax2.set_title('æ ‡å‡†åŒ–æ€§èƒ½æŒ‡æ ‡çƒ­åŠ›å›¾')
        ax2.set_xlabel('æ‰§è¡Œå®ä¾‹')
        ax2.set_ylabel('æ€§èƒ½æŒ‡æ ‡')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        if save_path is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"{self.output_dir}/performance_heatmap_{timestamp}.png"
            
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ”¥ æ€§èƒ½çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")
        return save_path
    
    def generate_comprehensive_report(self, save_dir: str = None) -> Dict[str, str]:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        if save_dir is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            save_dir = f"{self.output_dir}/comprehensive_report_{timestamp}"
        
        os.makedirs(save_dir, exist_ok=True)
        
        report_files = {}
        
        # 1. æ—¶é—´å¤æ‚åº¦åˆ†æ
        complexity_chart = self.plot_time_complexity_analysis(
            os.path.join(save_dir, "complexity_analysis.png")
        )
        if complexity_chart:
            report_files['complexity_analysis'] = complexity_chart
        
        # 2. æ€§èƒ½è¶‹åŠ¿åˆ†æ
        trends_chart = self.plot_performance_trends(
            os.path.join(save_dir, "performance_trends.png")
        )
        if trends_chart:
            report_files['performance_trends'] = trends_chart
        
        # 3. æ€§èƒ½çƒ­åŠ›å›¾
        heatmap_chart = self.generate_performance_heatmap(
            os.path.join(save_dir, "performance_heatmap.png")
        )
        if heatmap_chart:
            report_files['performance_heatmap'] = heatmap_chart
        
        # 4. å†…å­˜å¤æ‚åº¦åˆ†æ
        memory_chart = self.plot_memory_complexity_analysis(
            os.path.join(save_dir, "memory_complexity.png")
        )
        if memory_chart:
            report_files['memory_complexity'] = memory_chart
        
        # 5. APIæˆæœ¬åˆ†æ
        cost_chart = self.plot_api_cost_analysis(
            os.path.join(save_dir, "api_cost_analysis.png")
        )
        if cost_chart:
            report_files['api_cost_analysis'] = cost_chart
        
        # 4. ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
        summary_file = self.generate_statistical_summary(
            os.path.join(save_dir, "statistical_summary.json")
        )
        if summary_file:
            report_files['statistical_summary'] = summary_file
        
        # 5. ç”ŸæˆHTMLæŠ¥å‘Š
        html_report = self.generate_html_report(save_dir, report_files)
        if html_report:
            report_files['html_report'] = html_report
        
        print(f"ğŸ“Š ç»¼åˆæ€§èƒ½åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {save_dir}")
        return report_files
    
    def generate_statistical_summary(self, save_path: str) -> str:
        """ç”Ÿæˆç»Ÿè®¡æ‘˜è¦"""
        df = self.extract_data_for_analysis()
        
        if df.empty:
            return None
            
        summary = {
            'report_metadata': {
                'generated_at': datetime.datetime.now().isoformat(),
                'total_reports_analyzed': len(df),
                'analysis_period': {
                    'start': df['timestamp'].min() if 'timestamp' in df.columns else 'unknown',
                    'end': df['timestamp'].max() if 'timestamp' in df.columns else 'unknown'
                }
            },
            'performance_statistics': {
                'execution_time': {
                    'mean': float(df['total_time'].mean()),
                    'std': float(df['total_time'].std()),
                    'min': float(df['total_time'].min()),
                    'max': float(df['total_time'].max()),
                    'median': float(df['total_time'].median())
                },
                'efficiency': {
                    'mean_words_per_second': float(df['words_per_second'].mean()),
                    'std_words_per_second': float(df['words_per_second'].std()),
                    'best_efficiency': float(df['words_per_second'].max()),
                    'worst_efficiency': float(df['words_per_second'].min())
                },
                'text_characteristics': {
                    'avg_word_count': float(df['word_count'].mean()),
                    'avg_chapter_count': float(df['chapter_count'].mean()),
                    'avg_sentence_count': float(df['sentence_count'].mean())
                }
            },
            'complexity_analysis': self._analyze_complexity_trends(df),
            'recommendations': self._generate_performance_recommendations(df)
        }
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: {save_path}")
        return save_path
    
    def _analyze_complexity_trends(self, df: pd.DataFrame) -> Dict:
        """åˆ†æå¤æ‚åº¦è¶‹åŠ¿"""
        if len(df) < 3:
            return {'status': 'insufficient_data'}
        
        # å°è¯•æ‹Ÿåˆçº¿æ€§å…³ç³»
        try:
            correlation, p_value = pearsonr(df['word_count'], df['total_time'])
            
            # è®¡ç®—å¹³å‡å¤æ‚åº¦æŒ‡æ ‡
            avg_linear = df['linear_indicator'].mean() if 'linear_indicator' in df.columns else 0
            avg_sqrt = df['sqrt_indicator'].mean() if 'sqrt_indicator' in df.columns else 0
            
            return {
                'time_vs_wordcount_correlation': {
                    'correlation_coefficient': float(correlation),
                    'p_value': float(p_value),
                    'strength': 'strong' if abs(correlation) > 0.7 else 'moderate' if abs(correlation) > 0.3 else 'weak'
                },
                'complexity_indicators': {
                    'avg_linear_indicator': float(avg_linear),
                    'avg_sqrt_indicator': float(avg_sqrt),
                    'estimated_complexity': 'linear' if avg_linear > 0 and avg_sqrt > avg_linear * 10 else 'unknown'
                }
            }
        except:
            return {'status': 'analysis_failed'}
    
    def _generate_performance_recommendations(self, df: pd.DataFrame) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if df.empty:
            return recommendations
        
        # æ•ˆç‡åˆ†æ
        avg_efficiency = df['words_per_second'].mean()
        if avg_efficiency < 10:
            recommendations.append("ç”Ÿæˆæ•ˆç‡è¾ƒä½ï¼ˆ<10å­—/ç§’ï¼‰ï¼Œå»ºè®®ä¼˜åŒ–LLMè°ƒç”¨æˆ–æå‡ç¡¬ä»¶æ€§èƒ½")
        elif avg_efficiency > 50:
            recommendations.append("ç”Ÿæˆæ•ˆç‡ä¼˜ç§€ï¼ˆ>50å­—/ç§’ï¼‰ï¼Œæ€§èƒ½è¡¨ç°è‰¯å¥½")
        
        # æ—¶é—´åˆ†æ
        if df['total_time'].std() / df['total_time'].mean() > 0.5:
            recommendations.append("æ‰§è¡Œæ—¶é—´æ³¢åŠ¨è¾ƒå¤§ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œç¨³å®šæ€§å’Œç¼“å­˜ç­–ç•¥")
        
        # é˜¶æ®µåˆ†æ
        stage_cols = [col for col in df.columns if col.endswith('_time')]
        if stage_cols:
            stage_means = df[stage_cols].mean()
            slowest_stage = stage_means.idxmax()
            recommendations.append(f"æœ€è€—æ—¶é˜¶æ®µæ˜¯{slowest_stage.replace('_time', '')}ï¼Œå¯é‡ç‚¹ä¼˜åŒ–æ­¤é˜¶æ®µ")
        
        return recommendations
    
    def generate_html_report(self, save_dir: str, report_files: Dict[str, str]) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„ç»¼åˆæŠ¥å‘Š"""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>æ•…äº‹ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š</title>
            <meta charset="utf-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
                .header {{ text-align: center; color: #2c3e50; margin-bottom: 40px; }}
                .section {{ margin: 30px 0; }}
                .chart {{ text-align: center; margin: 20px 0; }}
                .chart img {{ max-width: 100%; height: auto; border: 1px solid #ddd; }}
                .summary {{ background: #f8f9fa; padding: 20px; border-radius: 8px; }}
                .recommendation {{ background: #e8f5e8; padding: 15px; border-left: 4px solid #28a745; margin: 10px 0; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ“Š æ•…äº‹ç”Ÿæˆç³»ç»Ÿæ€§èƒ½åˆ†ææŠ¥å‘Š</h1>
                <p>ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
            </div>
            
            <div class="section">
                <h2>ğŸ¯ æ—¶é—´å¤æ‚åº¦åˆ†æ</h2>
                <div class="chart">
                    <img src="complexity_analysis.png" alt="æ—¶é—´å¤æ‚åº¦åˆ†æå›¾">
                </div>
                <p>è¯¥å›¾å±•ç¤ºäº†ç”Ÿæˆæ—¶é—´ä¸æ–‡æœ¬é•¿åº¦çš„å…³ç³»ï¼Œä»¥åŠä¸åŒå¤æ‚åº¦æ¨¡å‹çš„æ‹Ÿåˆç»“æœã€‚</p>
            </div>
            
            <div class="section">
                <h2>ğŸ“ˆ æ€§èƒ½è¶‹åŠ¿åˆ†æ</h2>
                <div class="chart">
                    <img src="performance_trends.png" alt="æ€§èƒ½è¶‹åŠ¿å›¾">
                </div>
                <p>è¯¥å›¾å±•ç¤ºäº†æ‰§è¡Œæ—¶é—´ã€ç”Ÿæˆæ•ˆç‡ç­‰å…³é”®æŒ‡æ ‡çš„å˜åŒ–è¶‹åŠ¿ã€‚</p>
            </div>
            
            <div class="section">
                <h2>ğŸ”¥ æ€§èƒ½çƒ­åŠ›å›¾</h2>
                <div class="chart">
                    <img src="performance_heatmap.png" alt="æ€§èƒ½çƒ­åŠ›å›¾">
                </div>
                <p>è¯¥å›¾å±•ç¤ºäº†å„æ€§èƒ½æŒ‡æ ‡ä¹‹é—´çš„ç›¸å…³æ€§å’Œåˆ†å¸ƒç‰¹å¾ã€‚</p>
            </div>
            
            <div class="section">
                <h2>ğŸ§  å†…å­˜å¤æ‚åº¦åˆ†æ</h2>
                <div class="chart">
                    <img src="memory_complexity.png" alt="å†…å­˜å¤æ‚åº¦åˆ†æå›¾">
                </div>
                <p>è¯¥å›¾åˆ†æäº†å†…å­˜ä½¿ç”¨é‡ä¸è§’è‰²æ•°é‡çš„å…³ç³»ï¼Œä»¥åŠå„é˜¶æ®µçš„å†…å­˜å¼€é”€ã€‚</p>
            </div>
            
            <div class="section">
                <h2>ğŸ’° APIæˆæœ¬åˆ†æ</h2>
                <div class="chart">
                    <img src="api_cost_analysis.png" alt="APIæˆæœ¬åˆ†æå›¾">
                </div>
                <p>è¯¥å›¾å±•ç¤ºäº†APIè°ƒç”¨æˆæœ¬ä¸æ–‡æœ¬é•¿åº¦çš„å…³ç³»ï¼Œä»¥åŠæˆæœ¬æ•ˆç‡åˆ†æã€‚</p>
            </div>
            
            <div class="section summary">
                <h2>ğŸ“‹ åˆ†ææ€»ç»“</h2>
                <p>è¯¦ç»†çš„ç»Ÿè®¡æ•°æ®å’Œåˆ†æç»“æœè¯·æŸ¥çœ‹ <a href="statistical_summary.json">statistical_summary.json</a> æ–‡ä»¶ã€‚</p>
                
                <h3>ğŸ¯ æ€§èƒ½æ”¹è¿›å»ºè®®</h3>
                <div class="recommendation">
                    <strong>ğŸ’¡ å»ºè®®:</strong> åŸºäºå½“å‰æ•°æ®åˆ†æï¼Œå»ºè®®é‡ç‚¹å…³æ³¨æœ€è€—æ—¶çš„ç”Ÿæˆé˜¶æ®µï¼Œä¼˜åŒ–LLMè°ƒç”¨ç­–ç•¥ã€‚
                </div>
                <div class="recommendation">
                    <strong>âš¡ æ•ˆç‡æå‡:</strong> å¯ä»¥è€ƒè™‘å®ç°æ›´å¥½çš„ç¼“å­˜æœºåˆ¶æ¥å‡å°‘é‡å¤è®¡ç®—ã€‚
                </div>
                <div class="recommendation">
                    <strong>ğŸ“Š æŒç»­ç›‘æ§:</strong> å»ºè®®å®šæœŸè¿è¡Œæ­¤åˆ†æä»¥è·Ÿè¸ªæ€§èƒ½å˜åŒ–è¶‹åŠ¿ã€‚
                </div>
            </div>
            
            <div class="section">
                <h2>ğŸ› ï¸ æŠ€æœ¯è¯´æ˜</h2>
                <ul>
                    <li><strong>æ—¶é—´å¤æ‚åº¦åˆ†æ:</strong> é€šè¿‡æ‹Ÿåˆä¸åŒæ•°å­¦æ¨¡å‹æ¥ä¼°ç®—ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦</li>
                    <li><strong>æ•ˆç‡æŒ‡æ ‡:</strong> ä»¥"å­—/ç§’"ä¸ºå•ä½è¡¡é‡ç”Ÿæˆæ•ˆç‡</li>
                    <li><strong>é˜¶æ®µåˆ†æ:</strong> åˆ†è§£å„ä¸ªç”Ÿæˆæ­¥éª¤çš„è€—æ—¶å æ¯”</li>
                    <li><strong>ç›¸å…³æ€§åˆ†æ:</strong> è¯†åˆ«å½±å“æ€§èƒ½çš„å…³é”®å› ç´ </li>
                </ul>
            </div>
        </body>
        </html>
        """
        
        html_path = os.path.join(save_dir, "performance_report.html")
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"ğŸ“„ HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_path}")
        return html_path


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä½¿ç”¨æ–¹æ³•"""
    print("ğŸš€ å¯åŠ¨æ€§èƒ½æ•°æ®å¯è§†åŒ–å·¥å…·")
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = PerformanceVisualizer()
    
    # åŠ è½½æ•°æ®
    report_count = visualizer.load_performance_reports()
    
    if report_count == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ€§èƒ½æŠ¥å‘Šæ•°æ®")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ•…äº‹ç”Ÿæˆæµç¨‹ä»¥äº§ç”Ÿæ€§èƒ½æ•°æ®")
        return
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆç»¼åˆæ€§èƒ½åˆ†ææŠ¥å‘Š...")
    report_files = visualizer.generate_comprehensive_report()
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼ç”Ÿæˆäº†ä»¥ä¸‹æ–‡ä»¶:")
    for report_type, file_path in report_files.items():
        print(f"   {report_type}: {file_path}")
    
    print(f"\nğŸ¯ æ‰“å¼€ {report_files.get('html_report', 'æŠ¥å‘Šæ–‡ä»¶')} æŸ¥çœ‹å®Œæ•´åˆ†æç»“æœ")


if __name__ == "__main__":
    main()
