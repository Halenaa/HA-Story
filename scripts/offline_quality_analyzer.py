#!/usr/bin/env python3
"""
ç¦»çº¿è´¨é‡-æ•ˆç‡åˆ†æå·¥å…·

ä¸“é—¨ç”¨äºåˆ†æå·²ç”Ÿæˆçš„æ€§èƒ½æŠ¥å‘ŠJSONæ–‡ä»¶ï¼Œè¿›è¡Œè¯¦ç»†çš„è´¨é‡-æ•ˆç‡æƒè¡¡åˆ†æã€‚
æ”¯æŒå•ä¸ªæ–‡ä»¶åˆ†æå’Œæ‰¹é‡å¯¹æ¯”åˆ†æã€‚

ä½¿ç”¨æ–¹å¼:
1. å•ä¸ªæ–‡ä»¶åˆ†æ: python offline_quality_analyzer.py single report.json
2. æ‰¹é‡å¯¹æ¯”åˆ†æ: python offline_quality_analyzer.py batch data/output/
3. å‚æ•°å¯¹æ¯”åˆ†æ: python offline_quality_analyzer.py compare data/output/ --metric temperature
"""

import json
import os
import argparse
import glob
from typing import Dict, List, Any, Tuple
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import numpy as np

plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class OfflineQualityAnalyzer:
    """ç¦»çº¿è´¨é‡-æ•ˆç‡åˆ†æå™¨"""
    
    def __init__(self):
        self.reports = []
        self.analysis_results = {}
    
    def load_single_report(self, report_path: str) -> Dict:
        """åŠ è½½å•ä¸ªæ€§èƒ½æŠ¥å‘Š"""
        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                report = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½æŠ¥å‘Š: {report_path}")
            return report
        except Exception as e:
            print(f"âŒ åŠ è½½æŠ¥å‘Šå¤±è´¥ {report_path}: {e}")
            return None
    
    def load_batch_reports(self, directory: str) -> List[Dict]:
        """æ‰¹é‡åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰æ€§èƒ½æŠ¥å‘Š"""
        reports = []
        pattern = os.path.join(directory, "**/performance_analysis_*.json")
        
        for report_path in glob.glob(pattern, recursive=True):
            report = self.load_single_report(report_path)
            if report:
                # æ·»åŠ æ–‡ä»¶è·¯å¾„ä¿¡æ¯
                report['_file_path'] = report_path
                report['_file_name'] = os.path.basename(report_path)
                reports.append(report)
        
        print(f"ğŸ“Š å…±åŠ è½½äº† {len(reports)} ä¸ªæŠ¥å‘Šæ–‡ä»¶")
        return reports
    
    def analyze_single_report_quality(self, report: Dict) -> Dict:
        """åˆ†æå•ä¸ªæŠ¥å‘Šçš„è´¨é‡æŒ‡æ ‡"""
        text_features = report.get('text_features', {})
        memory_data = report.get('memory_complexity_data', {})
        api_data = report.get('api_cost_breakdown', {})
        metadata = report.get('metadata', {})
        
        # åŸºç¡€æŒ‡æ ‡
        total_words = text_features.get('total_word_count', 0)
        total_chars = text_features.get('total_char_count', 0)
        total_time = metadata.get('total_execution_time', 0)
        total_cost = metadata.get('total_api_cost', 0)
        peak_memory = metadata.get('peak_memory_usage_mb', 0)
        total_tokens = metadata.get('total_tokens', 0)
        
        # è´¨é‡æŒ‡æ ‡è®¡ç®—
        quality_metrics = self._calculate_quality_metrics(text_features, memory_data)
        
        # æ•ˆç‡æŒ‡æ ‡è®¡ç®—
        efficiency_metrics = self._calculate_efficiency_metrics(
            total_words, total_time, total_cost, peak_memory, total_tokens
        )
        
        # ç»¼åˆåˆ†æ
        overall_score = (quality_metrics['overall_score'] + efficiency_metrics['overall_score']) / 2
        
        return {
            'quality_metrics': quality_metrics,
            'efficiency_metrics': efficiency_metrics,
            'overall_score': overall_score,
            'basic_stats': {
                'words': total_words,
                'time_seconds': total_time,
                'cost_usd': total_cost,
                'memory_mb': peak_memory,
                'tokens': total_tokens
            }
        }
    
    def _calculate_quality_metrics(self, text_features: Dict, memory_data: Dict) -> Dict:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡ - åŸºäºå®é™…æ•°æ®çš„æ›´å‡†ç¡®ç‰ˆæœ¬"""
        
        total_words = text_features.get('total_word_count', 0)
        total_chars = text_features.get('total_char_count', 0)
        sentence_count = text_features.get('sentence_count', 0)
        chapter_count = text_features.get('chapter_count', 0)
        
        scores = {}
        
        # 1. æ–‡æœ¬ä¸°å¯Œåº¦ (0-25åˆ†)
        if total_words > 0 and sentence_count > 0:
            avg_sentence_length = total_words / sentence_count
            # ä¸­æ–‡ç†æƒ³å¥å­é•¿åº¦ 8-20å­—
            richness_score = 25
            if avg_sentence_length < 5:
                richness_score = avg_sentence_length / 5 * 15
            elif avg_sentence_length > 25:
                richness_score = max(15, 25 - (avg_sentence_length - 25) * 0.5)
                
            # å­—ç¬¦å¯†åº¦è¯„åˆ†
            char_density = total_chars / total_words if total_words > 0 else 0
            density_score = min(char_density / 2.5 * 10, 10)  # ä¸­æ–‡å¹³å‡2.5å­—/è¯
            
            scores['text_richness'] = min(richness_score + density_score, 25)
        else:
            scores['text_richness'] = 0
        
        # 2. æ•…äº‹ç»“æ„å®Œæ•´æ€§ (0-25åˆ†)
        if chapter_count > 0 and total_words > 0:
            avg_chapter_length = total_words / chapter_count
            
            # ç†æƒ³ç« èŠ‚é•¿åº¦ 300-800å­—
            if 300 <= avg_chapter_length <= 800:
                length_score = 15
            elif avg_chapter_length < 300:
                length_score = avg_chapter_length / 300 * 15
            else:
                length_score = max(10, 15 - (avg_chapter_length - 800) / 200)
            
            # ç« èŠ‚æ•°é‡åˆç†æ€§ (3-15ç« æ¯”è¾ƒåˆç†)
            if 3 <= chapter_count <= 15:
                count_score = 10
            elif chapter_count < 3:
                count_score = chapter_count / 3 * 10
            else:
                count_score = max(5, 10 - (chapter_count - 15) * 0.3)
            
            scores['story_structure'] = min(length_score + count_score, 25)
        else:
            scores['story_structure'] = 0
        
        # 3. å†…å®¹ä¸€è‡´æ€§ (0-25åˆ†) - åŸºäºè§’è‰²å’Œå‰§æƒ…å¤æ‚åº¦
        character_features = memory_data.get('story_features', {})
        char_count = character_features.get('character_count', 0)
        char_complexity = character_features.get('character_complexity_score', 0)
        
        if char_count > 0:
            # è§’è‰²æ•°é‡åˆç†æ€§
            if 2 <= char_count <= 8:
                char_balance_score = 15
            elif char_count == 1:
                char_balance_score = 10
            else:
                char_balance_score = max(8, 15 - abs(char_count - 5))
            
            # è§’è‰²å¤æ‚åº¦
            complexity_score = min(char_complexity / 50, 10)  # æ ‡å‡†åŒ–
            
            scores['content_consistency'] = min(char_balance_score + complexity_score, 25)
        else:
            scores['content_consistency'] = 0
        
        # 4. æ–‡æœ¬è¿è´¯æ€§ (0-25åˆ†) - åŸºäºå¥å­å’Œæ®µè½ç»“æ„
        if sentence_count > 0 and chapter_count > 0:
            sentences_per_chapter = sentence_count / chapter_count
            
            # ç†æƒ³èŒƒå›´ï¼šæ¯ç« 10-40å¥
            if 10 <= sentences_per_chapter <= 40:
                coherence_score = 25
            elif sentences_per_chapter < 10:
                coherence_score = sentences_per_chapter / 10 * 25
            else:
                coherence_score = max(15, 25 - (sentences_per_chapter - 40) * 0.2)
            
            scores['text_coherence'] = coherence_score
        else:
            scores['text_coherence'] = 0
        
        total_score = sum(scores.values())
        
        return {
            'overall_score': total_score,
            'breakdown': scores,
            'max_possible': 100,
            'percentage': total_score
        }
    
    def _calculate_efficiency_metrics(self, total_words: int, total_time: float, 
                                    total_cost: float, peak_memory: float, total_tokens: int) -> Dict:
        """è®¡ç®—æ•ˆç‡æŒ‡æ ‡"""
        
        scores = {}
        
        # 1. æ—¶é—´æ•ˆç‡ (0-25åˆ†)
        if total_time > 0 and total_words > 0:
            words_per_second = total_words / total_time
            # è¯„åˆ†æ ‡å‡†ï¼šä¸­æ–‡3å­—/ç§’ä¸ºæ»¡åˆ†ï¼Œ1å­—/ç§’ä¸ºåŠæ ¼
            if words_per_second >= 3:
                scores['time_efficiency'] = 25
            elif words_per_second >= 1:
                scores['time_efficiency'] = 15 + (words_per_second - 1) / 2 * 10
            else:
                scores['time_efficiency'] = words_per_second / 1 * 15
        else:
            scores['time_efficiency'] = 0
        
        # 2. æˆæœ¬æ•ˆç‡ (0-25åˆ†)
        if total_cost > 0 and total_words > 0:
            cost_per_word = total_cost / total_words
            # è¯„åˆ†æ ‡å‡†ï¼šæ¯å­—$0.0005ä¸ºæ»¡åˆ†ï¼Œ$0.002ä¸ºåŠæ ¼
            if cost_per_word <= 0.0005:
                scores['cost_efficiency'] = 25
            elif cost_per_word <= 0.002:
                scores['cost_efficiency'] = 15 + (0.002 - cost_per_word) / 0.0015 * 10
            else:
                scores['cost_efficiency'] = max(5, 15 - (cost_per_word - 0.002) * 5000)
        else:
            scores['cost_efficiency'] = 25  # æ— æˆæœ¬ç»™æ»¡åˆ†
        
        # 3. å†…å­˜æ•ˆç‡ (0-25åˆ†)
        if peak_memory > 0 and total_words > 0:
            memory_per_word = peak_memory / total_words
            # è¯„åˆ†æ ‡å‡†ï¼šæ¯å­—0.005MBä¸ºæ»¡åˆ†ï¼Œ0.02MBä¸ºåŠæ ¼
            if memory_per_word <= 0.005:
                scores['memory_efficiency'] = 25
            elif memory_per_word <= 0.02:
                scores['memory_efficiency'] = 15 + (0.02 - memory_per_word) / 0.015 * 10
            else:
                scores['memory_efficiency'] = max(5, 15 - (memory_per_word - 0.02) * 500)
        else:
            scores['memory_efficiency'] = 25  # æ— æ•°æ®ç»™æ»¡åˆ†
        
        # 4. Tokenæ•ˆç‡ (0-25åˆ†)
        if total_tokens > 0 and total_words > 0:
            tokens_per_word = total_tokens / total_words
            # è¯„åˆ†æ ‡å‡†ï¼šä¸­æ–‡æ¯å­—1.5tokenä¸ºæ»¡åˆ†ï¼Œ3tokenä¸ºåŠæ ¼
            if tokens_per_word <= 1.5:
                scores['token_efficiency'] = 25
            elif tokens_per_word <= 3:
                scores['token_efficiency'] = 15 + (3 - tokens_per_word) / 1.5 * 10
            else:
                scores['token_efficiency'] = max(5, 15 - (tokens_per_word - 3) * 5)
        else:
            scores['token_efficiency'] = 25  # æ— æ•°æ®ç»™æ»¡åˆ†
        
        total_score = sum(scores.values())
        
        return {
            'overall_score': total_score,
            'breakdown': scores,
            'max_possible': 100,
            'percentage': total_score
        }
    
    def generate_single_report_analysis(self, report_path: str, output_dir: str = None):
        """ç”Ÿæˆå•ä¸ªæŠ¥å‘Šçš„è¯¦ç»†åˆ†æ"""
        report = self.load_single_report(report_path)
        if not report:
            return
        
        analysis = self.analyze_single_report_quality(report)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        if output_dir is None:
            output_dir = os.path.dirname(report_path)
        
        base_name = os.path.splitext(os.path.basename(report_path))[0]
        output_path = os.path.join(output_dir, f"{base_name}_quality_analysis.json")
        
        detailed_analysis = {
            'analysis_timestamp': datetime.now().isoformat(),
            'source_report': report_path,
            'analysis_results': analysis,
            'recommendations': self._generate_recommendations(analysis)
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“‹ è´¨é‡åˆ†ææŠ¥å‘Š:")
        print(f"   ç»¼åˆå¾—åˆ†: {analysis['overall_score']:.1f}/100")
        print(f"   è´¨é‡å¾—åˆ†: {analysis['quality_metrics']['percentage']:.1f}%")
        print(f"   æ•ˆç‡å¾—åˆ†: {analysis['efficiency_metrics']['percentage']:.1f}%")
        print(f"   è¯¦ç»†æŠ¥å‘Š: {output_path}")
        
        return analysis
    
    def generate_batch_comparison(self, directory: str, output_dir: str = None):
        """ç”Ÿæˆæ‰¹é‡å¯¹æ¯”åˆ†æ"""
        reports = self.load_batch_reports(directory)
        if not reports:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„æŠ¥å‘Šæ–‡ä»¶")
            return
        
        analyses = []
        for report in reports:
            analysis = self.analyze_single_report_quality(report)
            analysis['task_name'] = report.get('metadata', {}).get('task_name', 'unknown')
            analysis['file_name'] = report['_file_name']
            analyses.append(analysis)
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        comparison_data = []
        for analysis in analyses:
            comparison_data.append({
                'Task': analysis['task_name'],
                'File': analysis['file_name'],
                'Overall Score': analysis['overall_score'],
                'Quality Score': analysis['quality_metrics']['percentage'],
                'Efficiency Score': analysis['efficiency_metrics']['percentage'],
                'Words': analysis['basic_stats']['words'],
                'Time (s)': analysis['basic_stats']['time_seconds'],
                'Cost ($)': analysis['basic_stats']['cost_usd'],
                'Memory (MB)': analysis['basic_stats']['memory_mb']
            })
        
        df = pd.DataFrame(comparison_data)
        
        if output_dir is None:
            output_dir = directory
        
        # ä¿å­˜å¯¹æ¯”è¡¨æ ¼
        csv_path = os.path.join(output_dir, f"quality_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._create_comparison_charts(df, output_dir)
        
        print(f"\nğŸ“Š æ‰¹é‡å¯¹æ¯”åˆ†æå®Œæˆ:")
        print(f"   åˆ†æäº† {len(analyses)} ä¸ªæŠ¥å‘Š")
        print(f"   å¯¹æ¯”è¡¨æ ¼: {csv_path}")
        print(f"   å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")
        
        return analyses
    
    def _create_comparison_charts(self, df: pd.DataFrame, output_dir: str):
        """åˆ›å»ºå¯¹æ¯”åˆ†æå›¾è¡¨"""
        
        # 1. ç»¼åˆå¾—åˆ†å¯¹æ¯”
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.bar(range(len(df)), df['Overall Score'])
        plt.title('ç»¼åˆå¾—åˆ†å¯¹æ¯”')
        plt.xlabel('å®éªŒ')
        plt.ylabel('å¾—åˆ†')
        plt.xticks(range(len(df)), df['Task'], rotation=45, ha='right')
        
        # 2. è´¨é‡vsæ•ˆç‡æ•£ç‚¹å›¾
        plt.subplot(1, 2, 2)
        plt.scatter(df['Quality Score'], df['Efficiency Score'], s=100, alpha=0.7)
        plt.xlabel('è´¨é‡å¾—åˆ†')
        plt.ylabel('æ•ˆç‡å¾—åˆ†')
        plt.title('è´¨é‡-æ•ˆç‡æƒè¡¡åˆ†æ')
        
        for i, row in df.iterrows():
            plt.annotate(row['Task'][:10], (row['Quality Score'], row['Efficiency Score']), 
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'quality_efficiency_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. æˆæœ¬-æ•ˆæœåˆ†æ
        plt.figure(figsize=(10, 6))
        plt.scatter(df['Cost ($)'], df['Words'], s=df['Overall Score']*2, alpha=0.6)
        plt.xlabel('æˆæœ¬ ($)')
        plt.ylabel('ç”Ÿæˆå­—æ•°')
        plt.title('æˆæœ¬-æ•ˆæœåˆ†æ (æ°”æ³¡å¤§å°ä»£è¡¨ç»¼åˆå¾—åˆ†)')
        
        for i, row in df.iterrows():
            plt.annotate(row['Task'][:8], (row['Cost ($)'], row['Words']), 
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        plt.savefig(os.path.join(output_dir, 'cost_effectiveness_analysis.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """åŸºäºåˆ†æç»“æœç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        quality_score = analysis['quality_metrics']['percentage']
        efficiency_score = analysis['efficiency_metrics']['percentage']
        quality_breakdown = analysis['quality_metrics']['breakdown']
        efficiency_breakdown = analysis['efficiency_metrics']['breakdown']
        
        # è´¨é‡å»ºè®®
        if quality_score < 60:
            if quality_breakdown.get('text_richness', 0) < 15:
                recommendations.append("ğŸ¨ æ–‡æœ¬ä¸°å¯Œåº¦åä½ï¼Œå»ºè®®è°ƒæ•´promptè®©æ¨¡å‹ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„è¡¨è¾¾")
            
            if quality_breakdown.get('story_structure', 0) < 15:
                recommendations.append("ğŸ“š æ•…äº‹ç»“æ„éœ€è¦ä¼˜åŒ–ï¼Œè€ƒè™‘è°ƒæ•´ç« èŠ‚åˆ†å¸ƒæˆ–æ¯ç« ç›®æ ‡é•¿åº¦")
            
            if quality_breakdown.get('content_consistency', 0) < 15:
                recommendations.append("ğŸ‘¥ è§’è‰²è®¾å®šå¯èƒ½è¿‡äºç®€å•æˆ–å¤æ‚ï¼Œå»ºè®®è°ƒæ•´è§’è‰²æ•°é‡å’ŒèƒŒæ™¯æ·±åº¦")
        
        # æ•ˆç‡å»ºè®®
        if efficiency_score < 60:
            if efficiency_breakdown.get('time_efficiency', 0) < 15:
                recommendations.append("âš¡ ç”Ÿæˆé€Ÿåº¦è¾ƒæ…¢ï¼Œè€ƒè™‘ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹æˆ–ä¼˜åŒ–å¹¶å‘è°ƒç”¨")
            
            if efficiency_breakdown.get('cost_efficiency', 0) < 15:
                recommendations.append("ğŸ’° æˆæœ¬åé«˜ï¼Œå»ºè®®å°è¯•æ›´ç»æµçš„æ¨¡å‹æˆ–ä¼˜åŒ–prompté•¿åº¦")
            
            if efficiency_breakdown.get('memory_efficiency', 0) < 15:
                recommendations.append("ğŸ’¾ å†…å­˜ä½¿ç”¨è¿‡å¤šï¼Œè€ƒè™‘åˆ†æ‰¹å¤„ç†æˆ–ä¼˜åŒ–æ•°æ®ç»“æ„")
        
        # æƒè¡¡å»ºè®®
        if quality_score > 80 and efficiency_score < 40:
            recommendations.append("âš–ï¸ è´¨é‡å¾ˆé«˜ä½†æ•ˆç‡åä½ï¼Œå¯ä»¥é€‚å½“ç®€åŒ–ä»¥æé«˜æ•ˆç‡")
        elif efficiency_score > 80 and quality_score < 40:
            recommendations.append("âš–ï¸ æ•ˆç‡å¾ˆé«˜ä½†è´¨é‡åä½ï¼Œå»ºè®®å¢åŠ ä¸€äº›è´¨é‡æ§åˆ¶æªæ–½")
        elif quality_score > 70 and efficiency_score > 70:
            recommendations.append("ğŸ‰ è´¨é‡å’Œæ•ˆç‡éƒ½å¾ˆå¥½ï¼Œç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼")
        
        if not recommendations:
            recommendations.append("âœ¨ ç³»ç»Ÿè¡¨ç°å‡è¡¡ï¼Œå»ºè®®æ ¹æ®å…·ä½“éœ€æ±‚å¾®è°ƒå‚æ•°")
        
        return recommendations

def main():
    parser = argparse.ArgumentParser(description="ç¦»çº¿è´¨é‡-æ•ˆç‡åˆ†æå·¥å…·")
    parser.add_argument("mode", choices=["single", "batch"], 
                       help="åˆ†ææ¨¡å¼ï¼šsingle=å•ä¸ªæ–‡ä»¶åˆ†æï¼Œbatch=æ‰¹é‡å¯¹æ¯”åˆ†æ")
    parser.add_argument("path", help="æŠ¥å‘Šæ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument("--output", "-o", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    analyzer = OfflineQualityAnalyzer()
    
    if args.mode == "single":
        if not os.path.isfile(args.path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.path}")
            return
        
        analyzer.generate_single_report_analysis(args.path, args.output)
    
    elif args.mode == "batch":
        if not os.path.isdir(args.path):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.path}")
            return
        
        analyzer.generate_batch_comparison(args.path, args.output or args.path)

if __name__ == "__main__":
    main()
