#!/usr/bin/env python3
"""
é‡æ–°æ„å»ºè¯šå®çš„baselineæ•°æ® - åªä½¿ç”¨çœŸå®æµ‹é‡å€¼
"""

import pandas as pd
import numpy as np
import json
import os

def extract_real_data_only():
    """åªæå–çœŸå®æµ‹é‡çš„æ•°æ®"""
    
    print("ğŸ¯ é‡æ–°æ„å»ºè¯šå®çš„baselineæ•°æ®")
    print("=" * 60)
    print("åªä½¿ç”¨çœŸå®åˆ†æç»“æœï¼Œä¸ä¼°ç®—ä»»ä½•æ•°æ®")
    print("=" * 60)
    
    # 1. ä»å¤‡ä»½æ¢å¤åŸå§‹æ•°æ®
    df = pd.read_csv('/Users/haha/Story/metrics_master_clean_backup.csv')
    print(f"ä»å¤‡ä»½æ¢å¤: {len(df)} è¡Œ")
    
    # 2. æ„å»ºçœŸå®çš„baselineæ•°æ®
    real_baselines = []
    
    baseline_mappings = {
        'simple_baseline_s1': {
            'story_id': 'simple_baseline_s1_Tbaseline_sbaseline',
            'source_dir': '/Users/haha/Story/baseline_analysis_results/baseline_s1',
            'diversity_dir': 'diversity_results_baseline_s1'
        },
        'simple_baseline_s2': {
            'story_id': 'simple_baseline_s2_Tbaseline_sbaseline', 
            'source_dir': '/Users/haha/Story/baseline_analysis_results/baseline_s2',
            'diversity_dir': 'diversity_results_baseline_s2'
        }
    }
    
    for config_name, info in baseline_mappings.items():
        print(f"\nğŸ“ æå– {config_name} çš„çœŸå®æ•°æ®...")
        
        # åŸºç¡€é…ç½®
        baseline_row = {
            'story_id': info['story_id'],
            'original_config_name': config_name,
            'test_type': 'simple_baseline_analysis',
            'genre': 'baseline',
            'structure': 'baseline',
            'temperature': 'baseline',
            'seed': 'baseline',
            'is_baseline': 1
        }
        
        # === çœŸå®æµ‹é‡æ•°æ® ===
        
        # 1. æµç•…æ€§æ•°æ® (GPUå®æµ‹)
        fluency_file = f"{info['source_dir']}/fluency_analysis.json"
        if os.path.exists(fluency_file):
            with open(fluency_file, 'r') as f:
                fluency = json.load(f)
            
            baseline_row.update({
                'pseudo_ppl': fluency.get('pseudo_ppl', np.nan),
                'err_per_100w': fluency.get('err_per_100w', np.nan),
                'error_count': fluency.get('error_count', np.nan),
                'fluency_word_count': fluency.get('word_count', np.nan)
            })
            print(f"   âœ… æµç•…æ€§: PPL={fluency.get('pseudo_ppl')}, é”™è¯¯ç‡={fluency.get('err_per_100w')}%")
        
        # 2. è¿è´¯æ€§æ•°æ® (å®æµ‹)
        coherence_file = f"{info['source_dir']}/hred_coherence_analysis.json"
        if os.path.exists(coherence_file):
            with open(coherence_file, 'r') as f:
                coherence = json.load(f)
            
            if 'detailed_analysis' in coherence and 'basic_statistics' in coherence['detailed_analysis']:
                stats = coherence['detailed_analysis']['basic_statistics']
                
                baseline_row.update({
                    'avg_coherence': stats.get('average_coherence', np.nan),
                    'coherence_std': stats.get('coherence_std', np.nan),
                    'coherence_median': stats.get('coherence_median', np.nan),
                    'coherence_max': stats.get('max_coherence', np.nan),
                    'coherence_min': stats.get('min_coherence', np.nan)
                })
            
            if 'HRED_coherence_evaluation' in coherence:
                hred = coherence['HRED_coherence_evaluation']
                baseline_row['coherence_sentence_count'] = hred.get('total_sentences', np.nan)
            
            # é«˜ä½è¿è´¯æ€§æ®µè½æ•° (å®æµ‹)
            if 'detailed_analysis' in coherence:
                detailed = coherence['detailed_analysis']
                baseline_row['low_coherence_points'] = len(detailed.get('coherence_breakpoints', []))
                baseline_row['high_coherence_segments'] = len(detailed.get('high_coherence_segments', []))
            
            print(f"   âœ… è¿è´¯æ€§: {stats.get('average_coherence', 'N/A'):.3f}")
        
        # 3. å¤šæ ·æ€§æ•°æ® (å®æµ‹)
        individual_file = f"{info['diversity_dir']}/individual_diversity_analysis.json"
        if os.path.exists(individual_file):
            with open(individual_file, 'r') as f:
                individual_data = json.load(f)
            
            if individual_data:
                first_key = list(individual_data.keys())[0]
                div_data = individual_data[first_key]
                
                baseline_row.update({
                    'distinct_avg': div_data.get('distinct_avg', np.nan),
                    'distinct_score': div_data.get('distinct_score', np.nan),
                    'total_words': div_data.get('total_words', np.nan),
                    'total_sentences': div_data.get('total_sentences', np.nan)
                })
                print(f"   âœ… å¤šæ ·æ€§: distinct_avg={div_data.get('distinct_avg'):.3f}")
        
        # 4. åŸºæœ¬æ–‡æœ¬ç‰¹å¾ (ä»æ–‡ä»¶ç›´æ¥è®¡ç®—)
        if config_name == 'simple_baseline_s1':
            source_file = '/Users/haha/Story/output/baseline_s1.md'
        elif config_name == 'simple_baseline_s2':
            source_file = '/Users/haha/Story/output/baseline_s2.md'
        
        if os.path.exists(source_file):
            with open(source_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # çœŸå®ç« èŠ‚è®¡æ•°
            import re
            chapter_count = len(re.findall(r'^#[^#]', content, re.MULTILINE))
            baseline_row['chapter_count'] = chapter_count
            print(f"   âœ… ç« èŠ‚æ•°: {chapter_count} (é‡æ–°è®¡ç®—)")
        
        # === åˆç†çš„baselineé»˜è®¤å€¼ (ä¸æ˜¯ä¼°ç®—) ===
        
        # å¯¹äºå•æ ·æœ¬baselineï¼Œè¿™äº›å€¼åœ¨æ¦‚å¿µä¸Šå°±æ˜¯è¿™æ ·çš„ï¼š
        baseline_row.update({
            'diversity_group_score': 0.0,      # å•æ ·æœ¬æ²¡æœ‰ç»„å†…å˜å¼‚
            'self_bleu_group': 0.0,            # å•æ ·æœ¬æ²¡æœ‰self-BLEU
            'one_minus_self_bleu': 1.0,        # 1 - 0 = 1
            'alpha_genre': 0.7,                # ä½¿ç”¨æ ‡å‡†alphaå€¼
            'alpha_value': 0.7                 # ä½¿ç”¨æ ‡å‡†alphaå€¼
        })
        
        # === æ— æ³•æµ‹é‡çš„æ•°æ®ä¿æŒNaN ===
        nan_fields = [
            'wall_time_sec',          # æ²¡æœ‰æ€§èƒ½ç›‘æ§
            'peak_mem_mb',            # æ²¡æœ‰å†…å­˜ç›‘æ§  
            'tokens_total',           # æ²¡æœ‰APIè°ƒç”¨è®°å½•
            'cost_usd',               # æ²¡æœ‰æˆæœ¬è®°å½•
            'roberta_avg_score',      # æƒ…æ„Ÿåˆ†æå¯èƒ½å¤±è´¥
            'correlation_coefficient', # ç›¸å…³æ€§åˆ†æå¯èƒ½å¤±è´¥
            'reagan_classification',  # Reaganåˆ†ç±»å¯èƒ½å¤±è´¥
            'tp_coverage',            # ç»“æ„åˆ†æå¯èƒ½å¤±è´¥
            'li_function_diversity',  # Liå‡½æ•°åˆ†æå¯èƒ½å¤±è´¥
            'total_events'            # äº‹ä»¶ç»Ÿè®¡å¯èƒ½å¤±è´¥
        ]
        
        for field in nan_fields:
            if field not in baseline_row:
                baseline_row[field] = np.nan
        
        real_baselines.append(baseline_row)
        print(f"   âœ… {config_name} çœŸå®æ•°æ®æå–å®Œæˆ")
    
    # 3. åˆå¹¶æ•°æ®
    print(f"\\nğŸ”— åˆå¹¶æ•°æ®...")
    df_new = pd.DataFrame(real_baselines)
    df_final = pd.concat([df, df_new], ignore_index=True)
    
    # 4. ä¿å­˜
    df_final.to_csv('/Users/haha/Story/metrics_master_clean.csv', index=False)
    
    print(f"\\nâœ… è¯šå®çš„baselineæ•°æ®é‡å»ºå®Œæˆ!")
    print(f"ğŸ“Š æ€»è¡Œæ•°: {len(df_final)}")
    print(f"ğŸ“ˆ æ–°å¢baseline: {len(real_baselines)} ä¸ª")
    
    # 5. æ˜¾ç¤ºæ•°æ®è¯šå®æ€§æŠ¥å‘Š
    print(f"\\nğŸ“‹ æ•°æ®è¯šå®æ€§æŠ¥å‘Š:")
    print("-" * 60)
    
    new_baselines_df = df_final[df_final['story_id'].str.startswith('simple_baseline', na=False)]
    
    for _, row in new_baselines_df.iterrows():
        config = row['original_config_name']
        print(f"â€¢ {config}:")
        
        # çœŸå®æµ‹é‡æ•°æ®
        real_data = []
        if not pd.isna(row['pseudo_ppl']): real_data.append(f"PPL={row['pseudo_ppl']}")
        if not pd.isna(row['avg_coherence']): real_data.append(f"è¿è´¯æ€§={row['avg_coherence']:.3f}")
        if not pd.isna(row['distinct_avg']): real_data.append(f"å¤šæ ·æ€§={row['distinct_avg']:.3f}")
        if not pd.isna(row['chapter_count']): real_data.append(f"ç« èŠ‚={row['chapter_count']}")
        
        print(f"  âœ… çœŸå®æ•°æ®: {', '.join(real_data)}")
        
        # NaNçš„å­—æ®µ
        nan_count = sum(1 for v in row.values if pd.isna(v))
        print(f"  âš ï¸  NaNå­—æ®µ: {nan_count} ä¸ª (ç¼ºä¹æµ‹é‡æ•°æ®)")
        print()
    
    print("ğŸ’¯ ç°åœ¨åªä½¿ç”¨çœŸå®æµ‹é‡çš„æ•°æ®ï¼Œä¸å†ä¼°ç®—!")
    
    return df_final

if __name__ == '__main__':
    extract_real_data_only()
"
