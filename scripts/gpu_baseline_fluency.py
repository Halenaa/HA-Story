#!/usr/bin/env python3
"""
GPUç‰ˆæœ¬baselineæµç•…æ€§åˆ†æ - è‡ªåŒ…å«ç‰ˆæœ¬
ç”¨äºåœ¨GPUæœåŠ¡å™¨ä¸Šè¿è¡Œ
"""

import os
import json
import time
from datetime import datetime

def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–"""
    print("ğŸ”§ å®‰è£…å¿…è¦çš„ä¾èµ–...")
    os.system("pip install transformers torch datasets pandas numpy")
    print("âœ… ä¾èµ–å®‰è£…å®Œæˆ")

class SimpleFluencyAnalyzer:
    """ç®€åŒ–ç‰ˆæµç•…æ€§åˆ†æå™¨ - GPUä¸“ç”¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        print("ğŸš€ åˆå§‹åŒ–GPUæµç•…æ€§åˆ†æå™¨...")
        
        try:
            from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM
            import torch
            
            # æ£€æŸ¥GPU
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")
            
            # ä½¿ç”¨RoBERTa-largeè¿›è¡Œæµç•…æ€§è¯„ä¼°
            model_name = "roberta-large"
            print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)
            self.device = device
            
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def calculate_perplexity(self, text):
        """è®¡ç®—ä¼ªå›°æƒ‘åº¦"""
        import torch
        
        # åˆ†è¯
        tokens = self.tokenizer.encode(text, return_tensors='pt', truncation=True, max_length=512)
        tokens = tokens.to(self.device)
        
        with torch.no_grad():
            outputs = self.model(tokens, labels=tokens)
            loss = outputs.loss
            perplexity = torch.exp(loss).item()
        
        return perplexity
    
    def count_grammar_errors(self, text):
        """ç®€å•çš„è¯­æ³•é”™è¯¯è®¡æ•°"""
        import re
        
        errors = 0
        
        # æ£€æŸ¥å¸¸è§é”™è¯¯æ¨¡å¼
        error_patterns = [
            r'\s+[.!?]',  # æ ‡ç‚¹å‰çš„ç©ºæ ¼
            r'[.!?]{2,}',  # é‡å¤æ ‡ç‚¹
            r'\b[a-z]+[A-Z]',  # å•è¯å†…å¤§å°å†™é”™è¯¯
            r'\s{2,}',  # å¤šä½™ç©ºæ ¼
        ]
        
        for pattern in error_patterns:
            errors += len(re.findall(pattern, text))
        
        return errors
    
    def analyze_fluency(self, text):
        """åˆ†ææ–‡æœ¬æµç•…æ€§"""
        print("ğŸ” åˆ†ææµç•…æ€§...")
        
        # åŸºæœ¬ç»Ÿè®¡
        words = text.split()
        word_count = len(words)
        
        # è®¡ç®—å›°æƒ‘åº¦
        try:
            perplexity = self.calculate_perplexity(text)
        except Exception as e:
            print(f"âš ï¸  å›°æƒ‘åº¦è®¡ç®—å¤±è´¥: {e}")
            perplexity = float('inf')
        
        # è¯­æ³•é”™è¯¯
        error_count = self.count_grammar_errors(text)
        err_per_100w = (error_count / word_count) * 100 if word_count > 0 else 0
        
        result = {
            'pseudo_ppl': perplexity,
            'error_count': error_count,
            'err_per_100w': err_per_100w,
            'word_count': word_count,
            'char_count': len(text),
            'analysis_time': datetime.now().isoformat()
        }
        
        print(f"   âœ… PPL: {perplexity:.2f}, é”™è¯¯ç‡: {err_per_100w:.2f}%, è¯æ•°: {word_count}")
        return result

def analyze_baseline_files():
    """åˆ†ææ‰€æœ‰baselineæ–‡ä»¶"""
    
    # baselineæ–‡ä»¶åˆ—è¡¨
    baseline_files = {
        'baseline_s1': 'baseline_s1.md',
        'baseline_s2': 'baseline_s2.md', 
        'baseline_s3': 'normal_baseline.md'
    }
    
    print("ğŸ¯ å¼€å§‹GPUæµç•…æ€§åˆ†æ")
    print("=" * 60)
    
    # æ£€æŸ¥æ–‡ä»¶
    missing_files = []
    for name, filename in baseline_files.items():
        if not os.path.exists(filename):
            missing_files.append(filename)
            print(f"âŒ {name}: {filename} - æ–‡ä»¶ä¸å­˜åœ¨")
        else:
            size = os.path.getsize(filename)
            print(f"âœ… {name}: {filename} ({size:,} bytes)")
    
    if missing_files:
        print(f"\nâŒ ç¼ºå°‘ {len(missing_files)} ä¸ªæ–‡ä»¶ï¼Œè¯·ä¸Šä¼ åå†è¿è¡Œ")
        return None
    
    # åˆå§‹åŒ–åˆ†æå™¨
    try:
        analyzer = SimpleFluencyAnalyzer()
    except Exception as e:
        print(f"âŒ åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return None
    
    # åˆ†ææ¯ä¸ªæ–‡ä»¶
    results = {}
    
    for baseline_name, filename in baseline_files.items():
        print(f"\nğŸ“ [{baseline_name}] åˆ†æ {filename}...")
        
        # è¯»å–æ–‡ä»¶
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            continue
        
        # åˆ†ææµç•…æ€§
        try:
            result = analyzer.analyze_fluency(content)
            result['baseline_name'] = baseline_name
            result['source_file'] = filename
            
            results[baseline_name] = result
            
            # ä¿å­˜å•ä¸ªç»“æœ
            output_file = f"{baseline_name}_fluency_result.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            print(f"   ğŸ’¾ ç»“æœä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")
            continue
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    if results:
        summary_file = "baseline_fluency_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ‰ GPUæµç•…æ€§åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸåˆ†æ: {len(results)}/{len(baseline_files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“ æ±‡æ€»ç»“æœ: {summary_file}")
        
        # æ˜¾ç¤ºç»“æœæ¦‚è§ˆ
        print(f"\nğŸ“‹ åˆ†æç»“æœæ¦‚è§ˆ:")
        print("-" * 60)
        for name, result in results.items():
            print(f"â€¢ {name}:")
            print(f"  - PPL: {result['pseudo_ppl']:.2f}")
            print(f"  - é”™è¯¯ç‡: {result['err_per_100w']:.2f}%")
            print(f"  - è¯æ•°: {result['word_count']:,}")
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GPU Baselineæµç•…æ€§åˆ†æç³»ç»Ÿ")
    print("=" * 60)
    print("æ­¤è„šæœ¬å°†ä½¿ç”¨GPUåˆ†æ3ä¸ªbaselineæ–‡ä»¶çš„æµç•…æ€§")
    print("ç¡®ä¿å·²ä¸Šä¼ ä»¥ä¸‹æ–‡ä»¶:")
    print("â€¢ baseline_s1.md")
    print("â€¢ baseline_s2.md") 
    print("â€¢ normal_baseline.md")
    print("=" * 60)
    
    try:
        # å®‰è£…ä¾èµ–
        install_dependencies()
        
        # è¿è¡Œåˆ†æ
        results = analyze_baseline_files()
        
        if results:
            print("\nâœ¨ æˆåŠŸ! è¯·ä¸‹è½½ä»¥ä¸‹ç»“æœæ–‡ä»¶:")
            print("â€¢ baseline_fluency_summary.json (æ±‡æ€»)")
            print("â€¢ baseline_s1_fluency_result.json")
            print("â€¢ baseline_s2_fluency_result.json")
            print("â€¢ baseline_s3_fluency_result.json")
            
            return True
        else:
            print("âŒ åˆ†æå¤±è´¥")
            return False
            
    except Exception as e:
        print(f"ğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()
