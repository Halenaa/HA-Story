#!/usr/bin/env python3
"""
ç»¼åˆbaselineåˆ†æå™¨ - ä¸º3ä¸ªbaselineæ–‡ä»¶ç”Ÿæˆå®Œæ•´çš„metricsæ•°æ®
åŒ…æ‹¬å¤šæ ·æ€§ã€æµç•…æ€§ã€è¿è´¯æ€§ã€æƒ…æ„Ÿã€ç»“æ„ç­‰æ‰€æœ‰ç»´åº¦åˆ†æ
"""

import os
import sys
import json
import shutil
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime

# å¯¼å…¥å„ç§åˆ†æå™¨
sys.path.append('/Users/haha/Story')
sys.path.append('/Users/haha/Story/src')

from advanced_diversity_analyzer import AdvancedDiversityAnalyzer
from src.analysis.emotional_arc_analyzer import analyze_story_dual_method
from src.analysis.hred_coherence_evaluator import HREDSemanticContinuityEvaluator
from src.analysis.story_evaluator import parse_markdown_story, run_story_evaluation
from src.analysis.fluency_analyzer import FluencyAnalyzer

class ComprehensiveBaselineAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–ç»¼åˆåˆ†æå™¨"""
        self.baseline_files = {
            'baseline_s1': '/Users/haha/Story/output/baseline_s1.md',  # ç§‘å¹»ç‰ˆ
            'baseline_s2': '/Users/haha/Story/output/baseline_s2.md',  # ä¼ ç»Ÿç‰ˆ
            'baseline_s3': '/Users/haha/Story/data/normal_baseline.md'  # åŸnormal_baseline
        }
        
        self.output_base_dir = '/Users/haha/Story/baseline_analysis_results'
        self.ensure_output_directories()
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.fluency_analyzer = None
        self.hred_evaluator = None
        
    def ensure_output_directories(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        os.makedirs(self.output_base_dir, exist_ok=True)
        for baseline_name in self.baseline_files.keys():
            os.makedirs(f"{self.output_base_dir}/{baseline_name}", exist_ok=True)
    
    def run_diversity_analysis(self, baseline_name, file_path):
        """è¿è¡Œå¤šæ ·æ€§åˆ†æ"""
        print(f"ğŸ” [{baseline_name}] è¿è¡Œå¤šæ ·æ€§åˆ†æ...")
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•ç»“æ„
        temp_dir = Path(f"temp_baseline_{baseline_name}")
        temp_dir.mkdir(exist_ok=True)
        
        try:
            # åˆ›å»ºå­ç›®å½•ï¼ˆæ¨¡æ‹Ÿå‚æ•°é…ç½®ï¼‰
            # ä½¿ç”¨æ ‡å‡†å‘½åæ ¼å¼ï¼šthelittleredridinghood_baselinerewrite_linear_T1.0_s1
            sub_dir = temp_dir / f"thelittleredridinghood_baselinerewrite_linear_T1.0_s1"
            sub_dir.mkdir(exist_ok=True)
            
            # å¤åˆ¶æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            target_file = sub_dir / "enhanced_story_dialogue_updated.md"
            shutil.copy2(file_path, target_file)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            diversity_output_dir = f"diversity_results_baseline_{baseline_name}"
            
            # åˆå§‹åŒ–åˆ†æå™¨
            analyzer = AdvancedDiversityAnalyzer(
                data_dir=str(temp_dir),
                output_dir=diversity_output_dir,
                window=1000,
                stride=500,
                bleu_sample_every=1,
                tokenizer='simple',
                p_low=5,
                p_high=95,
                alpha_min=0.4,
                alpha_max=0.8
            )
            
            # è¿è¡Œåˆ†æ
            individual_results, group_results = analyzer.run_analysis()
            analyzer.save_results()
            
            print(f"   âœ… å¤šæ ·æ€§åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {diversity_output_dir}")
            return individual_results, group_results
            
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
    
    def run_fluency_analysis(self, baseline_name, file_path):
        """è·³è¿‡æµç•…æ€§åˆ†æï¼ˆç”¨æˆ·å°†ä½¿ç”¨GPUè¿è¡Œï¼‰"""
        print(f"â­ï¸  [{baseline_name}] è·³è¿‡æµç•…æ€§åˆ†æï¼ˆç”¨æˆ·å°†ä½¿ç”¨GPUå•ç‹¬è¿è¡Œï¼‰...")
        return None
    
    def run_semantic_continuity_analysis(self, baseline_name, file_path):
        """è¿è¡Œè¯­ä¹‰è¿ç»­æ€§åˆ†æ"""
        print(f"ğŸ” [{baseline_name}] è¿è¡ŒHREDè¯­ä¹‰è¿ç»­æ€§åˆ†æ...")
        
        if self.hred_evaluator is None:
            self.hred_evaluator = HREDSemanticContinuityEvaluator(model_name='all-mpnet-base-v2')
        
        # è¯»å–å¹¶è§£ææ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è§£æä¸ºç« èŠ‚æ ¼å¼
        chapters = parse_markdown_story(content)
        
        # è¿è¡Œåˆ†æ
        result = self.hred_evaluator.evaluate_story_semantic_continuity(chapters, include_details=True)
        
        # ä¿å­˜ç»“æœ
        output_file = f"{self.output_base_dir}/{baseline_name}/hred_semantic_continuity_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # æå–å¹³å‡è¯­ä¹‰è¿ç»­æ€§å€¼
        avg_continuity = None
        try:
            avg_continuity = result['detailed_analysis']['basic_statistics']['average_semantic_continuity']
            print(f"   âœ… è¯­ä¹‰è¿ç»­æ€§åˆ†æå®Œæˆ - å¹³å‡è¯­ä¹‰è¿ç»­æ€§: {avg_continuity:.3f}")
        except KeyError:
            print(f"   âœ… è¯­ä¹‰è¿ç»­æ€§åˆ†æå®Œæˆ - ç»“æ„: {result.get('HRED_semantic_continuity_evaluation', {}).get('average_semantic_continuity', 'N/A')}")
        
        return result
    
    def run_emotional_analysis(self, baseline_name, file_path):
        """è¿è¡Œæƒ…æ„Ÿå¼§åˆ†æ"""
        print(f"ğŸ” [{baseline_name}] è¿è¡Œæƒ…æ„Ÿå¼§åˆ†æ...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        result_dir = f"{self.output_base_dir}/{baseline_name}"
        
        # è¿è¡Œæƒ…æ„Ÿåˆ†æ
        result = analyze_story_dual_method(file_path, result_dir)
        
        print(f"   âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ")
        return result
    
    def run_structure_analysis(self, baseline_name, file_path):
        """è¿è¡Œç»“æ„åˆ†æ"""
        print(f"ğŸ” [{baseline_name}] è¿è¡Œç»“æ„åˆ†æ...")
        
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è§£æmarkdownæ•…äº‹
            story_data = parse_markdown_story(content)
            
            # åˆ›å»ºä¸´æ—¶æ•…äº‹æ–‡ä»¶
            temp_story_file = f"{self.output_base_dir}/{baseline_name}/temp_story_structure.json"
            with open(temp_story_file, 'w', encoding='utf-8') as f:
                json.dump(story_data, f, ensure_ascii=False, indent=2)
            
            # åˆ›å»ºä¸´æ—¶ç‰ˆæœ¬ç›®å½•
            version = f"{baseline_name}_temp"
            version_dir = f"/Users/haha/Story/data/output/{version}"
            os.makedirs(version_dir, exist_ok=True)
            
            # å¤åˆ¶æ•…äº‹æ–‡ä»¶åˆ°ç‰ˆæœ¬ç›®å½•
            shutil.copy2(temp_story_file, f"{version_dir}/story_updated.json")
            
            # è¿è¡Œç»“æ„åˆ†æ
            result = run_story_evaluation(version, mode="default", runs=1, 
                                        story_file="story_updated.json", model="gpt-4.1")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_story_file):
                os.remove(temp_story_file)
            if os.path.exists(version_dir):
                shutil.rmtree(version_dir)
            
            # ä¿å­˜ç»“æœ
            output_file = f"{self.output_base_dir}/{baseline_name}/story_structure_analysis.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            print(f"   âœ… ç»“æ„åˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            print(f"   âŒ ç»“æ„åˆ†æå¤±è´¥: {e}")
            return None
    
    def extract_text_features(self, file_path):
        """æå–åŸºæœ¬æ–‡æœ¬ç‰¹å¾"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åŸºæœ¬ç»Ÿè®¡
        words = content.split()
        sentences = [s.strip() for s in content.replace('!', '.').replace('?', '.').split('.') if s.strip()]
        
        # è®¡ç®—ç« èŠ‚æ•°
        chapter_count = len([line for line in content.split('\n') if line.strip().startswith('#')])
        
        return {
            'total_words': len(words),
            'total_sentences': len(sentences),
            'chapter_count': chapter_count,
            'char_count': len(content)
        }
    
    def analyze_single_baseline(self, baseline_name):
        """åˆ†æå•ä¸ªbaselineæ–‡ä»¶"""
        file_path = self.baseline_files[baseline_name]
        
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹åˆ†æ {baseline_name}: {file_path}")
        print(f"{'='*60}")
        
        results = {'baseline_name': baseline_name, 'source_file': file_path}
        
        try:
            # 1. æå–åŸºæœ¬æ–‡æœ¬ç‰¹å¾
            print("ğŸ“ æå–åŸºæœ¬æ–‡æœ¬ç‰¹å¾...")
            text_features = self.extract_text_features(file_path)
            results['text_features'] = text_features
            print(f"   âœ… æ–‡æœ¬ç‰¹å¾: {text_features['total_words']} è¯, {text_features['chapter_count']} ç« ")
            
            # 2. å¤šæ ·æ€§åˆ†æ
            diversity_individual, diversity_group = self.run_diversity_analysis(baseline_name, file_path)
            results['diversity'] = {
                'individual': diversity_individual,
                'group': diversity_group
            }
            
            # 3. æµç•…æ€§åˆ†æï¼ˆè·³è¿‡ï¼Œç”¨æˆ·å°†ä½¿ç”¨GPUï¼‰
            fluency_result = self.run_fluency_analysis(baseline_name, file_path)
            if fluency_result:
                results['fluency'] = fluency_result
            
            # 4. è¯­ä¹‰è¿ç»­æ€§åˆ†æ
            continuity_result = self.run_semantic_continuity_analysis(baseline_name, file_path)
            results['semantic_continuity'] = continuity_result
            
            # 5. æƒ…æ„Ÿåˆ†æ
            emotional_result = self.run_emotional_analysis(baseline_name, file_path)
            results['emotion'] = emotional_result
            
            # 6. ç»“æ„åˆ†æ
            structure_result = self.run_structure_analysis(baseline_name, file_path)
            results['structure'] = structure_result
            
            # ä¿å­˜ç»¼åˆç»“æœï¼ˆè½¬æ¢tupleé”®ä¸ºå­—ç¬¦ä¸²ï¼‰
            def convert_keys(obj):
                if isinstance(obj, dict):
                    return {str(k): convert_keys(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_keys(item) for item in obj]
                else:
                    return obj
            
            results_serializable = convert_keys(results)
            output_file = f"{self.output_base_dir}/{baseline_name}/comprehensive_analysis.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results_serializable, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"\nâœ… {baseline_name} åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åˆ°: {self.output_base_dir}/{baseline_name}/")
            return results
            
        except Exception as e:
            print(f"âŒ {baseline_name} åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def run_all_analysis(self):
        """è¿è¡Œæ‰€æœ‰baselineçš„åˆ†æ"""
        print("ğŸ¯ å¼€å§‹ç»¼åˆbaselineåˆ†æ...")
        print(f"åˆ†ææ–‡ä»¶: {list(self.baseline_files.keys())}")
        print(f"è¾“å‡ºç›®å½•: {self.output_base_dir}")
        
        all_results = {}
        
        for baseline_name in self.baseline_files.keys():
            result = self.analyze_single_baseline(baseline_name)
            if result:
                all_results[baseline_name] = result
        
        # ä¿å­˜æ€»ç»“æœï¼ˆå¤„ç†tupleé”®ï¼‰
        def convert_keys(obj):
            if isinstance(obj, dict):
                return {str(k): convert_keys(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_keys(item) for item in obj]
            else:
                return obj
        
        all_results_serializable = convert_keys(all_results)
        summary_file = f"{self.output_base_dir}/all_baselines_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(all_results_serializable, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ‰ æ‰€æœ‰baselineåˆ†æå®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸåˆ†æ: {len(all_results)}/{len(self.baseline_files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“ ç»“æœä¿å­˜åˆ°: {self.output_base_dir}/")
        
        return all_results

def main():
    """ä¸»å‡½æ•°"""
    analyzer = ComprehensiveBaselineAnalyzer()
    results = analyzer.run_all_analysis()
    return results

if __name__ == "__main__":
    main()
