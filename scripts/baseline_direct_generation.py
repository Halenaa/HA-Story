#!/usr/bin/env python3
"""
Baseline 1: ç›´æ¥LLMç”Ÿæˆå®Œæ•´æ•…äº‹
ä½¿ç”¨ç®€å•prompt "Generate a complete story about [topic]" ç›´æ¥ç”Ÿæˆå®Œæ•´æ•…äº‹
ç”¨äºä¸å¤šæ­¥éª¤pipelineè¿›è¡Œæ€§èƒ½å’Œè´¨é‡æ¯”è¾ƒ
"""

import os
import json
import time
import argparse
from datetime import datetime
from src.utils.utils import generate_response, save_json, save_md
from src.constant import output_dir
from src.analysis.performance_analyzer import PerformanceAnalyzer


def create_baseline_prompt(topic, style):
    """åˆ›å»ºbaselineæç¤ºè¯"""
    if style and style.strip():
        return f"Generate a complete story about '{topic}' in the style of '{style}'. Write a full narrative story with multiple scenes, character development, dialogue, and a complete plot arc. The story should be engaging, well-structured, and include detailed descriptions of characters, settings, and events."
    else:
        return f"Generate a complete story about '{topic}'. Write a full narrative story with multiple scenes, character development, dialogue, and a complete plot arc. The story should be engaging, well-structured, and include detailed descriptions of characters, settings, and events."


def direct_story_generation(topic, style="", temperature=0.7, model="gpt-4.1", performance_analyzer=None):
    """ç›´æ¥ç”Ÿæˆå®Œæ•´æ•…äº‹"""
    
    # åˆ›å»ºç®€å•çš„baseline prompt
    prompt = create_baseline_prompt(topic, style)
    
    print(f"ä½¿ç”¨prompt: {prompt[:100]}...")
    
    # è°ƒç”¨LLMç”Ÿæˆæ•…äº‹
    messages = [{"role": "user", "content": prompt}]
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # ç”Ÿæˆæ•…äº‹
    story_content = generate_response(
        messages, 
        model=model, 
        temperature=temperature,
        performance_analyzer=performance_analyzer,
        stage_name="direct_generation" if performance_analyzer else None
    )
    
    # è®°å½•ç»“æŸæ—¶é—´
    generation_time = time.time() - start_time
    
    return {
        "story": story_content,
        "generation_time": generation_time,
        "prompt": prompt,
        "model": model,
        "temperature": temperature
    }


def run_baseline_experiment(
    topics_and_styles=None,
    temperature=0.7, 
    model="gpt-4.1",
    seed=1,
    output_prefix="baseline1"
):
    """è¿è¡Œbaselineå®éªŒ"""
    
    # é»˜è®¤æµ‹è¯•topicsï¼ˆä¸ä¸»é¡¹ç›®ä¿æŒä¸€è‡´ï¼‰
    if topics_and_styles is None:
        topics_and_styles = [
            {"topic": "Little Red Riding Hood", "style": "Sci-fi rewrite"},
            {"topic": "ç°å§‘å¨˜", "style": "ç°ä»£æ”¹å†™"},
            {"topic": "é’è›™ç‹å­", "style": "ABOæ‹çˆ±"},
            {"topic": "å°çº¢å¸½", "style": "ç§‘å¹»æ”¹å†™"},
            {"topic": "ç™½é›ªå…¬ä¸»", "style": "å†›äº‹æ”¹å†™"},
            {"topic": "Chang'e's flight to the moon", "style": "magic"}
        ]
    
    print(f"å¼€å§‹Baselineå®éªŒ - ç›´æ¥LLMç”Ÿæˆ")
    print(f"æ¨¡å‹: {model}, æ¸©åº¦: {temperature}, ç§å­: {seed}")
    print(f"æµ‹è¯• {len(topics_and_styles)} ä¸ªtopic")
    print()
    
    results = []
    
    for i, item in enumerate(topics_and_styles):
        topic = item["topic"]
        style = item.get("style", "")
        
        print(f"[{i+1}/{len(topics_and_styles)}] ç”Ÿæˆæ•…äº‹: {topic} ({style})")
        
        # åˆ›å»ºç‰ˆæœ¬åç§°
        topic_clean = topic.lower().replace(" ", "").replace("'", "")
        style_clean = style.lower().replace(" ", "").replace("-", "") if style else "nostyle"
        version_name = f"{output_prefix}_{topic_clean}_{style_clean}_T{temperature}_s{seed}"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_folder = os.path.join(output_dir, version_name)
        os.makedirs(output_folder, exist_ok=True)
        
        # åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨
        experiment_config = {
            'baseline_type': 'direct_generation',
            'topic': topic,
            'style': style,
            'temperature': temperature,
            'seed': seed,
            'model': model,
            'method': 'single_prompt'
        }
        
        performance_analyzer = PerformanceAnalyzer(
            task_name=version_name,
            experiment_config=experiment_config
        )
        performance_analyzer.start_total_timing()
        performance_analyzer.start_stage("direct_generation", {
            "topic": topic, 
            "style": style, 
            "method": "single_prompt"
        })
        
        try:
            # ç”Ÿæˆæ•…äº‹
            result = direct_story_generation(
                topic=topic,
                style=style,
                temperature=temperature,
                model=model,
                performance_analyzer=performance_analyzer
            )
            
            # ç»“æŸé˜¶æ®µè®°å½•
            performance_analyzer.end_stage("direct_generation", {
                "story_length": len(result["story"]),
                "success": True
            })
            
            # ä¿å­˜ç»“æœ
            result_data = {
                "version": version_name,
                "topic": topic,
                "style": style,
                "model": model,
                "temperature": temperature,
                "seed": seed,
                "timestamp": datetime.now().isoformat(),
                "story_content": result["story"],
                "prompt_used": result["prompt"],
                "generation_time": result["generation_time"],
                "method": "direct_llm_generation"
            }
            
            # ä¿å­˜JSONæ ¼å¼
            save_json(result_data, version_name, "baseline_result.json")
            
            # ä¿å­˜Markdownæ ¼å¼çš„æ•…äº‹
            story_md = f"# {topic}\n\n**Style:** {style}\n**Generated by:** Baseline Direct LLM\n**Model:** {model}\n**Temperature:** {temperature}\n\n---\n\n{result['story']}"
            save_md(story_md, os.path.join(output_folder, "story.md"))
            
            # è¿›è¡Œç®€å•çš„æ–‡æœ¬åˆ†æ
            word_count = len(result["story"].split())
            char_count = len(result["story"])
            lines_count = len(result["story"].split('\n'))
            
            # åˆ†ææ–‡æœ¬ç‰¹å¾
            performance_analyzer.analyze_text_features(
                story_data=[{"plot": result["story"]}],  # ç®€åŒ–æ ¼å¼ä»¥é€‚é…åˆ†æå™¨
                dialogue_data=[],
                characters_data=[]
            )
            
            # ä¿å­˜æ€§èƒ½åˆ†ææŠ¥å‘Š
            performance_report_path = performance_analyzer.save_report(
                output_folder, 
                f"performance_analysis_{version_name}.json"
            )
            
            # è®°å½•ç»“æœ
            experiment_result = {
                "version": version_name,
                "topic": topic,
                "style": style,
                "success": True,
                "story_length": len(result["story"]),
                "word_count": word_count,
                "generation_time": result["generation_time"],
                "output_folder": output_folder,
                "performance_report": performance_report_path
            }
            
            results.append(experiment_result)
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            total_time = performance_analyzer.get_total_time()
            print(f"   âœ“ å®Œæˆç”Ÿæˆ ({len(result['story'])} å­—ç¬¦, {word_count} è¯)")
            print(f"   â±ï¸ ç”Ÿæˆæ—¶é—´: {result['generation_time']:.2f}ç§’")
            print(f"   ğŸ’° APIæˆæœ¬: ${performance_analyzer.total_api_cost:.4f}")
            print(f"   ğŸ“Š Tokenæ•°: {performance_analyzer.total_tokens:,}")
            print()
            
        except Exception as e:
            print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
            experiment_result = {
                "version": version_name,
                "topic": topic,
                "style": style,
                "success": False,
                "error": str(e),
                "output_folder": output_folder
            }
            results.append(experiment_result)
        
        # é¿å…APIé™æµ
        time.sleep(1)
    
    # ä¿å­˜å®éªŒæ€»ç»“
    experiment_summary = {
        "baseline_type": "direct_llm_generation",
        "experiment_timestamp": datetime.now().isoformat(),
        "model": model,
        "temperature": temperature,
        "seed": seed,
        "total_topics": len(topics_and_styles),
        "successful_generations": len([r for r in results if r["success"]]),
        "failed_generations": len([r for r in results if not r["success"]]),
        "results": results
    }
    
    summary_path = os.path.join(output_dir, f"{output_prefix}_experiment_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(experiment_summary, f, ensure_ascii=False, indent=2)
    
    print("="*50)
    print("Baselineå®éªŒå®Œæˆ!")
    print(f"æ€»è®¡: {len(topics_and_styles)} ä¸ªtopics")
    print(f"æˆåŠŸ: {experiment_summary['successful_generations']} ä¸ª")
    print(f"å¤±è´¥: {experiment_summary['failed_generations']} ä¸ª")
    print(f"å®éªŒæ€»ç»“ä¿å­˜è‡³: {summary_path}")
    print("="*50)
    
    return results


def main():
    parser = argparse.ArgumentParser(description="Baseline 1: ç›´æ¥LLMç”Ÿæˆå®Œæ•´æ•…äº‹")
    parser.add_argument("--model", type=str, default="gpt-4.1", help="ä½¿ç”¨çš„LLMæ¨¡å‹")
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--seed", type=int, default=1, help="éšæœºç§å­")
    parser.add_argument("--topic", type=str, help="å•ä¸ªtopicæµ‹è¯•")
    parser.add_argument("--style", type=str, default="", help="æ•…äº‹é£æ ¼")
    parser.add_argument("--output-prefix", type=str, default="baseline1", help="è¾“å‡ºå‰ç¼€")
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†å•ä¸ªtopicï¼Œåªæµ‹è¯•è¯¥topic
    if args.topic:
        topics_and_styles = [{"topic": args.topic, "style": args.style}]
    else:
        topics_and_styles = None  # ä½¿ç”¨é»˜è®¤çš„topicsåˆ—è¡¨
    
    # è¿è¡Œå®éªŒ
    results = run_baseline_experiment(
        topics_and_styles=topics_and_styles,
        temperature=args.temperature,
        model=args.model,
        seed=args.seed,
        output_prefix=args.output_prefix
    )
    
    return results


if __name__ == "__main__":
    main()
