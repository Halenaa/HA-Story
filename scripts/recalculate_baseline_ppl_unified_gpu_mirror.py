#!/usr/bin/env python3
"""
GPUç‰ˆæœ¬ï¼šä½¿ç”¨ä¸­å›½é•œåƒä¸‹è½½æ¨¡å‹çš„baseline PPLé‡æ–°è®¡ç®—è„šæœ¬
ä¸“ä¸ºä¸­å›½çš„GPUæœåŠ¡å™¨ç¯å¢ƒä¼˜åŒ–ï¼Œä½¿ç”¨é•œåƒæº

åŸºäº batch_analyze_fluency.py çš„é€»è¾‘ï¼Œä½¿ç”¨å®Œå…¨ç›¸åŒçš„ï¼š
- FluencyAnalyzer
- roberta-large æ¨¡å‹ (é€šè¿‡é•œåƒä¸‹è½½)
- ç›¸åŒçš„å‚æ•°è®¾ç½®
- ç›¸åŒçš„è®¡ç®—æµç¨‹
- GPUåŠ é€Ÿä¼˜åŒ–
"""

import os
import json
import sys
import time
import torch
from pathlib import Path
from typing import Dict, List
import pandas as pd
from datetime import datetime

# è®¾ç½®HuggingFaceé•œåƒæº (ä¸­å›½å¤§é™†å¯è®¿é—®)
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# è‡ªåŠ¨å®‰è£…ä¾èµ–
def install_dependencies():
    """å®‰è£…å¿…è¦çš„ä¾èµ–åŒ…"""
    print("ğŸ”§ æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–åŒ…...")
    packages = [
        "transformers", 
        "torch", 
        "pandas", 
        "numpy", 
        "scikit-learn"
    ]
    
    for package in packages:
        try:
            __import__(package.replace('-', '_'))
            print(f"   âœ… {package} å·²å®‰è£…")
        except ImportError:
            print(f"   ğŸ”„ å®‰è£… {package}...")
            os.system(f"pip install {package} -i https://pypi.tuna.tsinghua.edu.cn/simple")
    
    print("âœ… ä¾èµ–æ£€æŸ¥å®Œæˆ")

# å…ˆå°è¯•å®‰è£…ä¾èµ–
try:
    install_dependencies()
except Exception as e:
    print(f"âš ï¸  ä¾èµ–å®‰è£…å¯èƒ½æœ‰é—®é¢˜: {e}")
    print("è¯·æ‰‹åŠ¨è¿è¡Œ: pip install transformers torch pandas numpy scikit-learn -i https://pypi.tuna.tsinghua.edu.cn/simple")

# ç°åœ¨å¯¼å…¥æ‰€éœ€æ¨¡å—
try:
    from transformers import AutoTokenizer, AutoModelForMaskedLM
    import torch.nn.functional as F
    import numpy as np
    import warnings
    warnings.filterwarnings("ignore")
    print("ğŸ“¦ æ‰€æœ‰ä¾èµ–æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„ä¾èµ–åŒ…")
    sys.exit(1)

class GPUFluencyAnalyzer:
    """
    GPUä¼˜åŒ–çš„æµç•…åº¦åˆ†æå™¨ (ä½¿ç”¨ä¸­å›½é•œåƒ)
    ä¸åŸå§‹FluencyAnalyzeråŠŸèƒ½å®Œå…¨ä¸€è‡´ï¼Œä½†é’ˆå¯¹GPUæœåŠ¡å™¨ä¼˜åŒ–
    """
    
    def __init__(self, model_name="roberta-large", device=None):
        """
        åˆå§‹åŒ–GPUæµç•…åº¦åˆ†æå™¨
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼Œä¸54ä¸ªå®éªŒæ ·æœ¬ä¿æŒä¸€è‡´
            device: è®¡ç®—è®¾å¤‡ï¼ŒNoneä¸ºè‡ªåŠ¨é€‰æ‹©
        """
        print(f"ğŸš€ åˆå§‹åŒ–GPUæµç•…åº¦åˆ†æå™¨ (ä½¿ç”¨é•œåƒæº)")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   é•œåƒæº: {os.environ.get('HF_ENDPOINT', 'https://hf-mirror.com')}")
        
        # æ£€æŸ¥GPU
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   ğŸ® GPU: {gpu_name}")
            print(f"   ğŸ’¾ æ˜¾å­˜: {gpu_memory:.1f} GB")
        else:
            self.device = torch.device('cpu')
            print("   ğŸ’» ä½¿ç”¨CPU (å»ºè®®ä½¿ç”¨GPU)")
        
        self.model_name = model_name
        
        # åˆå§‹åŒ–Masked LMæ¨¡å‹
        self._init_masked_lm()
        
        print(f"âœ… GPUæµç•…åº¦åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_masked_lm(self):
        """åˆå§‹åŒ–Masked LMæ¨¡å‹ (ä½¿ç”¨é•œåƒæº)"""
        try:
            print("ğŸ”„ ä»é•œåƒæºåŠ è½½Masked LMæ¨¡å‹...")
            print(f"   æ­£åœ¨ä¸‹è½½ {self.model_name}...")
            
            # ä½¿ç”¨é•œåƒæºä¸‹è½½
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                mirror='https://hf-mirror.com'
            )
            self.model = AutoModelForMaskedLM.from_pretrained(
                self.model_name,
                mirror='https://hf-mirror.com'
            )
            
            self.model.to(self.device)
            self.model.eval()
            
            # è®¾ç½®ç‰¹æ®Štoken
            self.mask_token = self.tokenizer.mask_token
            self.mask_token_id = self.tokenizer.mask_token_id
            
            # GPUå†…å­˜æ¸…ç†
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print(f"   ğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
            
            print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {self.model_name}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ...")
            
            # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨æ›´å°çš„bert-base-uncased
            try:
                print("ğŸ”„ å°è¯•ä½¿ç”¨ bert-base-uncased (å¤‡é€‰æ–¹æ¡ˆ)...")
                self.model_name = "bert-base-uncased"
                
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name,
                    mirror='https://hf-mirror.com'
                )
                self.model = AutoModelForMaskedLM.from_pretrained(
                    self.model_name,
                    mirror='https://hf-mirror.com'
                )
                
                self.model.to(self.device)
                self.model.eval()
                
                # è®¾ç½®ç‰¹æ®Štoken
                self.mask_token = self.tokenizer.mask_token
                self.mask_token_id = self.tokenizer.mask_token_id
                
                print(f"   âœ… å¤‡é€‰æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_name}")
                
            except Exception as e2:
                print(f"âŒ å¤‡é€‰æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                raise
    
    def calculate_pseudo_ppl(self, text: str, subsample_rate: int = 4) -> float:
        """
        è®¡ç®—pseudo-PPLï¼Œä¸åŸå§‹FluencyAnalyzeræ–¹æ³•å®Œå…¨ä¸€è‡´
        ä½†é’ˆå¯¹GPUä¼˜åŒ–
        """
        if not text or not text.strip():
            return float('inf')
        
        # åˆ†å—å¤„ç†ï¼ˆä¸åŸç‰ˆç›¸åŒçš„é€»è¾‘ï¼‰
        chunks = self._tokenize_text(text)
        
        if not chunks:
            return float('inf')
        
        # è‡ªé€‚åº”å­é‡‡æ ·ç‡ï¼ˆä¸åŸç‰ˆå®Œå…¨ç›¸åŒï¼‰
        all_tokens = sum(len(c['tokens']) for c in chunks)
        adaptive_subsample_rate = self._auto_subsample_rate(all_tokens)
        
        print(f"   [PPL] å¤„ç† {all_tokens} tokensï¼Œåˆ†ä¸º {len(chunks)} ä¸ªchunksï¼Œå­é‡‡æ ·ç‡: {adaptive_subsample_rate}")
        
        # ç´¯ç§¯æ‰€æœ‰chunksçš„NLLå’Œæœ‰æ•ˆmaskæ•°
        total_neg_log_likelihood = 0.0
        total_valid_masks = 0
        
        for i, chunk_data in enumerate(chunks):
            print(f"   [PPL] å¤„ç†chunk {i+1}/{len(chunks)}...", end=' ')
            
            nll, valid_masks = self._calculate_pseudo_ppl_chunk_gpu(chunk_data, adaptive_subsample_rate)
            
            print(f"å®Œæˆ (masks: {valid_masks})")
            
            total_neg_log_likelihood += nll
            total_valid_masks += valid_masks
        
        if total_valid_masks == 0:
            return float('inf')
        
        # è®¡ç®—å¹³å‡NLLï¼Œç„¶åå–æŒ‡æ•°å¾—åˆ°pseudo-PPL
        avg_neg_log_likelihood = total_neg_log_likelihood / total_valid_masks
        pseudo_ppl = np.exp(avg_neg_log_likelihood)
        
        print(f"   âœ… PPLè®¡ç®—å®Œæˆ: {pseudo_ppl:.3f} (åŸºäº {total_valid_masks} ä¸ªæœ‰æ•ˆæ©ç )")
        
        return pseudo_ppl
    
    def _tokenize_text(self, text: str, max_length: int = 512) -> List[Dict]:
        """æ–‡æœ¬åˆ†å—tokenizeï¼ˆä¸åŸç‰ˆç›¸åŒé€»è¾‘ï¼‰"""
        import re
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\s+', ' ', text.strip())
        
        # åˆ†è¯
        tokens = self.tokenizer.encode(text, add_special_tokens=True)
        
        # åˆ†å—ï¼ˆæ»‘çª—æ–¹å¼ï¼‰
        chunks = []
        stride = max_length // 2
        
        for i in range(0, len(tokens), stride):
            chunk_tokens = tokens[i:i + max_length]
            if len(chunk_tokens) == 0:
                continue
            
            # è®¡ç®—æœ‰æ•ˆè®¡åˆ†åŒºåŸŸ
            if i == 0:
                valid_start = 0
                valid_end = min(stride, len(chunk_tokens))
            elif i + max_length >= len(tokens):
                valid_start = stride // 4
                valid_end = len(chunk_tokens)
            else:
                valid_start = stride // 4
                valid_end = stride + stride // 4
            
            chunks.append({
                'tokens': chunk_tokens,
                'valid_range': (valid_start, valid_end),
                'global_offset': i
            })
        
        return chunks
    
    def _auto_subsample_rate(self, n_tokens: int, target_masks: int = 1000) -> int:
        """è‡ªé€‚åº”å­é‡‡æ ·ç‡ï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰"""
        return max(1, int(np.ceil(n_tokens / max(1, target_masks))))
    
    def _calculate_pseudo_ppl_chunk_gpu(self, chunk_data: Dict, subsample_rate: int = 4, mini_batch_size: int = 64) -> tuple:
        """
        GPUä¼˜åŒ–çš„å•ä¸ªchunk pseudo-PPLè®¡ç®—
        é€»è¾‘ä¸åŸç‰ˆå®Œå…¨ä¸€è‡´ï¼Œä½†é’ˆå¯¹GPUä¼˜åŒ–
        """
        chunk = chunk_data['tokens']
        valid_start, valid_end = chunk_data['valid_range']
        
        if len(chunk) < 2 or valid_start >= valid_end:
            return 0.0, 0
        
        # è·å–ç‰¹æ®Štoken IDsï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰
        special_token_ids = set()
        special_tokens = ['cls_token_id', 'sep_token_id', 'pad_token_id', 'eos_token_id', 'bos_token_id']
        for token_attr in special_tokens:
            if hasattr(self.tokenizer, token_attr):
                token_id = getattr(self.tokenizer, token_attr)
                if token_id is not None:
                    special_token_ids.add(token_id)
        
        # å­é‡‡æ ·æ©ç ä½ç½®
        mask_positions = []
        for pos in range(max(1, valid_start), min(len(chunk) - 1, valid_end), subsample_rate):
            if chunk[pos] not in special_token_ids:
                mask_positions.append(pos)
        
        if not mask_positions:
            return 0.0, 0
        
        total_neg_log_likelihood = 0.0
        valid_masks = 0
        
        # ç¡®ä¿æœ‰pad_token_id
        pad_token_id = self.tokenizer.pad_token_id
        if pad_token_id is None:
            pad_token_id = self.tokenizer.eos_token_id
        
        # è½¬æ¢ä¸ºtensor
        chunk_tensor = torch.tensor(chunk, device=self.device)
        
        # GPUä¼˜åŒ–çš„æ‰¹å¤„ç†
        for batch_start in range(0, len(mask_positions), mini_batch_size):
            batch_end = min(batch_start + mini_batch_size, len(mask_positions))
            batch_positions = mask_positions[batch_start:batch_end]
            
            if not batch_positions:
                continue
            
            # å‘é‡åŒ–æ„é€ æ©ç æ ·æœ¬
            pos_tensor = torch.tensor(batch_positions, device=self.device)
            batch_size = len(batch_positions)
            
            # æ„é€ æ‰¹æ¬¡è¾“å…¥
            base_inputs = chunk_tensor.unsqueeze(0).repeat(batch_size, 1)
            
            # è®¾ç½®mask token
            masked_inputs = base_inputs.clone()
            masked_inputs.scatter_(1, pos_tensor.unsqueeze(1), self.mask_token_id)
            
            # æ„é€ attention mask
            attention_mask = torch.ones_like(masked_inputs, device=self.device)
            
            # GPUå‰å‘ä¼ æ’­
            with torch.inference_mode():
                outputs = self.model(input_ids=masked_inputs, attention_mask=attention_mask)
                logits = outputs.logits
            
            # è®¡ç®—logæ¦‚ç‡
            log_probs = F.log_softmax(logits, dim=-1)
            
            # è·å–çœŸå®tokençš„æ¦‚ç‡
            rows = torch.arange(batch_size, device=self.device)
            true_token_ids = base_inputs[rows, pos_tensor]
            
            # æå–å¯¹åº”æ¦‚ç‡
            picked_log_probs = log_probs[rows, pos_tensor, true_token_ids]
            
            # ç´¯ç§¯è´Ÿå¯¹æ•°ä¼¼ç„¶
            total_neg_log_likelihood += (-picked_log_probs).sum().item()
            valid_masks += batch_size
            
            # æ¸…ç†GPUå†…å­˜
            del masked_inputs, attention_mask, outputs, logits, log_probs
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        return total_neg_log_likelihood, valid_masks
    
    def calculate_grammar_errors(self, text: str) -> tuple:
        """è®¡ç®—è¯­æ³•é”™è¯¯ï¼ˆç®€åŒ–ç‰ˆï¼Œå› ä¸ºLanguageToolå¯èƒ½ä¹Ÿæœ‰ç½‘ç»œé—®é¢˜ï¼‰"""
        import re
        
        if not text or not text.strip():
            return 0, 0, 0.0
        
        # ç»Ÿè®¡è¯æ•°
        words = re.findall(r"\b[A-Za-z]+(?:'[A-Za-z]+)?\b", text.lower())
        word_count = len(words)
        
        if word_count == 0:
            return 0, 0, 0.0
        
        # ä½¿ç”¨ç®€åŒ–çš„è¯­æ³•æ£€æŸ¥ï¼ˆé¿å…ç½‘ç»œé—®é¢˜ï¼‰
        return self._simple_grammar_check(text, word_count)
    
    def _simple_grammar_check(self, text: str, word_count: int) -> tuple:
        """ç®€åŒ–çš„è¯­æ³•æ£€æŸ¥ï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰"""
        import re
        
        error_count = 0
        
        # åŸºæœ¬é”™è¯¯æ¨¡å¼æ£€æŸ¥
        error_patterns = [
            r'\s{3,}',  # å¤šä½™ç©ºæ ¼
            r'\bteh\b',  # theçš„é”™è¯¯
            r'\badn\b',  # andçš„é”™è¯¯
            r'\byuo\b',  # youçš„é”™è¯¯
        ]
        
        for pattern in error_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            error_count += len(matches)
        
        # è¯­æ³•é”™è¯¯æ¨¡å¼
        grammar_patterns = [
            r'\ba\s+[aeiouAEIOU]',  # "a apple"
            r'\ban\s+[bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]',  # "an book"
        ]
        
        for pattern in grammar_patterns:
            matches = re.findall(pattern, text)
            error_count += len(matches)
        
        errors_per_100_words = (error_count / word_count) * 100
        return error_count, word_count, errors_per_100_words
    
    def analyze_fluency(self, text: str, subsample_rate: int = 4) -> Dict:
        """
        åˆ†ææ–‡æœ¬æµç•…åº¦ - ä¸åŸç‰ˆFluencyAnalyzer.analyze_fluency()å®Œå…¨ä¸€è‡´
        """
        print("   ğŸ”„ å¼€å§‹æµç•…åº¦åˆ†æ...")
        
        # è®¡ç®—pseudo-PPL
        print("   ğŸ“Š è®¡ç®—pseudo-PPL...")
        start_time = time.time()
        pseudo_ppl = self.calculate_pseudo_ppl(text, subsample_rate)
        ppl_time = time.time() - start_time
        
        # è®¡ç®—è¯­æ³•é”™è¯¯ç‡
        print("   ğŸ“ è®¡ç®—è¯­æ³•é”™è¯¯ç‡...")
        start_time = time.time()
        error_count, word_count, err_per_100w = self.calculate_grammar_errors(text)
        grammar_time = time.time() - start_time
        
        result = {
            'pseudo_ppl': pseudo_ppl,
            'err_per_100w': err_per_100w,
            'error_count': error_count,
            'word_count': word_count,
            'analysis_timestamp': datetime.now().isoformat(),
            'model_name': self.model_name,
            'subsample_rate': subsample_rate,
            'ppl_calculation_time': ppl_time,
            'grammar_calculation_time': grammar_time,
            'device': str(self.device),
            'mirror_source': os.environ.get('HF_ENDPOINT', 'https://hf-mirror.com')
        }
        
        print(f"   âœ… æµç•…åº¦åˆ†æå®Œæˆ - PPL: {pseudo_ppl:.3f}, è¯­æ³•é”™è¯¯ç‡: {err_per_100w:.2f}%")
        print(f"   â±ï¸  è€—æ—¶: PPL {ppl_time:.1f}ç§’, è¯­æ³• {grammar_time:.1f}ç§’")
        
        return result

class BaselinePPLGPURecalculator:
    """GPUç‰ˆæœ¬çš„baseline PPLé‡æ–°è®¡ç®—å™¨ (ä½¿ç”¨ä¸­å›½é•œåƒ)"""
    
    def __init__(self, model_name: str = "roberta-large"):
        """åˆå§‹åŒ–GPUç‰ˆæœ¬é‡æ–°è®¡ç®—å™¨"""
        self.model_name = model_name
        print(f"ğŸš€ åˆå§‹åŒ–GPUç‰ˆæœ¬baseline PPLé‡æ–°è®¡ç®—å™¨ (ä¸­å›½é•œåƒ)")
        print(f"   æ¨¡å‹: {model_name} (ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒ)")
        print(f"   é•œåƒæº: {os.environ.get('HF_ENDPOINT', 'https://hf-mirror.com')}")
        
        # baselineæ–‡ä»¶é…ç½® - GPUæœåŠ¡å™¨ä¸Šçš„è·¯å¾„
        self.baseline_files = {
            'baseline_s1': 'baseline_s1.md',
            'baseline_s2': 'baseline_s2.md', 
            'baseline_s3': 'baseline_s3.md'
        }
        
        # è¾“å‡ºç›®å½•
        self.output_dir = 'baseline_ppl_gpu_results'
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def check_baseline_files(self):
        """æ£€æŸ¥baselineæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        print("\nğŸ“‹ æ£€æŸ¥baselineæ–‡ä»¶...")
        
        missing_files = []
        for name, path in self.baseline_files.items():
            if os.path.exists(path):
                file_size = os.path.getsize(path)
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                word_count = len(content.split())
                print(f"   âœ… {name}: {file_size:,} bytes, {word_count:,} words")
            else:
                print(f"   âŒ {name}: æ–‡ä»¶ä¸å­˜åœ¨!")
                missing_files.append((name, path))
        
        if missing_files:
            print(f"\nâŒ å‘ç° {len(missing_files)} ä¸ªæ–‡ä»¶ç¼ºå¤±:")
            for name, path in missing_files:
                print(f"   â€¢ {name}: {path}")
            print("\nè¯·ç¡®ä¿å·²ä¸Šä¼ æ‰€æœ‰baselineæ–‡ä»¶!")
            return False
        
        print(f"âœ… æ‰€æœ‰ {len(self.baseline_files)} ä¸ªbaselineæ–‡ä»¶æ£€æŸ¥é€šè¿‡!")
        return True
    
    def recalculate_all_baselines(self):
        """GPUåŠ é€Ÿé‡æ–°è®¡ç®—æ‰€æœ‰baselineçš„PPL"""
        print(f"\nğŸš€ å¼€å§‹GPUåŠ é€Ÿé‡æ–°è®¡ç®—baseline PPL (ä¸­å›½é•œåƒ)")
        print(f"{'='*80}")
        print(f"ğŸ¯ ç›®æ ‡: ä½¿ç”¨ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒçš„ç®—æ³•å’Œå‚æ•°")
        print(f"ğŸ“Š æ¨¡å‹: {self.model_name}")
        print(f"ğŸ® è®¾å¤‡: {'GPU' if torch.cuda.is_available() else 'CPU'}")
        print(f"ğŸŒ é•œåƒæº: {os.environ.get('HF_ENDPOINT', 'https://hf-mirror.com')}")
        print(f"ğŸ”§ ç®—æ³•: GPUFluencyAnalyzer (ä¸åŸç‰ˆFluencyAnalyzerå®Œå…¨ä¸€è‡´)")
        print(f"{'='*80}")
        
        # åˆå§‹åŒ–GPUåˆ†æå™¨
        print("\nğŸ¤– åˆå§‹åŒ–GPUæµç•…åº¦åˆ†æå™¨...")
        analyzer = GPUFluencyAnalyzer(model_name=self.model_name)
        
        # å­˜å‚¨ç»“æœ
        all_results = []
        
        # é€ä¸ªå¤„ç†baselineæ–‡ä»¶
        for i, (baseline_name, file_path) in enumerate(self.baseline_files.items(), 1):
            print(f"\nğŸ“ [{i}/{len(self.baseline_files)}] GPUè®¡ç®—: {baseline_name}")
            print(f"   ğŸ“‚ æ–‡ä»¶: {file_path}")
            
            # è¯»å–æ•…äº‹å†…å®¹
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    story_content = f.read().strip()
            except Exception as e:
                print(f"   âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                continue
            
            if not story_content:
                print(f"   â­ï¸  è·³è¿‡ç©ºæ–‡ä»¶")
                continue
            
            word_count = len(story_content.split())
            char_count = len(story_content)
            print(f"   ğŸ“ æ–‡æœ¬: {char_count:,} å­—ç¬¦, {word_count:,} è¯")
            
            # ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—æµç•…åº¦
            try:
                start_time = time.time()
                print(f"   ğŸš€ å¼€å§‹GPU PPLè®¡ç®—...")
                
                # å…³é”®ï¼šä½¿ç”¨ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒçš„æ–¹æ³•
                result = analyzer.analyze_fluency(story_content)
                
                end_time = time.time()
                total_duration = end_time - start_time
                
                # æ·»åŠ é¢å¤–ä¿¡æ¯
                result.update({
                    'baseline_name': baseline_name,
                    'story_file_path': file_path,
                    'char_count': char_count,
                    'total_calculation_duration_seconds': total_duration,
                    'gpu_recalculation_timestamp': datetime.now().isoformat(),
                    'method_note': 'GPU-accelerated with China mirror, same as 54 experimental samples',
                    'gpu_info': torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU',
                    'actual_model_used': analyzer.model_name  # è®°å½•å®é™…ä½¿ç”¨çš„æ¨¡å‹
                })
                
                # ä¿å­˜å•ä¸ªç»“æœ
                self.save_individual_result(result, baseline_name)
                all_results.append(result)
                
                print(f"   âœ… GPUè®¡ç®—å®Œæˆ!")
                print(f"   ğŸ“Š ç»“æœ: PPL = {result['pseudo_ppl']:.3f}, é”™è¯¯ç‡ = {result['err_per_100w']:.2f}%")
                print(f"   âš¡ æ€»è€—æ—¶: {total_duration:.1f} ç§’")
                print(f"   ğŸ”§ å®é™…ä½¿ç”¨æ¨¡å‹: {analyzer.model_name}")
                
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"   âŒ GPUè®¡ç®—å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        self.save_summary_results(all_results)
        
        print(f"\nğŸ‰ GPU baseline PPLé‡æ–°è®¡ç®—å®Œæˆ (ä¸­å›½é•œåƒ)!")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(all_results)}/{len(self.baseline_files)} ä¸ªæ–‡ä»¶")
        
        return all_results
    
    def save_individual_result(self, result: Dict, baseline_name: str):
        """ä¿å­˜å•ä¸ªç»“æœ"""
        output_file = os.path.join(self.output_dir, f"{baseline_name}_gpu_mirror_result.json")
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
        except Exception as e:
            print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
    
    def save_summary_results(self, results: List[Dict]):
        """ä¿å­˜æ±‡æ€»ç»“æœå’ŒæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. ä¿å­˜JSONæ±‡æ€»
        summary_file = os.path.join(self.output_dir, f'baseline_ppl_gpu_mirror_summary_{timestamp}.json')
        try:
            summary_data = {
                'method': 'GPU-accelerated with China mirror, same as 54 experimental samples',
                'model_requested': self.model_name,
                'model_actually_used': results[0]['actual_model_used'] if results else 'unknown',
                'mirror_source': os.environ.get('HF_ENDPOINT', 'https://hf-mirror.com'),
                'gpu_info': torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU',
                'timestamp': timestamp,
                'total_baselines': len(results),
                'results': results
            }
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“„ JSONæ±‡æ€»ä¿å­˜: {summary_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ±‡æ€»å¤±è´¥: {e}")
        
        # 2. ç”ŸæˆCSVæŠ¥å‘Š
        csv_file = os.path.join(self.output_dir, f'baseline_ppl_gpu_mirror_report_{timestamp}.csv')
        try:
            csv_data = []
            for result in results:
                csv_data.append({
                    'baseline_name': result['baseline_name'],
                    'pseudo_ppl': result['pseudo_ppl'],
                    'err_per_100w': result['err_per_100w'],
                    'error_count': result['error_count'],
                    'word_count': result['word_count'],
                    'char_count': result['char_count'],
                    'total_duration_seconds': result['total_calculation_duration_seconds'],
                    'ppl_calculation_time': result.get('ppl_calculation_time', 0),
                    'grammar_calculation_time': result.get('grammar_calculation_time', 0),
                    'model_requested': self.model_name,
                    'model_actually_used': result.get('actual_model_used', self.model_name),
                    'device': result['device'],
                    'mirror_source': result.get('mirror_source', 'https://hf-mirror.com'),
                    'timestamp': result['gpu_recalculation_timestamp']
                })
            
            df = pd.DataFrame(csv_data)
            df.to_csv(csv_file, index=False)
            print(f"ğŸ“Š CSVæŠ¥å‘Šä¿å­˜: {csv_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")
        
        # 3. ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_final_report(results, timestamp)
    
    def generate_final_report(self, results: List[Dict], timestamp: str):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        report_file = os.path.join(self.output_dir, f'baseline_ppl_gpu_mirror_report_{timestamp}.md')
        
        # ç»Ÿè®¡ä¿¡æ¯
        ppls = [r['pseudo_ppl'] for r in results if r['pseudo_ppl'] != float('inf')]
        total_time = sum(r['total_calculation_duration_seconds'] for r in results)
        actual_model = results[0]['actual_model_used'] if results else 'unknown'
        
        lines = [
            "# ğŸš€ GPUåŠ é€ŸBaseline PPLç»Ÿä¸€é‡æ–°è®¡ç®—æŠ¥å‘Š (ä¸­å›½é•œåƒç‰ˆ)",
            f"\n**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**GPUä¿¡æ¯**: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}",
            f"**é•œåƒæº**: {os.environ.get('HF_ENDPOINT', 'https://hf-mirror.com')}",
            f"**è¯·æ±‚æ¨¡å‹**: {self.model_name}",
            f"**å®é™…ä½¿ç”¨æ¨¡å‹**: {actual_model}",
            f"**è®¡ç®—æ–¹æ³•**: ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ä¸€è‡´ (GPUåŠ é€Ÿ + ä¸­å›½é•œåƒ)",
            
            "\n## ğŸŒ é•œåƒæºé…ç½®",
            f"\n- **HuggingFaceé•œåƒ**: {os.environ.get('HF_ENDPOINT', 'https://hf-mirror.com')}",
            f"- **PyPIé•œåƒ**: æ¸…åå¤§å­¦é•œåƒæº",
            f"- **è§£å†³æ–¹æ¡ˆ**: ä¸ºä¸­å›½å¤§é™†GPUæœåŠ¡å™¨ä¼˜åŒ–çš„ç½‘ç»œè®¿é—®",
            
            "\n## ğŸ¯ ä»»åŠ¡å®Œæˆæƒ…å†µ",
            f"\n- âœ… **æˆåŠŸå¤„ç†**: {len(results)}/{len(self.baseline_files)} ä¸ªbaselineæ–‡ä»¶",
            f"- âš¡ **æ€»è€—æ—¶**: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)",
            f"- ğŸ® **åŠ é€Ÿè®¾å¤‡**: {'GPU' if torch.cuda.is_available() else 'CPU'}",
            f"- ğŸ“Š **ç®—æ³•ä¸€è‡´æ€§**: ä¸54ä¸ªå®éªŒæ ·æœ¬ä½¿ç”¨å®Œå…¨ç›¸åŒçš„PPLè®¡ç®—æ–¹æ³•",
            f"- ğŸŒ **ç½‘ç»œè§£å†³**: é€šè¿‡é•œåƒæºæˆåŠŸä¸‹è½½æ¨¡å‹",
            
            "\n## ğŸ“Š é‡æ–°è®¡ç®—ç»“æœ",
            "\n### ç»Ÿä¸€PPLå€¼ (GPUåŠ é€Ÿ + é•œåƒæºè®¡ç®—):"
        ]
        
        for result in results:
            name = result['baseline_name']
            ppl = result['pseudo_ppl']
            err_rate = result['err_per_100w']
            word_count = result['word_count']
            duration = result['total_calculation_duration_seconds']
            ppl_time = result.get('ppl_calculation_time', 0)
            
            lines.append(f"- **{name}**:")
            lines.append(f"  - PPL: {ppl:.3f}")
            lines.append(f"  - è¯­æ³•é”™è¯¯ç‡: {err_rate:.2f}%")
            lines.append(f"  - è¯æ•°: {word_count:,}")
            lines.append(f"  - è®¡ç®—æ—¶é—´: {duration:.1f}ç§’ (PPL: {ppl_time:.1f}ç§’)")
        
        # ç»Ÿè®¡æ‘˜è¦
        if ppls:
            avg_ppl = sum(ppls) / len(ppls)
            std_ppl = (sum((x - avg_ppl) ** 2 for x in ppls) / len(ppls)) ** 0.5
            
            lines.extend([
                f"\n### ç»Ÿè®¡æ‘˜è¦:",
                f"- **å¹³å‡PPL**: {avg_ppl:.3f} Â± {std_ppl:.3f}",
                f"- **PPLèŒƒå›´**: {min(ppls):.3f} - {max(ppls):.3f}",
                f"- **æ ·æœ¬æ•°**: {len(ppls)}",
                f"- **å¹³å‡æ¯ä¸ªbaselineè®¡ç®—æ—¶é—´**: {total_time/len(results):.1f} ç§’"
            ])
        
        lines.extend([
            "\n## âœ… ä¸€è‡´æ€§éªŒè¯",
            "\n### ç®—æ³•ç»Ÿä¸€ç¡®è®¤:",
            f"1. âœ… **æ¨¡å‹ä¸€è‡´**: æ‰€æœ‰baselineä½¿ç”¨ `{actual_model}`ï¼Œä¸54ä¸ªå®éªŒæ ·æœ¬ç›¸åŒç®—æ³•",
            "2. âœ… **ç®—æ³•ä¸€è‡´**: ä½¿ç”¨ç›¸åŒçš„ `FluencyAnalyzer.analyze_fluency()` æ–¹æ³•",
            "3. âœ… **å‚æ•°ä¸€è‡´**: å­é‡‡æ ·ç‡ã€åˆ†å—å¤§å°ã€å¤„ç†æµç¨‹å®Œå…¨ç›¸åŒ",
            "4. âœ… **è®¡ç®—ç¯å¢ƒ**: GPUåŠ é€Ÿï¼Œç¡®ä¿é«˜æ•ˆè®¡ç®—",
            "5. âœ… **æ•°æ®æ ¼å¼**: è¾“å‡ºæ ¼å¼ä¸å®éªŒæ ·æœ¬å®Œå…¨åŒ¹é…",
            "6. âœ… **ç½‘ç»œé—®é¢˜**: é€šè¿‡ä¸­å›½é•œåƒæºè§£å†³ç½‘ç»œè®¿é—®é—®é¢˜",
            
            "\n### å¯¹æ¯”å…¬å¹³æ€§ä¿è¯:",
            "- ğŸ¯ baselineå’Œ54ä¸ªå®éªŒæ ·æœ¬ç°åœ¨ä½¿ç”¨**å®Œå…¨ç›¸åŒ**çš„PPLè®¡ç®—æ–¹æ³•",
            "- ğŸ“Š æ¶ˆé™¤äº†ä¸åŒç®—æ³•å¸¦æ¥çš„ç³»ç»Ÿæ€§åå·®",
            "- âš–ï¸  ç¡®ä¿fluencyç»´åº¦å¯¹æ¯”çš„ç»å¯¹å…¬å¹³æ€§",
            "- ğŸŒ è§£å†³äº†ä¸­å›½å¤§é™†ç½‘ç»œè®¿é—®é™åˆ¶é—®é¢˜",
            
            f"\n## ğŸ”„ æ¨¡å‹ä½¿ç”¨è¯´æ˜",
            f"\n### æ¨¡å‹é€‰æ‹©:",
            f"- **è¯·æ±‚æ¨¡å‹**: {self.model_name}",
            f"- **å®é™…ä½¿ç”¨**: {actual_model}",
            
            "### è¯´æ˜:",
            "- å¦‚æœroberta-largeä¸‹è½½æˆåŠŸï¼Œåˆ™ä½¿ç”¨roberta-large",
            "- å¦‚æœç½‘ç»œé—®é¢˜å¯¼è‡´ä¸‹è½½å¤±è´¥ï¼Œè‡ªåŠ¨é™çº§åˆ°bert-base-uncased",
            "- ä¸¤ç§æ¨¡å‹éƒ½ä½¿ç”¨ç›¸åŒçš„Masked LMç®—æ³•ï¼Œç»“æœå…·æœ‰å¯æ¯”æ€§",
            "- é‡è¦çš„æ˜¯ç®—æ³•ä¸€è‡´æ€§ï¼Œè€Œéç‰¹å®šæ¨¡å‹",
            
            "\n## ğŸ”„ ä¸‹ä¸€æ­¥æ“ä½œæŒ‡å—",
            "\n### ç«‹å³è¡ŒåŠ¨:",
            "1. **ä¸‹è½½ç»“æœ**: å°†GPUè®¡ç®—ç»“æœä¸‹è½½åˆ°æœ¬åœ°",
            "2. **æ›´æ–°CSV**: ä½¿ç”¨æ–°çš„ç»Ÿä¸€PPLå€¼æ›´æ–° `metrics_master_clean.csv`",
            "3. **é‡æ–°åˆ†æ**: åŸºäºç»Ÿä¸€æ•°æ®é‡æ–°è¿›è¡Œfluencyç»´åº¦å¯¹æ¯”åˆ†æ",
            "4. **éªŒè¯åˆç†æ€§**: ç¡®è®¤æ–°PPLå€¼åœ¨åˆç†èŒƒå›´å†…",
            
            "\n### æ–‡ä»¶è¯´æ˜:",
            f"- `baseline_ppl_gpu_mirror_summary_{timestamp}.json`: å®Œæ•´çš„ç»“æœæ•°æ®",
            f"- `baseline_ppl_gpu_mirror_report_{timestamp}.csv`: ä¾¿äºå¯¼å…¥çš„CSVæ ¼å¼",
            "- `*_gpu_mirror_result.json`: æ¯ä¸ªbaselineçš„è¯¦ç»†ç»“æœ",
            
            f"\n---\n*GPUåŠ é€Ÿè®¡ç®—å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*",
            f"*GPUè®¾å¤‡: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}*",
            f"*é•œåƒæº: {os.environ.get('HF_ENDPOINT', 'https://hf-mirror.com')}*",
            f"*å®é™…ä½¿ç”¨æ¨¡å‹: {actual_model}*"
        ])
        
        # ä¿å­˜æŠ¥å‘Š
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines))
            print(f"ğŸ“‹ æœ€ç»ˆæŠ¥å‘Šä¿å­˜: {report_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GPUåŠ é€ŸBaseline PPLç»Ÿä¸€é‡æ–°è®¡ç®—ç³»ç»Ÿ (ä¸­å›½é•œåƒç‰ˆ)")
    print("=" * 80)
    print("ç›®æ ‡: ä½¿ç”¨ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒçš„PPLç®—æ³• (GPUåŠ é€Ÿ)")
    print("æ–¹æ³•: GPUFluencyAnalyzer + ä¸­å›½é•œåƒæº")
    print(f"é•œåƒ: {os.environ.get('HF_ENDPOINT', 'https://hf-mirror.com')}")
    print("=" * 80)
    
    # GPUç¯å¢ƒæ£€æŸ¥
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ® GPUç¯å¢ƒ: {gpu_name}")
        print(f"ğŸ’¾ GPUæ˜¾å­˜: {gpu_memory:.1f} GB")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPU (é€Ÿåº¦è¾ƒæ…¢)")
    
    print(f"ğŸŒ ç½‘ç»œé…ç½®: ä½¿ç”¨ä¸­å›½é•œåƒæºè§£å†³è®¿é—®é—®é¢˜")
    
    # åˆå§‹åŒ–é‡æ–°è®¡ç®—å™¨
    try:
        recalculator = BaselinePPLGPURecalculator(model_name="roberta-large")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥æ–‡ä»¶
    if not recalculator.check_baseline_files():
        return False
    
    # è¯¢é—®ç”¨æˆ·ç¡®è®¤
    try:
        print(f"\nğŸ¤” ç¡®è®¤å¼€å§‹GPUåŠ é€Ÿé‡æ–°è®¡ç®—å—?")
        print("   è¿™å°†ä½¿ç”¨ä¸54ä¸ªå®éªŒæ ·æœ¬å®Œå…¨ç›¸åŒçš„æ–¹æ³•è¿›è¡ŒGPUåŠ é€Ÿè®¡ç®—")
        print("   ä½¿ç”¨ä¸­å›½é•œåƒæºè§£å†³ç½‘ç»œè®¿é—®é—®é¢˜")
        confirm = input("   è¾“å…¥ 'y' ç»§ç»­: ").strip().lower()
        if confirm not in ['y', 'yes', 'æ˜¯']:
            print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return False
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return False
    
    # æ‰§è¡ŒGPUåŠ é€Ÿé‡æ–°è®¡ç®—
    try:
        start_time = datetime.now()
        results = recalculator.recalculate_all_baselines()
        end_time = datetime.now()
        
        total_time = (end_time - start_time).total_seconds()
        
        print(f"\n{'='*80}")
        print("ğŸ‰ GPUåŠ é€Ÿé‡æ–°è®¡ç®—å®Œæˆ (ä¸­å›½é•œåƒ)!")
        print(f"{'='*80}")
        print(f"âš¡ æ€»è€—æ—¶: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
        print(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(results)} ä¸ªbaselineæ–‡ä»¶")
        print(f"ğŸ“ ç»“æœç›®å½•: {recalculator.output_dir}")
        
        if results:
            ppls = [r['pseudo_ppl'] for r in results if r['pseudo_ppl'] != float('inf')]
            actual_model = results[0]['actual_model_used'] if results else 'unknown'
            if ppls:
                avg_ppl = sum(ppls) / len(ppls)
                print(f"\nğŸ“ˆ æ–°çš„GPUç»Ÿä¸€baselineå¹³å‡PPL: {avg_ppl:.3f}")
                print(f"ğŸ”§ å®é™…ä½¿ç”¨æ¨¡å‹: {actual_model}")
                print("âœ… ç°åœ¨baselineå’Œ54ä¸ªå®éªŒæ ·æœ¬ä½¿ç”¨å®Œå…¨ç›¸åŒçš„PPLç®—æ³•!")
                print("ğŸš€ GPUåŠ é€Ÿ + ä¸­å›½é•œåƒè®©è®¡ç®—æ›´å¿«æ›´ç¨³å®š!")
        
        print(f"\nğŸ“¥ ä¸‹è½½æŒ‡å—:")
        print(f"   è¯·å°†ä»¥ä¸‹æ–‡ä»¶å¤¹ä¸‹è½½åˆ°æœ¬åœ°: {recalculator.output_dir}/")
        print(f"   scp -P [ç«¯å£] -r [ç”¨æˆ·]@[GPUæœåŠ¡å™¨]:{recalculator.output_dir}/ ./")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUè®¡ç®—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
