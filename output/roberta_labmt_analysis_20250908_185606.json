{
  "metadata": {
    "total_chapters": 24,
    "primary_method": "RoBERTa",
    "validation_method": "LabMT-en-v1",
    "analysis_timestamp": "2025-09-08T18:56:06.741468"
  },
  "chapter_analysis": [
    {
      "chapter_num": 1,
      "title": "The Red Thread",
      "roberta_score": 0.0499,
      "labmt_score": 0.2958,
      "content_length": 2609
    },
    {
      "chapter_num": 2,
      "title": "Dockside Promises",
      "roberta_score": -0.5679,
      "labmt_score": 0.2506,
      "content_length": 1477
    },
    {
      "chapter_num": 3,
      "title": "The Long Dark",
      "roberta_score": -0.3759,
      "labmt_score": 0.284,
      "content_length": 2347
    },
    {
      "chapter_num": 4,
      "title": "The Wolf in the Wire",
      "roberta_score": -0.1799,
      "labmt_score": 0.1876,
      "content_length": 2034
    },
    {
      "chapter_num": 5,
      "title": "Grandmother’s Door",
      "roberta_score": -0.1859,
      "labmt_score": 0.2647,
      "content_length": 1672
    },
    {
      "chapter_num": 6,
      "title": "Teacups and Thresholds",
      "roberta_score": 0.2148,
      "labmt_score": 0.2377,
      "content_length": 2455
    },
    {
      "chapter_num": 7,
      "title": "Vale’s Parable",
      "roberta_score": -0.0614,
      "labmt_score": 0.2856,
      "content_length": 2631
    },
    {
      "chapter_num": 8,
      "title": "The Cathedral of Ice",
      "roberta_score": 0.2054,
      "labmt_score": 0.2582,
      "content_length": 1504
    },
    {
      "chapter_num": 9,
      "title": "Grandma’s Ghosts",
      "roberta_score": -0.0617,
      "labmt_score": 0.1564,
      "content_length": 1865
    },
    {
      "chapter_num": 10,
      "title": "How to Catch a Shadow",
      "roberta_score": -0.0589,
      "labmt_score": 0.2985,
      "content_length": 1287
    },
    {
      "chapter_num": 11,
      "title": "A Wolf Comes to Tea",
      "roberta_score": -0.148,
      "labmt_score": 0.2075,
      "content_length": 2495
    },
    {
      "chapter_num": 12,
      "title": "The Pact of Names",
      "roberta_score": -0.035,
      "labmt_score": 0.2918,
      "content_length": 1536
    },
    {
      "chapter_num": 13,
      "title": "The Long Walk Back",
      "roberta_score": -0.2825,
      "labmt_score": 0.246,
      "content_length": 1186
    },
    {
      "chapter_num": 14,
      "title": "What the Cloak Remembers",
      "roberta_score": 0.3232,
      "labmt_score": 0.2836,
      "content_length": 870
    },
    {
      "chapter_num": 15,
      "title": "The Mouth of the Ring",
      "roberta_score": 0.451,
      "labmt_score": 0.2377,
      "content_length": 1391
    },
    {
      "chapter_num": 16,
      "title": "A Lesson in Grammar",
      "roberta_score": 0.0036,
      "labmt_score": 0.3116,
      "content_length": 1963
    },
    {
      "chapter_num": 17,
      "title": "The Test of Hunger",
      "roberta_score": -0.5694,
      "labmt_score": 0.2538,
      "content_length": 1895
    },
    {
      "chapter_num": 18,
      "title": "Wolves at the Hearth",
      "roberta_score": -0.4226,
      "labmt_score": 0.2709,
      "content_length": 1086
    },
    {
      "chapter_num": 19,
      "title": "Red, Riding",
      "roberta_score": -0.1613,
      "labmt_score": 0.3065,
      "content_length": 1053
    },
    {
      "chapter_num": 20,
      "title": "The Red Thread (Encore)",
      "roberta_score": 0.0996,
      "labmt_score": 0.3113,
      "content_length": 2185
    },
    {
      "chapter_num": 21,
      "title": "The Moral, If You Need One",
      "roberta_score": 0.1969,
      "labmt_score": 0.2068,
      "content_length": 705
    },
    {
      "chapter_num": 22,
      "title": "Coda—A Cloak Like a Map",
      "roberta_score": 0.3217,
      "labmt_score": 0.3468,
      "content_length": 1173
    },
    {
      "chapter_num": 23,
      "title": "The Visitor with No Shoes",
      "roberta_score": -0.2077,
      "labmt_score": 0.2426,
      "content_length": 1792
    },
    {
      "chapter_num": 24,
      "title": "A Map that Eats Itself",
      "roberta_score": -0.2692,
      "labmt_score": 0.2835,
      "content_length": 1491
    }
  ],
  "primary_analysis": {
    "method": "RoBERTa",
    "scores": [
      0.04992478688557943,
      -0.5679088830947876,
      -0.3759444832801819,
      -0.1798901875813802,
      -0.18588289022445678,
      0.21478379170099896,
      -0.06138852834701538,
      0.20537372827529907,
      -0.06172101497650147,
      -0.0589471697807312,
      -0.14802411397298176,
      -0.035012352466583255,
      -0.28245150645573935,
      0.32324652671813964,
      0.45096909602483115,
      0.003574999173482259,
      -0.5694083015124003,
      -0.42256579399108884,
      -0.16131279468536378,
      0.09962358474731445,
      0.19685076291744524,
      0.32170151472091674,
      -0.2077101190884908,
      -0.2691984176635742
    ],
    "reagan_classification": {
      "method": "RoBERTa",
      "best_match": "Cinderella",
      "confidence": 0.2068,
      "all_similarities": {
        "Rags to riches": 0,
        "Tragedy": 0,
        "Man in a hole": 0.1238,
        "Icarus": 0,
        "Cinderella": 0.2068,
        "Oedipus": 0
      },
      "reagan_category": "CN"
    }
  },
  "validation_analysis": {
    "method": "LabMT",
    "scores": [
      0.29578651685393287,
      0.2506140350877193,
      0.2839606741573042,
      0.18758823529411783,
      0.26469594594594614,
      0.23773936170212795,
      0.28556451612903255,
      0.25816901408450743,
      0.15636363636363648,
      0.29846153846153856,
      0.20745000000000013,
      0.2918333333333336,
      0.24597222222222292,
      0.28362500000000024,
      0.23768867924528303,
      0.31160493827160485,
      0.2538437500000006,
      0.2708888888888892,
      0.3064705882352943,
      0.31130952380952404,
      0.20684210526315772,
      0.3468333333333333,
      0.24263636363636398,
      0.2834722222222221
    ],
    "reagan_classification": {
      "method": "LabMT",
      "best_match": "Rags to riches",
      "confidence": 0.7242,
      "all_similarities": {
        "Rags to riches": 0.7242,
        "Tragedy": 0.6694,
        "Man in a hole": 0.0437,
        "Icarus": 0,
        "Cinderella": 0,
        "Oedipus": 0.0778
      },
      "reagan_category": "RR"
    }
  },
  "correlation_analysis": {
    "pearson_correlation": {
      "r": 0.1199,
      "p_value": 0.5767,
      "significance": "Not Significant"
    },
    "spearman_correlation": {
      "r": 0.1635,
      "p_value": 0.4453
    },
    "direction_consistency": 0.6087,
    "consistency_level": "Low",
    "interpretation": "RoBERTa and LabMT correlation coefficient is 0.120, belongs to weak correlation"
  },
  "comparison_analysis": {
    "disagreement_points": [
      {
        "chapter": 2,
        "roberta_score": -0.5679088830947876,
        "labmt_score": 0.2506140350877193,
        "difference": 0.8185
      },
      {
        "chapter": 3,
        "roberta_score": -0.3759444832801819,
        "labmt_score": 0.2839606741573042,
        "difference": 0.6599
      },
      {
        "chapter": 4,
        "roberta_score": -0.1798901875813802,
        "labmt_score": 0.18758823529411783,
        "difference": 0.3675
      },
      {
        "chapter": 5,
        "roberta_score": -0.18588289022445678,
        "labmt_score": 0.26469594594594614,
        "difference": 0.4506
      },
      {
        "chapter": 7,
        "roberta_score": -0.06138852834701538,
        "labmt_score": 0.28556451612903255,
        "difference": 0.347
      },
      {
        "chapter": 10,
        "roberta_score": -0.0589471697807312,
        "labmt_score": 0.29846153846153856,
        "difference": 0.3574
      },
      {
        "chapter": 11,
        "roberta_score": -0.14802411397298176,
        "labmt_score": 0.20745000000000013,
        "difference": 0.3555
      },
      {
        "chapter": 12,
        "roberta_score": -0.035012352466583255,
        "labmt_score": 0.2918333333333336,
        "difference": 0.3268
      },
      {
        "chapter": 13,
        "roberta_score": -0.28245150645573935,
        "labmt_score": 0.24597222222222292,
        "difference": 0.5284
      },
      {
        "chapter": 16,
        "roberta_score": 0.003574999173482259,
        "labmt_score": 0.31160493827160485,
        "difference": 0.308
      },
      {
        "chapter": 17,
        "roberta_score": -0.5694083015124003,
        "labmt_score": 0.2538437500000006,
        "difference": 0.8233
      },
      {
        "chapter": 18,
        "roberta_score": -0.42256579399108884,
        "labmt_score": 0.2708888888888892,
        "difference": 0.6935
      },
      {
        "chapter": 19,
        "roberta_score": -0.16131279468536378,
        "labmt_score": 0.3064705882352943,
        "difference": 0.4678
      },
      {
        "chapter": 23,
        "roberta_score": -0.2077101190884908,
        "labmt_score": 0.24263636363636398,
        "difference": 0.4503
      },
      {
        "chapter": 24,
        "roberta_score": -0.2691984176635742,
        "labmt_score": 0.2834722222222221,
        "difference": 0.5527
      }
    ],
    "method_advantages": {
      "RoBERTa": {
        "strengths": [
          "Deep learning model with better contextual understanding",
          "Suitable for modern text and dialogue",
          "High sensitivity to sci-fi and technical texts"
        ],
        "classification": "Cinderella",
        "confidence": 0.2068
      },
      "LabMT": {
        "strengths": [
          "Fully consistent with Reagan's original method",
          "Based on large-scale human annotation",
          "Suitable for traditional literary analysis"
        ],
        "classification": "Rags to riches",
        "confidence": 0.7242
      }
    },
    "consistency_assessment": {
      "classification_agreement": false,
      "major_disagreements": 15,
      "recommendation": "Two methods differ significantly, recommend detailed analysis of difference causes, may need more in-depth text feature analysis"
    }
  },
  "final_conclusion": "RoBERTa + LabMT Dual-Method Analysis Conclusion:\n\nCorrelation Analysis:\n- Pearson correlation coefficient: r = 0.120 (Low)\n- Consistency level: RoBERTa and LabMT correlation coefficient is 0.120, belongs to weak correlation\n\nClassification Results:\n- RoBERTa (primary): Cinderella\n- LabMT (validation): Rags to riches\n\nMethodological Notes:\nThis study uses RoBERTa as the primary analysis method (modern deep learning model),\nwith LabMT for cross-validation (consistent with Reagan et al. 2016).\nThe correlation coefficient is 0.120, showing low consistency between the two methods.\n\nRecommendation: Use RoBERTa results as primary, with LabMT as academic benchmark validation."
}