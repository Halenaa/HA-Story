#!/usr/bin/env python3
"""
ä½¿ç”¨GPUè¿è¡Œbaselineæ–‡ä»¶çš„æµç•…æ€§åˆ†æ
åœ¨å®Œæˆå…¶ä»–åˆ†æåå•ç‹¬è¿è¡Œæ­¤è„šæœ¬
"""

import os
import sys
import json
import pandas as pd
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/Users/haha/Story')
sys.path.append('/Users/haha/Story/src')

from src.analysis.fluency_analyzer import FluencyAnalyzer

class BaselineFluencyGPU:
    def __init__(self):
        """åˆå§‹åŒ–GPUæµç•…æ€§åˆ†æå™¨"""
        self.baseline_files = {
            'baseline_s1': '/Users/haha/Story/output/baseline_s1.md',
            'baseline_s2': '/Users/haha/Story/output/baseline_s2.md', 
            'baseline_s3': '/Users/haha/Story/data/normal_baseline.md'
        }
        
        self.output_dir = '/Users/haha/Story/baseline_analysis_results'
        self.analyzer = None
    
    def run_fluency_analysis(self):
        """è¿è¡ŒGPUæµç•…æ€§åˆ†æ"""
        print("ğŸš€ å¼€å§‹GPUæµç•…æ€§åˆ†æ...")
        print("ğŸ“ ç¡®ä¿ä½ çš„GPUç¯å¢ƒå·²é…ç½®å®Œæˆ")
        
        # åˆå§‹åŒ–åˆ†æå™¨
        print("ğŸ”§ åˆå§‹åŒ–RoBERTaæµç•…æ€§åˆ†æå™¨...")
        self.analyzer = FluencyAnalyzer(model_name="roberta-large")
        
        results = {}
        
        for baseline_name, file_path in self.baseline_files.items():
            if not os.path.exists(file_path):
                print(f"âŒ {baseline_name}: æ–‡ä»¶ä¸å­˜åœ¨ {file_path}")
                continue
            
            print(f"\nğŸ“ [{baseline_name}] åˆ†ææµç•…æ€§...")
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è¿è¡Œåˆ†æ
            result = self.analyzer.analyze_fluency(content)
            
            # æ·»åŠ åŸºæœ¬ä¿¡æ¯
            result.update({
                'baseline_name': baseline_name,
                'source_file': file_path,
                'word_count': len(content.split()),
                'char_count': len(content)
            })
            
            # ä¿å­˜ç»“æœ
            output_file = f"{self.output_dir}/{baseline_name}/fluency_analysis.json"
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            results[baseline_name] = result
            
            print(f"   âœ… å®Œæˆ - PPL: {result['pseudo_ppl']:.2f}, é”™è¯¯ç‡: {result['err_per_100w']:.2f}")
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = f"{self.output_dir}/fluency_gpu_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ‰ GPUæµç•…æ€§åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š æˆåŠŸåˆ†æ: {len(results)}/{len(self.baseline_files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“ ç»“æœä¿å­˜åˆ°: {self.output_dir}/")
        
        return results
    
    def update_comprehensive_results(self):
        """æ›´æ–°ç»¼åˆåˆ†æç»“æœï¼ŒåŠ å…¥æµç•…æ€§æ•°æ®"""
        print("\nğŸ”„ æ›´æ–°ç»¼åˆåˆ†æç»“æœ...")
        
        for baseline_name in self.baseline_files.keys():
            # åŠ è½½æµç•…æ€§ç»“æœ
            fluency_file = f"{self.output_dir}/{baseline_name}/fluency_analysis.json"
            if not os.path.exists(fluency_file):
                print(f"âš ï¸  {baseline_name}: æœªæ‰¾åˆ°æµç•…æ€§ç»“æœ")
                continue
            
            with open(fluency_file, 'r', encoding='utf-8') as f:
                fluency_data = json.load(f)
            
            # åŠ è½½ç»¼åˆç»“æœ
            comprehensive_file = f"{self.output_dir}/{baseline_name}/comprehensive_analysis.json"
            if os.path.exists(comprehensive_file):
                with open(comprehensive_file, 'r', encoding='utf-8') as f:
                    comprehensive_data = json.load(f)
                
                # æ·»åŠ æµç•…æ€§æ•°æ®
                comprehensive_data['fluency'] = fluency_data
                
                # ä¿å­˜æ›´æ–°åçš„ç»“æœ
                with open(comprehensive_file, 'w', encoding='utf-8') as f:
                    json.dump(comprehensive_data, f, ensure_ascii=False, indent=2, default=str)
                
                print(f"   âœ… {baseline_name}: ç»¼åˆç»“æœå·²æ›´æ–°")
            else:
                print(f"   âš ï¸  {baseline_name}: æœªæ‰¾åˆ°ç»¼åˆåˆ†æç»“æœ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Baseline GPUæµç•…æ€§åˆ†æ")
    print("=" * 60)
    print("æ­¤è„šæœ¬å°†ä½¿ç”¨GPUä¸º3ä¸ªbaselineæ–‡ä»¶è¿è¡Œæµç•…æ€§åˆ†æ")
    print("ç¡®ä¿åœ¨å…¶ä»–åˆ†æå®Œæˆåè¿è¡Œæ­¤è„šæœ¬")
    print("=" * 60)
    
    analyzer = BaselineFluencyGPU()
    
    try:
        # è¿è¡Œæµç•…æ€§åˆ†æ
        results = analyzer.run_fluency_analysis()
        
        # æ›´æ–°ç»¼åˆç»“æœ
        analyzer.update_comprehensive_results()
        
        print("\nâœ¨ å®Œæˆ! ç°åœ¨å¯ä»¥è¿è¡Œupdate_metrics_with_baselines.pyæ›´æ–°CSV")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()
