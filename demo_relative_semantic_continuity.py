#!/usr/bin/env python3
"""
æ¼”ç¤ºè¯­ä¹‰è¿ç»­æ€§ç›¸å¯¹æ¯”è¾ƒç³»ç»Ÿçš„ä½¿ç”¨
å±•ç¤ºå¦‚ä½•å°†ç»å¯¹åˆ†æ•°è½¬æ¢ä¸º"é«˜äºX%æ ·æœ¬"çš„ç›¸å¯¹è¯„ä»·
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.append('/Users/haha/Story/src/utils')
sys.path.append('/Users/haha/Story/src/analysis')

from semantic_continuity_relative_comparison import SemanticContinuityRelativeComparison
from hred_coherence_evaluator import HREDSemanticContinuityEvaluator

def load_reference_data():
    """åŠ è½½å‚è€ƒæ•°æ®é›†"""
    print("ğŸ“Š åŠ è½½å‚è€ƒæ•°æ®é›†...")
    
    # ä»CSVåŠ è½½æ‰€æœ‰æ ·æœ¬çš„è¯­ä¹‰è¿ç»­æ€§åˆ†æ•°
    df = pd.read_csv('/Users/haha/Story/metrics_master_clean.csv')
    scores = df['avg_semantic_continuity'].dropna().tolist()
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(scores)} ä¸ªæ ·æœ¬ä½œä¸ºå‚è€ƒåŸºå‡†")
    print(f"   åˆ†æ•°èŒƒå›´: {min(scores):.4f} - {max(scores):.4f}")
    print(f"   å¹³å‡å€¼: {np.mean(scores):.4f}")
    print(f"   æ ‡å‡†å·®: {np.std(scores):.4f}")
    
    return scores

def analyze_story_with_relative_comparison(story_file, story_name):
    """åˆ†æå•ä¸ªæ•…äº‹å¹¶æä¾›ç›¸å¯¹æ¯”è¾ƒ"""
    print(f"\n{'='*60}")
    print(f"ğŸ” åˆ†ææ•…äº‹: {story_name}")
    print(f"{'='*60}")
    
    # è¯»å–æ•…äº‹æ–‡ä»¶
    with open(story_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # è§£æç« èŠ‚
    chapters = []
    current_chapter = None
    for line in content.split('\n'):
        if line.startswith('# Chapter '):
            if current_chapter:
                chapters.append(current_chapter)
            current_chapter = {'title': line[2:].strip(), 'plot': ''}
        elif current_chapter and line.strip():
            current_chapter['plot'] += line.strip() + ' '
    
    if current_chapter:
        chapters.append(current_chapter)
    
    # æ¸…ç†plotå­—æ®µ
    for chapter in chapters:
        chapter['plot'] = chapter['plot'].strip()
    
    print(f"ğŸ“ è§£æå‡º {len(chapters)} ä¸ªç« èŠ‚")
    
    # è¿›è¡Œè¯­ä¹‰è¿ç»­æ€§åˆ†æ
    print("ğŸ”¬ è¿›è¡Œè¯­ä¹‰è¿ç»­æ€§åˆ†æ...")
    evaluator = HREDSemanticContinuityEvaluator()
    result = evaluator.evaluate_story_semantic_continuity(chapters)
    
    # æå–åˆ†æç»“æœ
    continuity_eval = result.get('HRED_semantic_continuity_evaluation', {})
    avg_continuity = continuity_eval.get('average_semantic_continuity')
    sentence_count = continuity_eval.get('total_sentences')
    
    print(f"ğŸ“Š ç»å¯¹åˆ†æ•°ç»“æœ:")
    print(f"   å¹³å‡è¯­ä¹‰è¿ç»­æ€§: {avg_continuity:.4f}")
    print(f"   åˆ†æå¥å­æ•°: {sentence_count}")
    
    return avg_continuity

def demonstrate_relative_comparison():
    """æ¼”ç¤ºç›¸å¯¹æ¯”è¾ƒç³»ç»Ÿ"""
    print("ğŸ¯ è¯­ä¹‰è¿ç»­æ€§ç›¸å¯¹æ¯”è¾ƒç³»ç»Ÿæ¼”ç¤º")
    print("=" * 80)
    
    # 1. åŠ è½½å‚è€ƒæ•°æ®
    reference_scores = load_reference_data()
    
    # 2. åˆ›å»ºæ¯”è¾ƒç³»ç»Ÿ
    comparator = SemanticContinuityRelativeComparison()
    comparator.add_baseline_data(reference_scores, 'research_corpus')
    
    # 3. åˆ†æå‡ ä¸ªbaselineæ•…äº‹
    baseline_files = {
        'baseline_s1': '/Users/haha/Story/baseline_s1.md',
        'baseline_s2': '/Users/haha/Story/baseline_s2.md',
        'baseline_s3': '/Users/haha/Story/baseline_s3.md'
    }
    
    results = {}
    
    for story_name, story_file in baseline_files.items():
        if Path(story_file).exists():
            score = analyze_story_with_relative_comparison(story_file, story_name)
            if score is not None:
                results[story_name] = score
    
    # 4. è¿›è¡Œç›¸å¯¹æ¯”è¾ƒåˆ†æ
    print(f"\n{'='*60}")
    print("ğŸ“ˆ ç›¸å¯¹æ¯”è¾ƒåˆ†æç»“æœ")
    print(f"{'='*60}")
    
    for story_name, score in results.items():
        comparison = comparator.compare_to_baseline(score, 'research_corpus')
        
        print(f"\nğŸª {story_name}:")
        print(f"   ğŸ“Š ç»å¯¹åˆ†æ•°: {score:.4f}")
        print(f"   ğŸ“ˆ ç›¸å¯¹ä½ç½®: {comparison['comparison_description']}")
        print(f"   ğŸ¯ ç™¾åˆ†ä½æ’å: ç¬¬{comparison['percentile_rank']}ç™¾åˆ†ä½")
        print(f"   ğŸ“ è¯„ä»·ç­‰çº§: {comparison['position_description']}")
        print(f"   âš ï¸  è¯´æ˜: {comparison['measurement_note']}")
    
    # 5. ç”Ÿæˆåˆ†å¸ƒæ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ“Š å‚è€ƒæ•°æ®é›†åˆ†å¸ƒæ€»ç»“")
    print(f"{'='*60}")
    
    distribution = comparator.generate_distribution_summary('research_corpus')
    dist_summary = distribution['distribution_summary']
    
    print(f"ğŸ“ˆ åŸºå‡†æ•°æ®é›†ç»Ÿè®¡ ({distribution['sample_count']} ä¸ªæ ·æœ¬):")
    print(f"   å¹³å‡å€¼: {dist_summary['mean']:.4f}")
    print(f"   æ ‡å‡†å·®: {dist_summary['std']:.4f}")
    print(f"   åˆ†æ•°èŒƒå›´: {dist_summary['min']:.4f} - {dist_summary['max']:.4f}")
    
    percentiles = distribution['percentile_benchmarks']
    print(f"\nğŸ¯ ç™¾åˆ†ä½åŸºå‡†:")
    for desc, pct, value in [
        ("ä½æ°´å¹³åˆ†ç•Œçº¿", "25th", percentiles['p25']),
        ("ä¸­ä½æ•°", "50th", percentiles['p50']),
        ("è¾ƒé«˜æ°´å¹³åˆ†ç•Œçº¿", "75th", percentiles['p75']),
        ("é«˜æ°´å¹³åˆ†ç•Œçº¿", "90th", percentiles['p90']),
        ("æé«˜æ°´å¹³åˆ†ç•Œçº¿", "95th", percentiles['p95'])
    ]:
        print(f"   {desc} ({pct}): {value:.4f}")
    
    # 6. æ”¹è¿›å»ºè®®
    print(f"\n{'='*60}")
    print("ğŸ’¡ ç›¸å¯¹æ¯”è¾ƒç³»ç»Ÿçš„ä¼˜åŠ¿")
    print(f"{'='*60}")
    
    improvements = [
        "âœ… é¿å…ä¸»è§‚é˜ˆå€¼ï¼šä¸å†ä½¿ç”¨'0.7=ä¼˜ç§€'ç­‰æ­¦æ–­æ ‡å‡†",
        "âœ… åŸºäºæ•°æ®é©±åŠ¨ï¼šåŸºäºå®é™…æ ·æœ¬åˆ†å¸ƒè¿›è¡Œæ¯”è¾ƒ",
        "âœ… æä¾›ç›¸å¯¹ä½ç½®ï¼š'é«˜äº73.2%çš„æ ·æœ¬'æ›´æœ‰æ„ä¹‰",
        "âœ… è¯šå®è¡¨è¿°å±€é™ï¼šæ˜ç¡®è¯´æ˜ä»…æµ‹é‡è¯­ä¹‰ç›¸ä¼¼åº¦",
        "âœ… ç§‘å­¦ä¸¥è°¨æ€§ï¼šåŸºäºç™¾åˆ†ä½æ’åçš„å®¢è§‚è¯„ä»·",
        "âœ… å¯è§£é‡Šæ€§å¼ºï¼šç”¨æˆ·æ›´å®¹æ˜“ç†è§£ç›¸å¯¹ä½ç½®"
    ]
    
    for improvement in improvements:
        print(f"   {improvement}")
    
    print(f"\nğŸ‰ ç›¸å¯¹æ¯”è¾ƒç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
    print("è¿™ä¸ªç³»ç»Ÿç°åœ¨æä¾›æ›´åŠ è¯šå®ã€å‡†ç¡®å’Œç§‘å­¦çš„è¯­ä¹‰è¿ç»­æ€§è¯„ä¼°ã€‚")

if __name__ == "__main__":
    demonstrate_relative_comparison()
