"""
GPUæœåŠ¡å™¨ç‰ˆæœ¬çš„æ‰¹é‡æµç•…åº¦åˆ†æè„šæœ¬
é€‚é…æœåŠ¡å™¨ç¯å¢ƒè·¯å¾„
"""

import os
import json
import sys
from pathlib import Path
from typing import List, Dict
import pandas as pd
from datetime import datetime

# æ·»åŠ srcè·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from analysis.fluency_analyzer import FluencyAnalyzer

def find_story_files(base_dir: str) -> List[Dict]:
    """
    æŸ¥æ‰¾æ‰€æœ‰éœ€è¦åˆ†æçš„æ•…äº‹æ–‡ä»¶
    
    Args:
        base_dir: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    story_files = []
    base_path = Path(base_dir)
    
    if not base_path.exists():
        print(f"é”™è¯¯ï¼šç›®å½•ä¸å­˜åœ¨ {base_dir}")
        return story_files
    
    # éå†æ‰€æœ‰å­ç›®å½•
    for subdir in base_path.iterdir():
        if subdir.is_dir():
            # æŸ¥æ‰¾enhanced_story_dialogue_updated.mdæ–‡ä»¶
            story_file = subdir / "enhanced_story_dialogue_updated.md"
            if story_file.exists():
                story_files.append({
                    'subdir_name': subdir.name,
                    'story_file_path': str(story_file),
                    'output_dir': str(subdir)
                })
    
    print(f"æ‰¾åˆ° {len(story_files)} ä¸ªæ•…äº‹æ–‡ä»¶")
    return story_files

def read_story_content(file_path: str) -> str:
    """
    è¯»å–æ•…äº‹æ–‡ä»¶å†…å®¹
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ–‡ä»¶å†…å®¹
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return content
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return ""

def save_fluency_analysis(result: Dict, base_output_dir: str, subdir_name: str):
    """
    ä¿å­˜æµç•…åº¦åˆ†æç»“æœ
    
    Args:
        result: åˆ†æç»“æœ
        base_output_dir: åŸºç¡€è¾“å‡ºç›®å½•
        subdir_name: å­ç›®å½•åç§°
    """
    try:
        # åˆ›å»ºå¯¹åº”å‚æ•°é…ç½®çš„è¾“å‡ºç›®å½•
        output_path = Path(base_output_dir) / subdir_name
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜JSONç»“æœ
        json_file = output_path / "fluency_analysis.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"æµç•…åº¦åˆ†æç»“æœå·²ä¿å­˜: {json_file}")
        
    except Exception as e:
        print(f"ä¿å­˜ç»“æœå¤±è´¥ {subdir_name}: {e}")

def batch_analyze_fluency(input_dir: str, output_dir: str, model_name: str = "roberta-large"):
    """
    æ‰¹é‡åˆ†ææµç•…åº¦
    
    Args:
        input_dir: è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
        output_dir: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
    """
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡æµç•…åº¦åˆ†æ")
    print(f"è¾“å…¥ç›®å½•: {input_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    print("ğŸ¤– åˆå§‹åŒ–æµç•…åº¦åˆ†æå™¨...")
    analyzer = FluencyAnalyzer(model_name=model_name)
    
    # æŸ¥æ‰¾æ‰€æœ‰æ•…äº‹æ–‡ä»¶
    story_files = find_story_files(input_dir)
    
    if not story_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°éœ€è¦åˆ†æçš„æ•…äº‹æ–‡ä»¶")
        return
    
    # åˆ†æç»“æœåˆ—è¡¨
    all_results = []
    
    # é€ä¸ªåˆ†æ
    for i, file_info in enumerate(story_files, 1):
        subdir_name = file_info['subdir_name']
        story_file_path = file_info['story_file_path']
        
        print(f"\nğŸ“ [{i}/{len(story_files)}] åˆ†æ: {subdir_name}")
        
        # è¯»å–æ•…äº‹å†…å®¹
        story_content = read_story_content(story_file_path)
        if not story_content:
            print(f"â­ï¸  è·³è¿‡ç©ºæ–‡ä»¶: {subdir_name}")
            continue
        
        print(f"   æ–‡ä»¶é•¿åº¦: {len(story_content)} å­—ç¬¦, {len(story_content.split())} è¯")
        
        # åˆ†ææµç•…åº¦
        try:
            result = analyzer.analyze_fluency(story_content)
            
            # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
            result.update({
                'subdir_name': subdir_name,
                'story_file_path': story_file_path,
                'word_count': len(story_content.split()),
                'char_count': len(story_content)
            })
            
            # ä¿å­˜å•ä¸ªç»“æœ
            save_fluency_analysis(result, output_dir, subdir_name)
            
            # æ·»åŠ åˆ°æ€»ç»“æœåˆ—è¡¨
            all_results.append(result)
            
            print(f"   âœ… å®Œæˆ - PPL: {result['pseudo_ppl']:.2f}, é”™è¯¯ç‡: {result['err_per_100w']:.2f}")
            
        except Exception as e:
            print(f"   âŒ åˆ†æå¤±è´¥ {subdir_name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # å½’ä¸€åŒ–åˆ†æ•°
    if all_results:
        print(f"\nğŸ”¢ å½’ä¸€åŒ–åˆ†æ•°...")
        all_results = analyzer.normalize_scores(all_results)
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        save_summary_results(all_results, output_dir)
        
        # ç”ŸæˆCSVæŠ¥å‘Š
        generate_csv_report(all_results, output_dir)
    
    print(f"\nğŸ‰ æ‰¹é‡æµç•…åº¦åˆ†æå®Œæˆï¼å…±å¤„ç† {len(all_results)} ä¸ªæ–‡ä»¶")

def save_summary_results(results: List[Dict], output_dir: str):
    """
    ä¿å­˜æ±‡æ€»ç»“æœ
    
    Args:
        results: æ‰€æœ‰åˆ†æç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    """
    try:
        summary_file = Path(output_dir) / "fluency_analysis_summary.json"
        
        summary_data = {
            'analysis_timestamp': datetime.now().isoformat(),
            'total_files': len(results),
            'results': results
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ æ±‡æ€»ç»“æœå·²ä¿å­˜: {summary_file}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ±‡æ€»ç»“æœå¤±è´¥: {e}")

def generate_csv_report(results: List[Dict], output_dir: str):
    """
    ç”ŸæˆCSVæŠ¥å‘Š
    
    Args:
        results: æ‰€æœ‰åˆ†æç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    """
    try:
        # æå–å…³é”®æŒ‡æ ‡
        csv_data = []
        for result in results:
            csv_data.append({
                'subdir_name': result['subdir_name'],
                'pseudo_ppl': result['pseudo_ppl'],
                'err_per_100w': result['err_per_100w'],
                'pseudo_ppl_score': result.get('pseudo_ppl_score', 0),
                'grammar_score': result.get('grammar_score', 0),
                'fluency_score': result.get('fluency_score', 0),
                'word_count': result.get('word_count', 0),
                'char_count': result.get('char_count', 0),
                'error_count': result.get('error_count', 0)
            })
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(csv_data)
        
        # ä¿å­˜CSV
        csv_file = Path(output_dir) / "fluency_analysis_report.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        print(f"ğŸ“Š CSVæŠ¥å‘Šå·²ä¿å­˜: {csv_file}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ æµç•…åº¦åˆ†æç»Ÿè®¡:")
        print(f"   å¹³å‡Pseudo-PPL: {df['pseudo_ppl'].mean():.2f}")
        print(f"   å¹³å‡è¯­æ³•é”™è¯¯ç‡: {df['err_per_100w'].mean():.2f}")
        print(f"   å¹³å‡æµç•…åº¦åˆ†æ•°: {df['fluency_score'].mean():.2f}")
        print(f"   æœ€ä½³æµç•…åº¦: {df['fluency_score'].max():.2f}")
        print(f"   æœ€ä½æµç•…åº¦: {df['fluency_score'].min():.2f}")
        
    except Exception as e:
        print(f"âŒ ç”ŸæˆCSVæŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æœåŠ¡å™¨è·¯å¾„
    base_dir = "/root/Story/data/output"
    output_dir = "/root/Story/data/analysis_test/fluency_analysis"
    
    print("ğŸ” æ£€æŸ¥è¾“å…¥ç›®å½•...")
    if not os.path.exists(base_dir):
        # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
        alternative_paths = [
            "data/output",
            "./data/output",
            "Story/data/output"
        ]
        
        found = False
        for alt_path in alternative_paths:
            if os.path.exists(alt_path):
                base_dir = alt_path
                found = True
                print(f"âœ… æ‰¾åˆ°è¾“å…¥ç›®å½•: {base_dir}")
                break
        
        if not found:
            print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥ç›®å½•")
            print("ğŸ” å½“å‰ç›®å½•å†…å®¹:")
            for item in os.listdir('.'):
                print(f"   {item}")
            return
    else:
        print(f"âœ… è¾“å…¥ç›®å½•å­˜åœ¨: {base_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    
    # å¼€å§‹æ‰¹é‡åˆ†æ
    batch_analyze_fluency(base_dir, output_dir)

if __name__ == "__main__":
    main()