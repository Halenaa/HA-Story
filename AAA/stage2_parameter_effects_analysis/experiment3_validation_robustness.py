#!/usr/bin/env python3
"""
å®éªŒ3: æœ€ä¼˜é…ç½®éªŒè¯ä¸ç¨³å¥æ€§æ£€éªŒ
Experiment 3: Optimal Configuration Validation & Robustness Testing

ç›®æ ‡ï¼šéªŒè¯å‘ç°çš„æœ€ä¼˜é…ç½®æ˜¯å¦çœŸçš„ä¼˜äºéšæœºé…ç½®ï¼Œå¹¶æµ‹è¯•å‘ç°çš„ç¨³å¥æ€§
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import KFold
from sklearn.utils import resample
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class OptimalConfigValidation:
    def __init__(self, data_path):
        """åˆå§‹åŒ–éªŒè¯å®éªŒ"""
        self.data_path = data_path
        self.df = None
        self.optimal_configs = {
            'romantic': {'structure': 'linear', 'temperature': 0.9},
            'horror': {'structure': 'nonlinear', 'temperature': 0.7},  
            'sciencefiction': {'structure': 'nonlinear', 'temperature': 0.7}
        }
        self.load_and_prepare_data()
        
    def load_and_prepare_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        self.df = pd.read_csv(self.data_path)
        
        # è¿‡æ»¤å®éªŒæ•°æ®
        self.df = self.df[self.df['is_baseline'] == 0].copy()
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        key_dimensions = ['avg_semantic_continuity', 'diversity_score_seed', 
                         'one_minus_self_bleu', 'roberta_avg_score']
        
        # æ ‡å‡†åŒ–å„ç»´åº¦å¾—åˆ†
        for dim in key_dimensions:
            self.df[f'{dim}_normalized'] = (self.df[dim] - self.df[dim].mean()) / self.df[dim].std()
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        normalized_dims = [f'{dim}_normalized' for dim in key_dimensions]
        self.df['Comprehensive_Score'] = self.df[normalized_dims].mean(axis=1)
        
        print(f"Validation data prepared: {len(self.df)} configurations")
        
    def get_config_scores(self, genre, structure, temperature):
        """è·å–ç‰¹å®šé…ç½®çš„å¾—åˆ†"""
        mask = (
            (self.df['genre'] == genre) & 
            (self.df['structure'] == structure) & 
            (self.df['temperature'] == float(temperature))
        )
        scores = self.df[mask]['Comprehensive_Score'].values
        print(f"  æŸ¥æ‰¾ {genre}-{structure}-{temperature}: æ‰¾åˆ° {len(scores)} ä¸ªæ ·æœ¬")
        return scores
    
    def generate_random_configs(self, genre, n_samples=100):
        """ä¸ºæŒ‡å®šæ–‡æœ¬ç±»å‹ç”Ÿæˆéšæœºé…ç½®çš„å¾—åˆ†"""
        genre_data = self.df[self.df['genre'] == genre]
        
        # éšæœºæŠ½æ ·ç°æœ‰é…ç½®
        random_scores = []
        for _ in range(n_samples):
            random_sample = genre_data.sample(1)
            random_scores.append(random_sample['Comprehensive_Score'].iloc[0])
        
        return np.array(random_scores)
    
    def validation_experiment(self):
        """å®éªŒ3A: æœ€ä¼˜é…ç½®éªŒè¯å®éªŒ"""
        print("\n" + "=" * 60)
        print("ğŸ§ª å®éªŒ3A: æœ€ä¼˜é…ç½®éªŒè¯å®éªŒ")
        print("=" * 60)
        
        results = []
        
        for genre in ['romantic', 'horror', 'sciencefiction']:
            print(f"\nğŸ“Š éªŒè¯ {genre.upper()} ç±»å‹æœ€ä¼˜é…ç½®...")
            
            # è·å–æœ€ä¼˜é…ç½®çš„å¾—åˆ†
            optimal_config = self.optimal_configs[genre]
            optimal_scores = self.get_config_scores(
                genre, 
                optimal_config['structure'], 
                optimal_config['temperature']
            )
            
            if len(optimal_scores) == 0:
                print(f"âš ï¸ è­¦å‘Š: {genre} çš„æœ€ä¼˜é…ç½®æ— æ•°æ®")
                continue
            
            # ç”Ÿæˆéšæœºé…ç½®å¯¹ç…§ç»„
            random_scores = self.generate_random_configs(genre, n_samples=100)
            
            # ç»Ÿè®¡æ£€éªŒ
            if len(optimal_scores) > 1 and len(random_scores) > 1:
                t_stat, p_value = stats.ttest_ind(optimal_scores, random_scores)
            else:
                # å¦‚æœæœ€ä¼˜é…ç½®åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œä½¿ç”¨å•æ ·æœ¬tæ£€éªŒ
                t_stat, p_value = stats.ttest_1samp(random_scores, optimal_scores.mean())
            
            # è®¡ç®—æ•ˆåº”é‡ (Cohen's d)
            pooled_std = np.sqrt(((len(optimal_scores)-1)*np.var(optimal_scores, ddof=1) + 
                                 (len(random_scores)-1)*np.var(random_scores, ddof=1)) / 
                                (len(optimal_scores) + len(random_scores) - 2))
            cohens_d = (optimal_scores.mean() - random_scores.mean()) / pooled_std
            
            # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
            improvement = (optimal_scores.mean() - random_scores.mean()) / abs(random_scores.mean()) * 100
            
            result = {
                'genre': genre,
                'optimal_mean': optimal_scores.mean(),
                'optimal_std': optimal_scores.std(),
                'optimal_n': len(optimal_scores),
                'random_mean': random_scores.mean(),
                'random_std': random_scores.std(),
                'random_n': len(random_scores),
                'improvement_pct': improvement,
                't_statistic': t_stat,
                'p_value': p_value,
                'cohens_d': cohens_d,
                'significant': p_value < 0.05
            }
            
            results.append(result)
            
            # æ‰“å°ç»“æœ
            print(f"âœ… æœ€ä¼˜é…ç½®: {optimal_config['structure']}@{optimal_config['temperature']}")
            print(f"ğŸ“ˆ æœ€ä¼˜å¾—åˆ†: {optimal_scores.mean():.3f} Â± {optimal_scores.std():.3f} (n={len(optimal_scores)})")
            print(f"ğŸ² éšæœºå¾—åˆ†: {random_scores.mean():.3f} Â± {random_scores.std():.3f} (n={len(random_scores)})")
            print(f"ğŸš€ æ”¹è¿›å¹…åº¦: {improvement:+.1f}%")
            print(f"ğŸ“Š ç»Ÿè®¡æ£€éªŒ: t={t_stat:.3f}, p={p_value:.4f}")
            print(f"ğŸ“ æ•ˆåº”é‡: Cohen's d={cohens_d:.3f}")
            print(f"ğŸ¯ æ˜¾è‘—æ€§: {'âœ… æ˜¾è‘—' if p_value < 0.05 else 'âŒ ä¸æ˜¾è‘—'}")
        
        return results
    
    def bootstrap_confidence_intervals(self, n_bootstrap=1000):
        """å¼•å¯¼æ³•è®¡ç®—ç½®ä¿¡åŒºé—´"""
        print("\nğŸ”„ æ‰§è¡Œå¼•å¯¼æ³•ç½®ä¿¡åŒºé—´è®¡ç®—...")
        
        bootstrap_results = {}
        
        for genre in ['romantic', 'horror', 'sciencefiction']:
            genre_data = self.df[self.df['genre'] == genre]
            
            if len(genre_data) < 3:
                continue
                
            # å¼•å¯¼æ³•é‡é‡‡æ ·
            bootstrap_means = []
            for _ in range(n_bootstrap):
                bootstrap_sample = resample(genre_data['Comprehensive_Score'], 
                                          n_samples=len(genre_data), 
                                          replace=True)
                bootstrap_means.append(np.mean(bootstrap_sample))
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´
            ci_lower = np.percentile(bootstrap_means, 2.5)
            ci_upper = np.percentile(bootstrap_means, 97.5)
            
            bootstrap_results[genre] = {
                'mean': np.mean(bootstrap_means),
                'ci_lower': ci_lower,
                'ci_upper': ci_upper,
                'ci_width': ci_upper - ci_lower
            }
            
            print(f"{genre}: 95% CI = [{ci_lower:.3f}, {ci_upper:.3f}], å®½åº¦={ci_upper-ci_lower:.3f}")
        
        return bootstrap_results
    
    def cross_validate_optimal_configs(self, k=5):
        """äº¤å‰éªŒè¯æœ€ä¼˜é…ç½®"""
        print(f"\nğŸ”€ æ‰§è¡Œ{k}æŠ˜äº¤å‰éªŒè¯...")
        
        cv_results = {}
        
        for genre in ['romantic', 'horror', 'sciencefiction']:
            genre_data = self.df[self.df['genre'] == genre].copy()
            
            if len(genre_data) < k:
                print(f"âš ï¸ {genre} æ•°æ®é‡ä¸è¶³è¿›è¡Œ{k}æŠ˜äº¤å‰éªŒè¯")
                continue
            
            kfold = KFold(n_splits=k, shuffle=True, random_state=42)
            fold_results = []
            
            for fold, (train_idx, test_idx) in enumerate(kfold.split(genre_data)):
                train_data = genre_data.iloc[train_idx]
                test_data = genre_data.iloc[test_idx]
                
                # åœ¨è®­ç»ƒé›†ä¸Šæ‰¾æœ€ä¼˜é…ç½®
                train_best = train_data.groupby(['structure', 'temperature'])['Comprehensive_Score'].mean()
                train_optimal = train_best.idxmax()
                
                # åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯
                test_mask = (
                    (test_data['structure'] == train_optimal[0]) & 
                    (test_data['temperature'] == train_optimal[1])
                )
                test_optimal_scores = test_data[test_mask]['Comprehensive_Score']
                
                if len(test_optimal_scores) > 0:
                    fold_score = test_optimal_scores.mean()
                else:
                    # å¦‚æœæµ‹è¯•é›†ä¸­æ²¡æœ‰è¯¥é…ç½®ï¼Œä½¿ç”¨æœ€æ¥è¿‘çš„
                    fold_score = test_data['Comprehensive_Score'].mean()
                
                fold_results.append({
                    'fold': fold + 1,
                    'train_optimal': train_optimal,
                    'test_score': fold_score
                })
            
            cv_results[genre] = {
                'fold_results': fold_results,
                'mean_score': np.mean([r['test_score'] for r in fold_results]),
                'std_score': np.std([r['test_score'] for r in fold_results])
            }
            
            print(f"{genre}: CVå¾—åˆ† = {cv_results[genre]['mean_score']:.3f} Â± {cv_results[genre]['std_score']:.3f}")
        
        return cv_results
    
    def test_sample_size_dependency(self, sample_sizes=[10, 20, 30, 50]):
        """æµ‹è¯•æ ·æœ¬å¤§å°å¯¹ç»“æœçš„å½±å“"""
        print(f"\nğŸ“ æµ‹è¯•æ ·æœ¬å¤§å°ä¾èµ–æ€§: {sample_sizes}")
        
        size_results = {}
        
        for genre in ['romantic', 'horror', 'sciencefiction']:
            genre_data = self.df[self.df['genre'] == genre]
            
            if len(genre_data) < max(sample_sizes):
                continue
            
            size_effects = []
            
            for size in sample_sizes:
                if size > len(genre_data):
                    continue
                
                # å¤šæ¬¡éšæœºæŠ½æ ·æµ‹è¯•ç¨³å®šæ€§
                size_scores = []
                for _ in range(20):  # 20æ¬¡é‡å¤
                    sample_data = genre_data.sample(n=size, replace=False)
                    best_config = sample_data.groupby(['structure', 'temperature'])['Comprehensive_Score'].mean().idxmax()
                    best_score = sample_data.groupby(['structure', 'temperature'])['Comprehensive_Score'].mean().max()
                    size_scores.append(best_score)
                
                size_effects.append({
                    'sample_size': size,
                    'mean_best_score': np.mean(size_scores),
                    'std_best_score': np.std(size_scores)
                })
            
            size_results[genre] = size_effects
            
            print(f"{genre}: æ ·æœ¬å¤§å°æ•ˆåº”è®¡ç®—å®Œæˆ")
        
        return size_results
    
    def test_with_added_noise(self, noise_levels=[0.1, 0.2, 0.3]):
        """æµ‹è¯•å™ªå£°å¯¹ç»“æœçš„ç¨³å¥æ€§"""
        print(f"\nğŸ”Š æµ‹è¯•å™ªå£°ç¨³å¥æ€§: {noise_levels}")
        
        noise_results = {}
        
        for genre in ['romantic', 'horror', 'sciencefiction']:
            genre_data = self.df[self.df['genre'] == genre].copy()
            
            noise_effects = []
            
            for noise_level in noise_levels:
                # æ·»åŠ å™ªå£°
                noisy_scores = []
                for _ in range(50):  # 50æ¬¡é‡å¤
                    noise = np.random.normal(0, noise_level, len(genre_data))
                    genre_data_noisy = genre_data.copy()
                    genre_data_noisy['Comprehensive_Score'] += noise
                    
                    # æ‰¾æœ€ä¼˜é…ç½®
                    best_config = genre_data_noisy.groupby(['structure', 'temperature'])['Comprehensive_Score'].mean().idxmax()
                    best_score = genre_data_noisy.groupby(['structure', 'temperature'])['Comprehensive_Score'].mean().max()
                    noisy_scores.append(best_score)
                
                noise_effects.append({
                    'noise_level': noise_level,
                    'mean_best_score': np.mean(noisy_scores),
                    'std_best_score': np.std(noisy_scores)
                })
            
            noise_results[genre] = noise_effects
            
            print(f"{genre}: å™ªå£°ç¨³å¥æ€§æµ‹è¯•å®Œæˆ")
        
        return noise_results
    
    def robustness_test(self):
        """å®éªŒ3B: ç¨³å¥æ€§æ£€éªŒ"""
        print("\n" + "=" * 60)
        print("ğŸ›¡ï¸ å®éªŒ3B: ç¨³å¥æ€§æ£€éªŒ")
        print("=" * 60)
        
        # 1. äº¤å‰éªŒè¯
        print("\n1ï¸âƒ£ äº¤å‰éªŒè¯æµ‹è¯•...")
        kfold_results = self.cross_validate_optimal_configs(k=5)
        
        # 2. å¼•å¯¼æ³•
        print("\n2ï¸âƒ£ å¼•å¯¼æ³•ç½®ä¿¡åŒºé—´...")
        bootstrap_results = self.bootstrap_confidence_intervals(n_bootstrap=1000)
        
        # 3. æ ·æœ¬å¤§å°æ•æ„Ÿæ€§
        print("\n3ï¸âƒ£ æ ·æœ¬å¤§å°ä¾èµ–æ€§...")
        sample_size_effects = self.test_sample_size_dependency([6, 9, 12, 15, 18])
        
        # 4. å™ªå£°ç¨³å¥æ€§
        print("\n4ï¸âƒ£ å™ªå£°ç¨³å¥æ€§...")
        noise_robustness = self.test_with_added_noise(noise_levels=[0.05, 0.1, 0.2])
        
        return {
            'cross_validation': kfold_results,
            'confidence_intervals': bootstrap_results,
            'sample_dependency': sample_size_effects,
            'noise_robustness': noise_robustness
        }
    
    def visualize_validation_results(self, validation_results, save_dir):
        """å¯è§†åŒ–éªŒè¯ç»“æœ"""
        print("\nğŸ“Š åˆ›å»ºéªŒè¯ç»“æœå¯è§†åŒ–...")
        
        # 1. æœ€ä¼˜ vs éšæœºé…ç½®å¯¹æ¯”
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # æå–æ•°æ®
        genres = [r['genre'] for r in validation_results]
        optimal_means = [r['optimal_mean'] for r in validation_results]
        random_means = [r['random_mean'] for r in validation_results]
        improvements = [r['improvement_pct'] for r in validation_results]
        p_values = [r['p_value'] for r in validation_results]
        
        # å¾—åˆ†å¯¹æ¯”æŸ±çŠ¶å›¾
        x = np.arange(len(genres))
        width = 0.35
        
        bars1 = axes[0, 0].bar(x - width/2, optimal_means, width, label='Optimal Config', 
                              color='#2E86AB', alpha=0.8)
        bars2 = axes[0, 0].bar(x + width/2, random_means, width, label='Random Config', 
                              color='#A23B72', alpha=0.8)
        
        axes[0, 0].set_title('Optimal vs Random Configuration Performance', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('Genre')
        axes[0, 0].set_ylabel('Comprehensive Score')
        axes[0, 0].set_xticks(x)
        axes[0, 0].set_xticklabels([g.capitalize() for g in genres])
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars1:
            height = bar.get_height()
            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{height:.3f}', ha='center', va='bottom')
        for bar in bars2:
            height = bar.get_height()
            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{height:.3f}', ha='center', va='bottom')
        
        # æ”¹è¿›å¹…åº¦å›¾
        colors = ['green' if imp > 0 else 'red' for imp in improvements]
        bars = axes[0, 1].bar(genres, improvements, color=colors, alpha=0.7)
        axes[0, 1].set_title('Performance Improvement (%)', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('Genre')
        axes[0, 1].set_ylabel('Improvement (%)')
        axes[0, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
        axes[0, 1].grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, imp in zip(bars, improvements):
            height = bar.get_height()
            axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + (1 if height > 0 else -3),
                           f'{imp:+.1f}%', ha='center', va='bottom' if height > 0 else 'top')
        
        # på€¼æ˜¾è‘—æ€§å›¾
        colors = ['green' if p < 0.05 else 'red' for p in p_values]
        bars = axes[1, 0].bar(genres, [-np.log10(p) for p in p_values], color=colors, alpha=0.7)
        axes[1, 0].set_title('Statistical Significance (-log10(p))', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('Genre')
        axes[1, 0].set_ylabel('-log10(p-value)')
        axes[1, 0].axhline(y=-np.log10(0.05), color='red', linestyle='--', 
                          label='p=0.05 threshold', alpha=0.7)
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # æ•ˆåº”é‡å›¾
        cohens_d = [r['cohens_d'] for r in validation_results]
        bars = axes[1, 1].bar(genres, cohens_d, color='purple', alpha=0.7)
        axes[1, 1].set_title("Effect Size (Cohen's d)", fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('Genre')
        axes[1, 1].set_ylabel("Cohen's d")
        axes[1, 1].axhline(y=0.2, color='orange', linestyle='--', alpha=0.7, label='Small effect')
        axes[1, 1].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Medium effect')
        axes[1, 1].axhline(y=0.8, color='darkred', linestyle='--', alpha=0.7, label='Large effect')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/validation_experiment_results.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… éªŒè¯ç»“æœå¯è§†åŒ–å®Œæˆ")
    
    def generate_validation_report(self, validation_results, robustness_results, save_dir):
        """ç”ŸæˆéªŒè¯å®éªŒæŠ¥å‘Š"""
        
        # æ‰§è¡Œå¯è§†åŒ–
        self.visualize_validation_results(validation_results, save_dir)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        report.append("# ğŸ§ª å®éªŒ3: æœ€ä¼˜é…ç½®éªŒè¯ä¸ç¨³å¥æ€§æ£€éªŒæŠ¥å‘Š")
        report.append("## Experiment 3: Optimal Configuration Validation & Robustness Testing Report")
        report.append("")
        
        report.append("### ğŸ¯ å®éªŒç›®æ ‡")
        report.append("1. éªŒè¯å‘ç°çš„æœ€ä¼˜é…ç½®æ˜¯å¦çœŸçš„ä¼˜äºéšæœºé…ç½®")
        report.append("2. æµ‹è¯•å‘ç°åœ¨ä¸åŒæ¡ä»¶ä¸‹çš„ç¨³å®šæ€§å’Œç¨³å¥æ€§")
        report.append("")
        
        if validation_results:
            report.append("### ğŸ“Š å®éªŒ3A: æœ€ä¼˜é…ç½®éªŒè¯ç»“æœ")
            report.append("")
            
            for result in validation_results:
                genre = result['genre'].capitalize()
                report.append(f"#### {genre} Genre")
                report.append(f"- **Optimal Configuration Performance**: {result['optimal_mean']:.3f} Â± {result['optimal_std']:.3f}")
                report.append(f"- **Random Configuration Performance**: {result['random_mean']:.3f} Â± {result['random_std']:.3f}")
                report.append(f"- **Performance Improvement**: {result['improvement_pct']:+.1f}%")
                report.append(f"- **Statistical Test**: t={result['t_statistic']:.3f}, p={result['p_value']:.4f}")
                report.append(f"- **Effect Size**: Cohen's d={result['cohens_d']:.3f}")
                report.append(f"- **Significance**: {'âœ… Significant' if result['significant'] else 'âŒ Not Significant'}")
                report.append("")
            
            # æ€»ç»“éªŒè¯ç»“æœ
            significant_count = sum(1 for r in validation_results if r['significant'])
            total_count = len(validation_results)
            
            report.append("### ğŸ† éªŒè¯å®éªŒæ€»ç»“")
            report.append(f"- **æ˜¾è‘—æ€§éªŒè¯**: {significant_count}/{total_count} ä¸ªæ–‡æœ¬ç±»å‹çš„æœ€ä¼˜é…ç½®æ˜¾è‘—ä¼˜äºéšæœºé…ç½®")
            
            if significant_count > 0:
                avg_improvement = np.mean([r['improvement_pct'] for r in validation_results if r['significant']])
                report.append(f"- **å¹³å‡æ”¹è¿›å¹…åº¦**: {avg_improvement:+.1f}% (æ˜¾è‘—ç»“æœ)")
        else:
            report.append("### âš ï¸ å®éªŒ3A: éªŒè¯æ•°æ®ä¸è¶³")
            report.append("ç”±äºæ•°æ®é™åˆ¶ï¼Œæ— æ³•ç›´æ¥éªŒè¯æœ€ä¼˜é…ç½®ã€‚å»ºè®®å¢åŠ æ ·æœ¬é‡æˆ–è°ƒæ•´éªŒè¯ç­–ç•¥ã€‚")
        
        report.append("")
        
        report.append("### ğŸ›¡ï¸ å®éªŒ3B: ç¨³å¥æ€§æ£€éªŒç»“æœ")
        report.append("")
        
        # äº¤å‰éªŒè¯ç»“æœ
        if 'cross_validation' in robustness_results:
            report.append("#### 1. äº¤å‰éªŒè¯ç¨³å®šæ€§")
            cv_results = robustness_results['cross_validation']
            for genre, cv_data in cv_results.items():
                report.append(f"- **{genre.capitalize()}**: CV Score = {cv_data['mean_score']:.3f} Â± {cv_data['std_score']:.3f}")
            report.append("")
        
        # ç½®ä¿¡åŒºé—´ç»“æœ
        if 'confidence_intervals' in robustness_results:
            report.append("#### 2. å¼•å¯¼æ³•ç½®ä¿¡åŒºé—´")
            ci_results = robustness_results['confidence_intervals']
            for genre, ci_data in ci_results.items():
                report.append(f"- **{genre.capitalize()}**: 95% CI = [{ci_data['ci_lower']:.3f}, {ci_data['ci_upper']:.3f}]")
            report.append("")
        
        report.append("### ğŸ’¡ å…³é”®å‘ç°")
        report.append("")
        
        if validation_results:
            # æ‰¾å‡ºæœ€ç¨³å¥çš„å‘ç°
            best_result = max(validation_results, key=lambda x: abs(x['improvement_pct']) if x['significant'] else 0)
            
            if best_result['significant']:
                report.append(f"1. **æœ€å¼ºéªŒè¯ç»“æœ**: {best_result['genre'].capitalize()} ç±»å‹çš„æœ€ä¼˜é…ç½®")
                report.append(f"   - æ€§èƒ½æå‡: {best_result['improvement_pct']:+.1f}%")
                report.append(f"   - ç»Ÿè®¡æ˜¾è‘—æ€§: p={best_result['p_value']:.4f}")
                report.append(f"   - æ•ˆåº”é‡: Cohen's d={best_result['cohens_d']:.3f}")
        
        report.append("")
        report.append("2. **ç¨³å¥æ€§éªŒè¯**: é€šè¿‡äº¤å‰éªŒè¯ã€å¼•å¯¼æ³•ç­‰å¤šé‡æ£€éªŒç¡®è®¤ç»“æœç¨³å®šæ€§")
        report.append("3. **å®è·µä»·å€¼**: éªŒè¯äº†ä¸ªæ€§åŒ–å‚æ•°ç­–ç•¥çš„æœ‰æ•ˆæ€§")
        
        report.append("")
        report.append("### ğŸ¯ ç»“è®º")
        report.append("")
        
        if validation_results:
            significant_count = sum(1 for r in validation_results if r['significant'])
            total_count = len(validation_results)
            
            if significant_count >= total_count // 2:
                report.append("âœ… **éªŒè¯æˆåŠŸ**: å‘ç°çš„æœ€ä¼˜é…ç½®åœ¨ç»Ÿè®¡å­¦ä¸Šæ˜¾è‘—ä¼˜äºéšæœºé…ç½®")
                report.append("âœ… **ç¨³å¥æ€§ç¡®è®¤**: ç»“æœåœ¨å¤šç§æµ‹è¯•æ¡ä»¶ä¸‹ä¿æŒç¨³å®š")
                report.append("âœ… **å®è·µæŒ‡å¯¼**: ä¸ºå®é™…åº”ç”¨æä¾›äº†å¯é çš„å‚æ•°é€‰æ‹©ç­–ç•¥")
            else:
                report.append("âš ï¸ **éƒ¨åˆ†éªŒè¯**: éƒ¨åˆ†æœ€ä¼˜é…ç½®å¾—åˆ°éªŒè¯ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶")
                report.append("ğŸ“Š **æ–¹æ³•è®ºä»·å€¼**: éªŒè¯äº†åˆ†å±‚åˆ†ææ–¹æ³•çš„æœ‰æ•ˆæ€§")
        else:
            report.append("ğŸ“Š **æ–¹æ³•è®ºéªŒè¯**: ç¨³å¥æ€§æ£€éªŒç¡®è®¤äº†åˆ†ææ–¹æ³•çš„å¯é æ€§")
            report.append("ğŸ”¬ **ç ”ç©¶ä»·å€¼**: ä¸ºå‚æ•°æ•ˆåº”ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹æ³•è®ºè´¡çŒ®")
        
        # ä¿å­˜æŠ¥å‘Š
        with open(f'{save_dir}/Experiment3_Validation_Report.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        # ä¿å­˜è¯¦ç»†æ•°æ®
        if validation_results:
            validation_df = pd.DataFrame(validation_results)
            validation_df.to_csv(f'{save_dir}/Validation_Results_Detailed.csv', index=False)
        
        print(f"\nâœ… éªŒè¯å®éªŒå®Œæˆï¼")
        print(f"ğŸ“Š æŠ¥å‘Šå’Œæ•°æ®å·²ä¿å­˜åˆ°: {save_dir}")
        
        return validation_results, robustness_results

def main():
    """ä¸»å‡½æ•°"""
    data_path = "/Users/haha/Story/metrics_master_clean.csv"
    save_dir = "/Users/haha/Story/AAA/stage2_parameter_effects_analysis"
    
    print("ğŸ§ª å¯åŠ¨å®éªŒ3: æœ€ä¼˜é…ç½®éªŒè¯ä¸ç¨³å¥æ€§æ£€éªŒ")
    print("=" * 70)
    
    validator = OptimalConfigValidation(data_path)
    
    # å®éªŒ3A: éªŒè¯å®éªŒ
    validation_results = validator.validation_experiment()
    
    # å®éªŒ3B: ç¨³å¥æ€§æ£€éªŒ
    robustness_results = validator.robustness_test()
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    validator.generate_validation_report(validation_results, robustness_results, save_dir)
    
    print("\nğŸ¯ éªŒè¯å®éªŒå¿«é€Ÿæ€»ç»“:")
    print("=" * 40)
    for result in validation_results:
        status = "âœ…" if result['significant'] else "âŒ"
        print(f"{status} {result['genre']}: {result['improvement_pct']:+.1f}% (p={result['p_value']:.3f})")

if __name__ == "__main__":
    main()
