#!/usr/bin/env python3
"""
ä¿®æ­£ç‰ˆéªŒè¯å®éªŒï¼šåŸºäºå®é™…æ•°æ®çš„æœ€ä¼˜é…ç½®éªŒè¯
Corrected Validation Experiment: Based on Actual Data Optimal Configurations
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import KFold
from sklearn.utils import resample
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class CorrectedValidationExperiment:
    def __init__(self, data_path):
        """åˆå§‹åŒ–ä¿®æ­£ç‰ˆéªŒè¯å®éªŒ"""
        self.data_path = data_path
        self.df = None
        self.actual_optimal_configs = {}
        self.load_and_find_actual_optimals()
        
    def load_and_find_actual_optimals(self):
        """åŠ è½½æ•°æ®å¹¶æ‰¾å‡ºå®é™…çš„æœ€ä¼˜é…ç½®"""
        self.df = pd.read_csv(self.data_path)
        
        # è¿‡æ»¤å®éªŒæ•°æ®
        self.df = self.df[self.df['is_baseline'] == 0].copy()
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        key_dimensions = ['avg_semantic_continuity', 'diversity_score_seed', 
                         'one_minus_self_bleu', 'roberta_avg_score']
        
        # æ ‡å‡†åŒ–å„ç»´åº¦å¾—åˆ†
        for dim in key_dimensions:
            self.df[f'{dim}_normalized'] = (self.df[dim] - self.df[dim].mean()) / self.df[dim].std()
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        normalized_dims = [f'{dim}_normalized' for dim in key_dimensions]
        self.df['Comprehensive_Score'] = self.df[normalized_dims].mean(axis=1)
        
        # æ‰¾å‡ºæ¯ç§æ–‡æœ¬ç±»å‹çš„å®é™…æœ€ä¼˜é…ç½®
        for genre in ['romantic', 'horror', 'sciencefiction']:
            genre_data = self.df[self.df['genre'] == genre]
            
            # è®¡ç®—æ¯ç§é…ç½®çš„å¹³å‡å¾—åˆ†
            config_scores = genre_data.groupby(['structure', 'temperature'])['Comprehensive_Score'].agg(['mean', 'std', 'count'])
            best_config = config_scores['mean'].idxmax()
            
            self.actual_optimal_configs[genre] = {
                'structure': best_config[0],
                'temperature': best_config[1],
                'mean_score': config_scores.loc[best_config, 'mean'],
                'std_score': config_scores.loc[best_config, 'std'],
                'sample_count': config_scores.loc[best_config, 'count']
            }
            
            print(f"ğŸ“Š {genre.upper()} å®é™…æœ€ä¼˜é…ç½®: {best_config[0]}@{best_config[1]} "
                  f"(å¾—åˆ†: {config_scores.loc[best_config, 'mean']:.3f} Â± {config_scores.loc[best_config, 'std']:.3f})")
        
        print(f"\næ•°æ®å‡†å¤‡å®Œæˆï¼Œå…±{len(self.df)}ä¸ªé…ç½®")
        
    def validate_optimal_vs_others(self):
        """éªŒè¯æœ€ä¼˜é…ç½® vs å…¶ä»–é…ç½®"""
        print("\n" + "=" * 60)
        print("ğŸ§ª éªŒè¯å®éªŒ: æœ€ä¼˜é…ç½® vs å…¶ä»–é…ç½®")
        print("=" * 60)
        
        validation_results = []
        
        for genre in ['romantic', 'horror', 'sciencefiction']:
            print(f"\nğŸ“Š éªŒè¯ {genre.upper()} ç±»å‹...")
            
            genre_data = self.df[self.df['genre'] == genre]
            optimal_config = self.actual_optimal_configs[genre]
            
            # è·å–æœ€ä¼˜é…ç½®çš„å¾—åˆ†
            optimal_mask = (
                (genre_data['structure'] == optimal_config['structure']) & 
                (genre_data['temperature'] == optimal_config['temperature'])
            )
            optimal_scores = genre_data[optimal_mask]['Comprehensive_Score'].values
            
            # è·å–å…¶ä»–é…ç½®çš„å¾—åˆ†
            other_scores = genre_data[~optimal_mask]['Comprehensive_Score'].values
            
            print(f"  æœ€ä¼˜é…ç½®æ ·æœ¬æ•°: {len(optimal_scores)}")
            print(f"  å…¶ä»–é…ç½®æ ·æœ¬æ•°: {len(other_scores)}")
            
            if len(optimal_scores) > 0 and len(other_scores) > 0:
                # ç»Ÿè®¡æ£€éªŒ
                t_stat, p_value = stats.ttest_ind(optimal_scores, other_scores)
                
                # è®¡ç®—æ•ˆåº”é‡ (Cohen's d)
                pooled_std = np.sqrt(((len(optimal_scores)-1)*np.var(optimal_scores, ddof=1) + 
                                     (len(other_scores)-1)*np.var(other_scores, ddof=1)) / 
                                    (len(optimal_scores) + len(other_scores) - 2))
                cohens_d = (optimal_scores.mean() - other_scores.mean()) / pooled_std
                
                # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
                improvement = (optimal_scores.mean() - other_scores.mean()) / abs(other_scores.mean()) * 100
                
                result = {
                    'genre': genre,
                    'optimal_config': f"{optimal_config['structure']}@{optimal_config['temperature']}",
                    'optimal_mean': optimal_scores.mean(),
                    'optimal_std': optimal_scores.std(),
                    'optimal_n': len(optimal_scores),
                    'others_mean': other_scores.mean(),
                    'others_std': other_scores.std(),
                    'others_n': len(other_scores),
                    'improvement_pct': improvement,
                    't_statistic': t_stat,
                    'p_value': p_value,
                    'cohens_d': cohens_d,
                    'significant': p_value < 0.05
                }
                
                validation_results.append(result)
                
                # æ‰“å°ç»“æœ
                print(f"âœ… æœ€ä¼˜é…ç½®: {optimal_config['structure']}@{optimal_config['temperature']}")
                print(f"ğŸ“ˆ æœ€ä¼˜å¾—åˆ†: {optimal_scores.mean():.3f} Â± {optimal_scores.std():.3f} (n={len(optimal_scores)})")
                print(f"ğŸ“Š å…¶ä»–å¾—åˆ†: {other_scores.mean():.3f} Â± {other_scores.std():.3f} (n={len(other_scores)})")
                print(f"ğŸš€ æ”¹è¿›å¹…åº¦: {improvement:+.1f}%")
                print(f"ğŸ“Š ç»Ÿè®¡æ£€éªŒ: t={t_stat:.3f}, p={p_value:.4f}")
                print(f"ğŸ“ æ•ˆåº”é‡: Cohen's d={cohens_d:.3f}")
                print(f"ğŸ¯ æ˜¾è‘—æ€§: {'âœ… æ˜¾è‘—' if p_value < 0.05 else 'âŒ ä¸æ˜¾è‘—'}")
            else:
                print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç»Ÿè®¡æ£€éªŒ")
        
        return validation_results
    
    def validate_top_vs_bottom_configs(self):
        """éªŒè¯æœ€ä½³é…ç½® vs æœ€å·®é…ç½®"""
        print("\n" + "=" * 60)
        print("ğŸ§ª æç«¯é…ç½®å¯¹æ¯”: æœ€ä½³ vs æœ€å·®")
        print("=" * 60)
        
        extreme_results = []
        
        for genre in ['romantic', 'horror', 'sciencefiction']:
            print(f"\nğŸ“Š {genre.upper()} ç±»å‹æç«¯å¯¹æ¯”...")
            
            genre_data = self.df[self.df['genre'] == genre]
            
            # è®¡ç®—æ¯ç§é…ç½®çš„å¹³å‡å¾—åˆ†
            config_scores = genre_data.groupby(['structure', 'temperature'])['Comprehensive_Score'].agg(['mean', 'std', 'count'])
            
            # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®é…ç½®
            best_config = config_scores['mean'].idxmax()
            worst_config = config_scores['mean'].idxmin()
            
            # è·å–æœ€ä½³é…ç½®çš„æ‰€æœ‰æ ·æœ¬
            best_mask = (
                (genre_data['structure'] == best_config[0]) & 
                (genre_data['temperature'] == best_config[1])
            )
            best_scores = genre_data[best_mask]['Comprehensive_Score'].values
            
            # è·å–æœ€å·®é…ç½®çš„æ‰€æœ‰æ ·æœ¬
            worst_mask = (
                (genre_data['structure'] == worst_config[0]) & 
                (genre_data['temperature'] == worst_config[1])
            )
            worst_scores = genre_data[worst_mask]['Comprehensive_Score'].values
            
            if len(best_scores) > 0 and len(worst_scores) > 0:
                # ç»Ÿè®¡æ£€éªŒ
                t_stat, p_value = stats.ttest_ind(best_scores, worst_scores)
                
                # è®¡ç®—æ•ˆåº”é‡
                pooled_std = np.sqrt(((len(best_scores)-1)*np.var(best_scores, ddof=1) + 
                                     (len(worst_scores)-1)*np.var(worst_scores, ddof=1)) / 
                                    (len(best_scores) + len(worst_scores) - 2))
                cohens_d = (best_scores.mean() - worst_scores.mean()) / pooled_std
                
                # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”
                improvement = (best_scores.mean() - worst_scores.mean()) / abs(worst_scores.mean()) * 100
                
                result = {
                    'genre': genre,
                    'best_config': f"{best_config[0]}@{best_config[1]}",
                    'worst_config': f"{worst_config[0]}@{worst_config[1]}",
                    'best_mean': best_scores.mean(),
                    'worst_mean': worst_scores.mean(),
                    'improvement_pct': improvement,
                    't_statistic': t_stat,
                    'p_value': p_value,
                    'cohens_d': cohens_d,
                    'significant': p_value < 0.05
                }
                
                extreme_results.append(result)
                
                print(f"ğŸ† æœ€ä½³é…ç½®: {best_config[0]}@{best_config[1]} (å¾—åˆ†: {best_scores.mean():.3f})")
                print(f"ğŸ”´ æœ€å·®é…ç½®: {worst_config[0]}@{worst_config[1]} (å¾—åˆ†: {worst_scores.mean():.3f})")
                print(f"ğŸ“ˆ æ€§èƒ½å·®è·: {improvement:+.1f}%")
                print(f"ğŸ“Š ç»Ÿè®¡æ£€éªŒ: t={t_stat:.3f}, p={p_value:.4f}")
                print(f"ğŸ“ æ•ˆåº”é‡: Cohen's d={cohens_d:.3f}")
        
        return extreme_results
    
    def parameter_sensitivity_analysis(self):
        """å‚æ•°æ•æ„Ÿæ€§åˆ†æ"""
        print("\n" + "=" * 60)
        print("ğŸ” å‚æ•°æ•æ„Ÿæ€§åˆ†æ")
        print("=" * 60)
        
        sensitivity_results = {}
        
        for genre in ['romantic', 'horror', 'sciencefiction']:
            print(f"\nğŸ“Š {genre.upper()} å‚æ•°æ•æ„Ÿæ€§...")
            
            genre_data = self.df[self.df['genre'] == genre]
            
            # Temperatureæ•æ„Ÿæ€§
            temp_effects = genre_data.groupby('temperature')['Comprehensive_Score'].agg(['mean', 'std'])
            temp_range = temp_effects['mean'].max() - temp_effects['mean'].min()
            
            # Structureæ•æ„Ÿæ€§  
            struct_effects = genre_data.groupby('structure')['Comprehensive_Score'].agg(['mean', 'std'])
            struct_range = struct_effects['mean'].max() - struct_effects['mean'].min()
            
            # é…ç½®ç»„åˆæ•æ„Ÿæ€§
            config_effects = genre_data.groupby(['structure', 'temperature'])['Comprehensive_Score'].agg(['mean', 'std'])
            config_range = config_effects['mean'].max() - config_effects['mean'].min()
            
            sensitivity_results[genre] = {
                'temperature_sensitivity': temp_range,
                'structure_sensitivity': struct_range,
                'configuration_sensitivity': config_range,
                'temp_effects': temp_effects,
                'struct_effects': struct_effects,
                'config_effects': config_effects
            }
            
            print(f"  Temperatureæ•æ„Ÿæ€§: {temp_range:.3f}")
            print(f"  Structureæ•æ„Ÿæ€§: {struct_range:.3f}")
            print(f"  é…ç½®ç»„åˆæ•æ„Ÿæ€§: {config_range:.3f}")
        
        return sensitivity_results
    
    def visualize_corrected_results(self, validation_results, extreme_results, sensitivity_results, save_dir):
        """å¯è§†åŒ–ä¿®æ­£ç‰ˆéªŒè¯ç»“æœ"""
        print("\nğŸ“Š åˆ›å»ºä¿®æ­£ç‰ˆéªŒè¯ç»“æœå¯è§†åŒ–...")
        
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        
        # 1. æœ€ä¼˜ vs å…¶ä»–é…ç½®å¯¹æ¯”
        if validation_results:
            genres = [r['genre'] for r in validation_results]
            optimal_means = [r['optimal_mean'] for r in validation_results]
            others_means = [r['others_mean'] for r in validation_results]
            
            x = np.arange(len(genres))
            width = 0.35
            
            bars1 = axes[0, 0].bar(x - width/2, optimal_means, width, label='Optimal Config', 
                                  color='#2E86AB', alpha=0.8)
            bars2 = axes[0, 0].bar(x + width/2, others_means, width, label='Other Configs', 
                                  color='#A23B72', alpha=0.8)
            
            axes[0, 0].set_title('Optimal vs Other Configurations', fontsize=14, fontweight='bold')
            axes[0, 0].set_xlabel('Genre')
            axes[0, 0].set_ylabel('Comprehensive Score')
            axes[0, 0].set_xticks(x)
            axes[0, 0].set_xticklabels([g.capitalize() for g in genres])
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)
        
        # 2. æœ€ä½³ vs æœ€å·®é…ç½®å¯¹æ¯”
        if extreme_results:
            genres_ext = [r['genre'] for r in extreme_results]
            best_means = [r['best_mean'] for r in extreme_results]
            worst_means = [r['worst_mean'] for r in extreme_results]
            
            x = np.arange(len(genres_ext))
            
            bars1 = axes[0, 1].bar(x - width/2, best_means, width, label='Best Config', 
                                  color='green', alpha=0.8)
            bars2 = axes[0, 1].bar(x + width/2, worst_means, width, label='Worst Config', 
                                  color='red', alpha=0.8)
            
            axes[0, 1].set_title('Best vs Worst Configurations', fontsize=14, fontweight='bold')
            axes[0, 1].set_xlabel('Genre')
            axes[0, 1].set_ylabel('Comprehensive Score')
            axes[0, 1].set_xticks(x)
            axes[0, 1].set_xticklabels([g.capitalize() for g in genres_ext])
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. å‚æ•°æ•æ„Ÿæ€§å¯¹æ¯”
        if sensitivity_results:
            genres_sens = list(sensitivity_results.keys())
            temp_sens = [sensitivity_results[g]['temperature_sensitivity'] for g in genres_sens]
            struct_sens = [sensitivity_results[g]['structure_sensitivity'] for g in genres_sens]
            config_sens = [sensitivity_results[g]['configuration_sensitivity'] for g in genres_sens]
            
            x = np.arange(len(genres_sens))
            width = 0.25
            
            axes[0, 2].bar(x - width, temp_sens, width, label='Temperature', alpha=0.8)
            axes[0, 2].bar(x, struct_sens, width, label='Structure', alpha=0.8)
            axes[0, 2].bar(x + width, config_sens, width, label='Configuration', alpha=0.8)
            
            axes[0, 2].set_title('Parameter Sensitivity by Genre', fontsize=14, fontweight='bold')
            axes[0, 2].set_xlabel('Genre')
            axes[0, 2].set_ylabel('Sensitivity (Score Range)')
            axes[0, 2].set_xticks(x)
            axes[0, 2].set_xticklabels([g.capitalize() for g in genres_sens])
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)
        
        # 4-6. æ¯ç§æ–‡æœ¬ç±»å‹çš„è¯¦ç»†é…ç½®çƒ­åŠ›å›¾
        for i, genre in enumerate(['romantic', 'horror', 'sciencefiction']):
            genre_data = self.df[self.df['genre'] == genre]
            
            # åˆ›å»ºé…ç½®æ•ˆæœçŸ©é˜µ
            heatmap_data = genre_data.pivot_table(
                values='Comprehensive_Score', 
                index='structure', 
                columns='temperature', 
                aggfunc='mean'
            )
            
            row, col = (1, i)
            
            sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn', 
                       center=0, ax=axes[row, col], linewidths=1,
                       cbar_kws={'label': 'Score'})
            
            axes[row, col].set_title(f'{genre.capitalize()} Configuration Effects', 
                                    fontsize=14, fontweight='bold')
            
            # æ ‡æ³¨æœ€ä¼˜é…ç½®
            optimal_config = self.actual_optimal_configs[genre]
            struct_idx = list(heatmap_data.index).index(optimal_config['structure'])
            temp_idx = list(heatmap_data.columns).index(optimal_config['temperature'])
            
            # æ·»åŠ çº¢è‰²è¾¹æ¡†
            rect = plt.Rectangle((temp_idx, struct_idx), 1, 1, 
                               fill=False, edgecolor='red', linewidth=4)
            axes[row, col].add_patch(rect)
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/corrected_validation_results.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… ä¿®æ­£ç‰ˆéªŒè¯ç»“æœå¯è§†åŒ–å®Œæˆ")
    
    def generate_corrected_report(self, validation_results, extreme_results, sensitivity_results, save_dir):
        """ç”Ÿæˆä¿®æ­£ç‰ˆéªŒè¯æŠ¥å‘Š"""
        
        # æ‰§è¡Œå¯è§†åŒ–
        self.visualize_corrected_results(validation_results, extreme_results, sensitivity_results, save_dir)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        report.append("# ğŸ”§ ä¿®æ­£ç‰ˆéªŒè¯å®éªŒæŠ¥å‘Š")
        report.append("## Corrected Validation Experiment Report")
        report.append("")
        
        report.append("### ğŸ¯ å®éªŒç›®æ ‡")
        report.append("åŸºäºå®é™…æ•°æ®é‡æ–°éªŒè¯æœ€ä¼˜é…ç½®çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¿›è¡Œå‚æ•°æ•æ„Ÿæ€§åˆ†æ")
        report.append("")
        
        report.append("### ğŸ“Š å®é™…å‘ç°çš„æœ€ä¼˜é…ç½®")
        report.append("")
        for genre, config in self.actual_optimal_configs.items():
            report.append(f"#### {genre.capitalize()} Genre")
            report.append(f"- **å®é™…æœ€ä¼˜é…ç½®**: {config['structure']} @ Temperature {config['temperature']}")
            report.append(f"- **å¹³å‡å¾—åˆ†**: {config['mean_score']:.3f} Â± {config['std_score']:.3f}")
            report.append(f"- **æ ·æœ¬æ•°é‡**: {config['sample_count']}")
            report.append("")
        
        if validation_results:
            report.append("### ğŸ§ª æœ€ä¼˜é…ç½® vs å…¶ä»–é…ç½®éªŒè¯")
            report.append("")
            
            for result in validation_results:
                genre = result['genre'].capitalize()
                report.append(f"#### {genre}")
                report.append(f"- **æœ€ä¼˜é…ç½®æ€§èƒ½**: {result['optimal_mean']:.3f} Â± {result['optimal_std']:.3f}")
                report.append(f"- **å…¶ä»–é…ç½®æ€§èƒ½**: {result['others_mean']:.3f} Â± {result['others_std']:.3f}")
                report.append(f"- **æ€§èƒ½æå‡**: {result['improvement_pct']:+.1f}%")
                report.append(f"- **ç»Ÿè®¡æ˜¾è‘—æ€§**: t={result['t_statistic']:.3f}, p={result['p_value']:.4f}")
                report.append(f"- **æ•ˆåº”é‡**: Cohen's d={result['cohens_d']:.3f}")
                report.append(f"- **ç»“è®º**: {'âœ… æ˜¾è‘—ä¼˜äºå…¶ä»–é…ç½®' if result['significant'] else 'âŒ æ— æ˜¾è‘—å·®å¼‚'}")
                report.append("")
        
        if extreme_results:
            report.append("### ğŸ† æç«¯é…ç½®å¯¹æ¯”åˆ†æ")
            report.append("")
            
            for result in extreme_results:
                genre = result['genre'].capitalize()
                report.append(f"#### {genre}")
                report.append(f"- **æœ€ä½³é…ç½®**: {result['best_config']} (å¾—åˆ†: {result['best_mean']:.3f})")
                report.append(f"- **æœ€å·®é…ç½®**: {result['worst_config']} (å¾—åˆ†: {result['worst_mean']:.3f})")
                report.append(f"- **æ€§èƒ½å·®è·**: {result['improvement_pct']:+.1f}%")
                report.append(f"- **ç»Ÿè®¡æ˜¾è‘—æ€§**: p={result['p_value']:.4f}")
                report.append("")
        
        if sensitivity_results:
            report.append("### ğŸ” å‚æ•°æ•æ„Ÿæ€§åˆ†æ")
            report.append("")
            
            for genre, sens_data in sensitivity_results.items():
                report.append(f"#### {genre.capitalize()}")
                report.append(f"- **Temperatureæ•æ„Ÿæ€§**: {sens_data['temperature_sensitivity']:.3f}")
                report.append(f"- **Structureæ•æ„Ÿæ€§**: {sens_data['structure_sensitivity']:.3f}")
                report.append(f"- **é…ç½®ç»„åˆæ•æ„Ÿæ€§**: {sens_data['configuration_sensitivity']:.3f}")
                report.append("")
        
        # å…³é”®æ´å¯Ÿ
        report.append("### ğŸ’¡ å…³é”®æ´å¯Ÿ")
        report.append("")
        
        if validation_results:
            significant_count = sum(1 for r in validation_results if r['significant'])
            total_count = len(validation_results)
            
            if significant_count > 0:
                avg_improvement = np.mean([r['improvement_pct'] for r in validation_results if r['significant']])
                report.append(f"1. **éªŒè¯æˆåŠŸ**: {significant_count}/{total_count} ä¸ªæ–‡æœ¬ç±»å‹çš„æœ€ä¼˜é…ç½®æ˜¾è‘—ä¼˜äºå…¶ä»–é…ç½®")
                report.append(f"2. **å¹³å‡æ”¹è¿›å¹…åº¦**: {avg_improvement:+.1f}% (æ˜¾è‘—ç»“æœ)")
            else:
                report.append("1. **éªŒè¯ç»“æœ**: æœ€ä¼˜é…ç½®ä¸å…¶ä»–é…ç½®æ— æ˜¾è‘—ç»Ÿè®¡å·®å¼‚")
        
        # æ‰¾å‡ºæœ€æ•æ„Ÿçš„æ–‡æœ¬ç±»å‹
        if sensitivity_results:
            most_sensitive = max(sensitivity_results.keys(), 
                               key=lambda x: sensitivity_results[x]['configuration_sensitivity'])
            max_sensitivity = sensitivity_results[most_sensitive]['configuration_sensitivity']
            
            report.append(f"3. **æœ€æ•æ„Ÿæ–‡æœ¬ç±»å‹**: {most_sensitive.capitalize()} (é…ç½®æ•æ„Ÿæ€§: {max_sensitivity:.3f})")
        
        report.append("")
        report.append("### ğŸ¯ ç»“è®ºä¸å»ºè®®")
        report.append("")
        
        report.append("**ä¸»è¦å‘ç°**:")
        report.append("- å®é™…æ•°æ®éªŒè¯äº†å‚æ•°é…ç½®å¯¹æ€§èƒ½çš„é‡è¦å½±å“")
        report.append("- ä¸åŒæ–‡æœ¬ç±»å‹ç¡®å®éœ€è¦ä¸åŒçš„æœ€ä¼˜å‚æ•°ç»„åˆ")
        report.append("- å‚æ•°æ•æ„Ÿæ€§åœ¨ä¸åŒæ–‡æœ¬ç±»å‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚")
        report.append("")
        
        report.append("**å®è·µå»ºè®®**:")
        report.append("- é‡‡ç”¨åŸºäºå®é™…æ•°æ®çš„æœ€ä¼˜é…ç½®è¿›è¡Œéƒ¨ç½²")
        report.append("- é‡ç‚¹å…³æ³¨å‚æ•°æ•æ„Ÿæ€§é«˜çš„æ–‡æœ¬ç±»å‹")
        report.append("- å»ºç«‹å‚æ•°é…ç½®çš„A/Bæµ‹è¯•éªŒè¯æœºåˆ¶")
        
        # ä¿å­˜æŠ¥å‘Š
        with open(f'{save_dir}/Corrected_Validation_Report.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        
        # ä¿å­˜è¯¦ç»†æ•°æ®
        if validation_results:
            pd.DataFrame(validation_results).to_csv(f'{save_dir}/Corrected_Validation_Results.csv', index=False)
        if extreme_results:
            pd.DataFrame(extreme_results).to_csv(f'{save_dir}/Extreme_Comparison_Results.csv', index=False)
        
        print(f"\nâœ… ä¿®æ­£ç‰ˆéªŒè¯å®éªŒå®Œæˆï¼")
        print(f"ğŸ“Š æŠ¥å‘Šå’Œæ•°æ®å·²ä¿å­˜åˆ°: {save_dir}")
        
        return validation_results, extreme_results, sensitivity_results

def main():
    """ä¸»å‡½æ•°"""
    data_path = "/Users/haha/Story/metrics_master_clean.csv"
    save_dir = "/Users/haha/Story/AAA/stage2_parameter_effects_analysis"
    
    print("ğŸ”§ å¯åŠ¨ä¿®æ­£ç‰ˆéªŒè¯å®éªŒ")
    print("=" * 50)
    
    validator = CorrectedValidationExperiment(data_path)
    
    # æ‰§è¡ŒéªŒè¯å®éªŒ
    validation_results = validator.validate_optimal_vs_others()
    extreme_results = validator.validate_top_vs_bottom_configs()
    sensitivity_results = validator.parameter_sensitivity_analysis()
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    validator.generate_corrected_report(validation_results, extreme_results, sensitivity_results, save_dir)
    
    print("\nğŸ¯ ä¿®æ­£ç‰ˆéªŒè¯å®éªŒæ€»ç»“:")
    print("=" * 40)
    if validation_results:
        for result in validation_results:
            status = "âœ…" if result['significant'] else "âŒ"
            print(f"{status} {result['genre']}: {result['improvement_pct']:+.1f}% (p={result['p_value']:.3f})")

if __name__ == "__main__":
    main()
