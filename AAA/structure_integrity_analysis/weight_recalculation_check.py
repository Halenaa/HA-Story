#!/usr/bin/env python3
"""
Weight Recalculation Analysis

Check if the weight calculation needs to be updated based on the new dataset,
and analyze whether the 54 main samples composition has changed.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json

def analyze_weight_calculation_basis():
    """Analyze whether weight calculation basis has changed with new data."""
    
    print("üîç Weight Calculation Basis Analysis")
    print("="*60)
    
    # Load the updated CSV
    df = pd.read_csv('/Users/haha/Story/metrics_master_clean.csv')
    print(f"üìä Total samples in CSV: {len(df)}")
    
    # Separate main vs baseline (same logic as structure_analysis.py)
    df_main = df.query("genre!='baseline' and structure!='baseline'").copy()
    df_baseline = df.query("genre=='baseline' or structure=='baseline'").copy()
    
    print(f"üìä Sample split: {len(df_main)} main, {len(df_baseline)} baseline")
    
    # Analyze main sample composition
    print(f"\nüìã Main Sample Composition:")
    composition = df_main.groupby(['genre', 'structure']).size().unstack(fill_value=0)
    print(composition)
    
    # Analyze baseline sample details
    print(f"\nüìã Baseline Sample Details:")
    if len(df_baseline) > 0:
        baseline_details = df_baseline[['story_id', 'genre', 'structure', 'chapter_count', 'avg_coherence', 'distinct_avg']].copy()
        print(baseline_details.to_string(index=False))
        
        print(f"\nüìä Baseline Statistics:")
        print(f"  Chapter count: {df_baseline['chapter_count'].mean():.1f} ¬± {df_baseline['chapter_count'].std():.1f}")
        print(f"  Coherence: {df_baseline['avg_coherence'].mean():.3f} ¬± {df_baseline['avg_coherence'].std():.3f}")
        print(f"  Diversity: {df_baseline['distinct_avg'].mean():.3f} ¬± {df_baseline['distinct_avg'].std():.3f}")
    
    # Check TP coverage status
    print(f"\nüîç TP Coverage Analysis:")
    
    # Parse TP coverage
    def parse_tp_coverage(tp_str):
        if pd.isna(tp_str) or str(tp_str).strip() == '':
            return np.nan
        s = str(tp_str).strip()
        if '/' in s:
            m, n = s.split('/')
            return float(m) / float(n) if float(n) > 0 else np.nan
        try:
            return float(s)
        except:
            return np.nan
    
    df_main['tp_coverage_numeric'] = df_main['tp_coverage'].apply(parse_tp_coverage)
    
    print(f"  Main samples TP coverage:")
    print(f"    Mean: {df_main['tp_coverage_numeric'].mean():.3f}")
    print(f"    Std: {df_main['tp_coverage_numeric'].std():.6f}")
    print(f"    Unique values: {df_main['tp_coverage_numeric'].nunique(dropna=True)}")
    print(f"    All >= 1.0: {(df_main['tp_coverage_numeric'] >= 1.0).all()}")
    
    if len(df_baseline) > 0:
        df_baseline['tp_coverage_numeric'] = df_baseline['tp_coverage'].apply(parse_tp_coverage)
        print(f"  Baseline TP coverage:")
        print(f"    Mean: {df_baseline['tp_coverage_numeric'].mean():.3f}")
        print(f"    Std: {df_baseline['tp_coverage_numeric'].std():.6f}")
    
    # Check quality target calculation
    print(f"\nüéØ Quality Target Analysis:")
    
    if 'avg_coherence' in df_main.columns and 'distinct_avg' in df_main.columns:
        # Calculate quality target (same logic as in structure_analysis.py)
        coherence_norm = (df_main['avg_coherence'] - df_main['avg_coherence'].min()) / \
                        (df_main['avg_coherence'].max() - df_main['avg_coherence'].min())
        diversity_norm = (df_main['distinct_avg'] - df_main['distinct_avg'].min()) / \
                        (df_main['distinct_avg'].max() - df_main['distinct_avg'].min())
        quality_target = 0.5 * coherence_norm + 0.5 * diversity_norm
        
        print(f"  Quality target range: [{quality_target.min():.3f}, {quality_target.max():.3f}]")
        print(f"  Quality target mean: {quality_target.mean():.3f} ¬± {quality_target.std():.3f}")
        
        # Calculate current correlations
        print(f"\nüìà Component-Quality Correlations (Current Data):")
        
        # Event density
        event_density = df_main['total_events'] / (df_main['total_words'] / 1000)
        event_corr = event_density.corr(quality_target)
        print(f"  Event Density: r={event_corr:.3f}")
        
        # Chapter score (calculate it)
        def chapter_score(ch):
            if pd.isna(ch):
                return np.nan
            if ch < 5:
                return max(0.0, (ch - 1) / (5 - 1))
            if ch <= 8:
                return 1.0
            upper = max(8, min(14, int(df_main['chapter_count'].quantile(0.95))))
            if upper > 8:
                return max(0.0, 1 - (ch - 8) / (upper - 8))
            else:
                return 0.0
        
        chapter_scores = df_main['chapter_count'].apply(chapter_score)
        chapter_corr = chapter_scores.corr(quality_target)
        print(f"  Chapter Score: r={chapter_corr:.3f}")
        
        # Li function diversity
        li_corr = df_main['li_function_diversity'].corr(quality_target)
        print(f"  Li Function Diversity: r={li_corr:.3f}")
        
        # Calculate what the weights should be
        print(f"\nüîß Weight Calculation Check:")
        
        theoretical_baseline = {'chapter_score': 0.5, 'li_function_diversity': 0.3, 'event_density': 0.2}
        
        print("  Theoretical baseline:")
        for comp, weight in theoretical_baseline.items():
            print(f"    {comp}: {weight:.1%}")
        
        print("  Data-driven adjustments:")
        correlations = {'chapter_score': chapter_corr, 'li_function_diversity': li_corr, 'event_density': event_corr}
        
        adjusted_weights = {}
        for comp, baseline in theoretical_baseline.items():
            corr = correlations[comp]
            adjustment_factor = corr * 0.15  # ¬±15% max
            adjusted_weight = baseline * (1 + adjustment_factor)
            adjusted_weights[comp] = adjusted_weight
            print(f"    {comp}: {corr:.3f} ‚Üí {adjustment_factor:+.1%} ‚Üí {adjusted_weight:.3f}")
        
        # Normalize
        total_weight = sum(adjusted_weights.values())
        final_weights = {comp: weight/total_weight for comp, weight in adjusted_weights.items()}
        
        print("  Final normalized weights:")
        for comp, weight in final_weights.items():
            print(f"    {comp}: {weight:.1%}")
        
        return {
            'correlations': correlations,
            'theoretical_baseline': theoretical_baseline,
            'final_weights': final_weights,
            'quality_target_stats': {
                'mean': quality_target.mean(),
                'std': quality_target.std(),
                'range': [quality_target.min(), quality_target.max()]
            }
        }
    
    return None

def compare_with_stored_weights():
    """Compare calculated weights with stored weights in JSON."""
    
    print(f"\nüîç Comparing with Stored Weights...")
    
    # Load stored weights
    json_path = Path("structure_analysis_results/comprehensive_structure_report.json")
    if json_path.exists():
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        if 'weight_comparison' in data:
            stored_weights = {}
            stored_correlations = {}
            
            for comp, comp_data in data['weight_comparison'].items():
                if isinstance(comp_data, dict) and 'final_weight' in comp_data:
                    stored_weights[comp] = comp_data['final_weight']
                    stored_correlations[comp] = comp_data['correlation_with_quality']
            
            print("üìä Stored vs Expected Weights:")
            print("| Component | Stored Weight | Stored Correlation |")
            print("|-----------|---------------|-------------------|")
            
            for comp in ['chapter_score', 'li_function_diversity', 'event_density']:
                if comp in stored_weights:
                    print(f"| {comp.replace('_', ' ').title()} | {stored_weights[comp]:.1%} | r={stored_correlations[comp]:.3f} |")
            
            return stored_weights, stored_correlations
    
    return {}, {}

def main():
    """Main analysis function."""
    
    # Analyze current calculation basis
    current_analysis = analyze_weight_calculation_basis()
    
    # Compare with stored
    stored_weights, stored_correlations = compare_with_stored_weights()
    
    # Conclusion
    print(f"\nüéØ WEIGHT RECALCULATION ANALYSIS")
    print("="*60)
    
    if current_analysis:
        print("üìä Data Basis for Weight Calculation:")
        print(f"  - Main samples: 54 (unchanged)")
        print(f"  - TP gate pass rate: 100% (all 54 samples)")
        print(f"  - Quality target mean: {current_analysis['quality_target_stats']['mean']:.3f}")
        print(f"  - Quality target std: {current_analysis['quality_target_stats']['std']:.3f}")
        
        print(f"\nüìà Component Correlations:")
        for comp, corr in current_analysis['correlations'].items():
            stored_corr = stored_correlations.get(comp, 0)
            diff = abs(corr - stored_corr)
            status = "‚úÖ Match" if diff < 0.001 else f"‚ö†Ô∏è Diff: {diff:.3f}"
            print(f"  {comp.replace('_', ' ').title()}: r={corr:.3f} (stored: {stored_corr:.3f}) {status}")
        
        print(f"\nüîß Weight Recalculation Need:")
        needs_recalc = any(abs(current_analysis['correlations'][comp] - stored_correlations.get(comp, 0)) > 0.001 
                          for comp in current_analysis['correlations'].keys())
        
        if needs_recalc:
            print("‚ùå RECALCULATION NEEDED - Correlations have changed")
        else:
            print("‚úÖ NO RECALCULATION NEEDED - Correlations are identical")
            print("   The 54 main samples are the same, only baseline samples increased")
    
    return current_analysis, stored_weights

if __name__ == "__main__":
    results = main()
