#!/usr/bin/env python3
"""
æ›´æ–°ç»¼åˆåˆ†ææŠ¥å‘Šï¼ŒåŒ…å«normal_baselineæ•°æ®
"""

import pandas as pd
import json
import numpy as np
from collections import Counter

def generate_comprehensive_report():
    """ç”ŸæˆåŒ…å«æ‰€æœ‰baselineçš„ç»¼åˆåˆ†ææŠ¥å‘Š"""
    
    # è¯»å–æ›´æ–°åçš„CSVæ•°æ®
    df = pd.read_csv('/Users/haha/Story/data/core_metrics_analysis.csv')
    
    print(f"æ€»é…ç½®æ•°: {len(df)}")
    print(f"åŒ…å«çš„ç±»å‹: {df['genre'].unique()}")
    
    # åŸºç¡€ç»Ÿè®¡
    analysis_summary = {
        "total_configurations": len(df),
        "genres": dict(df['genre'].value_counts()),
        "structures": dict(df['structure'].value_counts()),
        "temperatures": dict(df['temperature'].value_counts()),
        "data_completeness": {
            "emotional_analysis": f"{df['roberta_avg_score'].notna().sum()}/{len(df)}",
            "coherence_analysis": f"{df['hred_avg_coherence'].notna().sum()}/{len(df)}",
            "structure_analysis": f"{df['tp_coverage'].notna().sum()}/{len(df)}",
            "text_features": f"{df['total_words'].notna().sum()}/{len(df)}"
        }
    }
    
    # æŒ‰ç±»å‹åˆ†ç»„åˆ†æ
    def analyze_group(group):
        result = {
            "count": int(len(group)),
            "emotional_metrics": {
                "avg_roberta_score": float(group['roberta_avg_score'].mean()),
                "reagan_classifications": dict(group['reagan_classification'].value_counts())
            },
            "coherence_metrics": {
                "avg_hred_coherence": float(group['hred_avg_coherence'].mean())
            },
            "structure_metrics": {
                "avg_tp_coverage": dict(group['tp_coverage'].value_counts()),
                "avg_li_diversity": float(group['li_function_diversity'].mean())
            },
            "text_metrics": {
                "avg_words": float(group['total_words'].mean()),
                "avg_chapters": float(group['chapter_count'].mean()),
                "avg_sentences": float(group['sentence_count'].mean())
            }
        }
        
        # è½¬æ¢value_countsç»“æœä¸­çš„numpyç±»å‹
        for key in ['reagan_classifications', 'avg_tp_coverage']:
            if key in result['emotional_metrics']:
                result['emotional_metrics'][key] = {str(k): int(v) for k, v in result['emotional_metrics'][key].items()}
            elif key in result['structure_metrics']:
                result['structure_metrics'][key] = {str(k): int(v) for k, v in result['structure_metrics'][key].items()}
        
        return result
    
    # æŒ‰ç±»å‹åˆ†æ
    genre_comparison = {}
    for genre in df['genre'].unique():
        genre_data = df[df['genre'] == genre]
        genre_comparison[genre] = analyze_group(genre_data)
        # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        for key, value in genre_comparison[genre].items():
            if isinstance(value, dict):
                for subkey, subvalue in value.items():
                    if hasattr(subvalue, 'item'):  # numpy scalar
                        genre_comparison[genre][key][subkey] = subvalue.item()
                    elif isinstance(subvalue, dict):
                        for k, v in subvalue.items():
                            if hasattr(v, 'item'):
                                genre_comparison[genre][key][subkey][k] = v.item()
    
    # æŒ‰ç»“æ„åˆ†æ
    structure_comparison = {}
    for structure in df['structure'].unique():
        structure_data = df[df['structure'] == structure]
        structure_comparison[structure] = analyze_group(structure_data)
    
    # æŒ‰æ¸©åº¦åˆ†æ
    temperature_comparison = {}
    for temp in df['temperature'].unique():
        temp_key = f"T{temp}" if temp != 'baseline' else f"T{temp}"
        temp_data = df[df['temperature'] == temp]
        temperature_comparison[temp_key] = analyze_group(temp_data)
    
    # è¯¦ç»†æŒ‡æ ‡ç»Ÿè®¡
    detailed_metrics = {
        "emotional_arc": {
            "roberta_scores": {
                "mean": df['roberta_avg_score'].mean(),
                "std": df['roberta_avg_score'].std(),
                "min": df['roberta_avg_score'].min(),
                "max": df['roberta_avg_score'].max()
            },
            "reagan_classifications": dict(df['reagan_classification'].value_counts()),
            "correlation_coefficients": {
                "mean": df['correlation_coefficient'].mean(),
                "std": df['correlation_coefficient'].std()
            }
        },
        "coherence": {
            "hred_scores": {
                "mean": df['hred_avg_coherence'].mean(),
                "std": df['hred_avg_coherence'].std(),
                "min": df['hred_avg_coherence'].min(),
                "max": df['hred_avg_coherence'].max()
            }
        },
        "structure": {
            "tp_coverage_distribution": dict(df['tp_coverage'].value_counts()),
            "li_diversity": {
                "mean": df['li_function_diversity'].mean(),
                "std": df['li_function_diversity'].std(),
                "min": df['li_function_diversity'].min(),
                "max": df['li_function_diversity'].max()
            }
        },
        "text_features": {
            "word_counts": {
                "mean": df['total_words'].mean(),
                "std": df['total_words'].std(),
                "min": df['total_words'].min(),
                "max": df['total_words'].max()
            },
            "chapter_counts": {
                "mean": df['chapter_count'].mean(),
                "std": df['chapter_count'].std(),
                "min": df['chapter_count'].min(),
                "max": df['chapter_count'].max()
            },
            "sentence_counts": {
                "mean": df['sentence_count'].mean(),
                "std": df['sentence_count'].std(),
                "min": df['sentence_count'].min(),
                "max": df['sentence_count'].max()
            }
        }
    }
    
    # ç»„è£…å®Œæ•´æŠ¥å‘Š
    comprehensive_report = {
        "analysis_summary": analysis_summary,
        "genre_comparison": genre_comparison,
        "structure_comparison": structure_comparison,
        "temperature_comparison": temperature_comparison,
        "detailed_metrics": detailed_metrics
    }
    
    return comprehensive_report

def generate_markdown_report(report_data):
    """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
    
    md_content = """# æ•…äº‹ç”Ÿæˆåˆ†æç»¼åˆæŠ¥å‘Š

## ğŸ“Š æ€»ä½“æ¦‚è§ˆ

- **æ€»é…ç½®æ•°**: {total_configs}
- **ç±»å‹åˆ†å¸ƒ**: {genre_dist}
- **ç»“æ„åˆ†å¸ƒ**: {structure_dist}
- **æ¸©åº¦åˆ†å¸ƒ**: {temp_dist}

## ğŸ“ˆ æ•°æ®å®Œæ•´æ€§

- **æƒ…æ„Ÿåˆ†æ**: {emotional_complete}
- **è¿è´¯æ€§åˆ†æ**: {coherence_complete}
- **ç»“æ„åˆ†æ**: {structure_complete}
- **æ–‡æœ¬ç‰¹å¾**: {text_complete}

## ğŸ­ æŒ‰ç±»å‹æ¯”è¾ƒ

""".format(
        total_configs=report_data["analysis_summary"]["total_configurations"],
        genre_dist=report_data["analysis_summary"]["genres"],
        structure_dist=report_data["analysis_summary"]["structures"],
        temp_dist=report_data["analysis_summary"]["temperatures"],
        emotional_complete=report_data["analysis_summary"]["data_completeness"]["emotional_analysis"],
        coherence_complete=report_data["analysis_summary"]["data_completeness"]["coherence_analysis"],
        structure_complete=report_data["analysis_summary"]["data_completeness"]["structure_analysis"],
        text_complete=report_data["analysis_summary"]["data_completeness"]["text_features"]
    )
    
    # æ·»åŠ å„ç±»å‹è¯¦ç»†åˆ†æ
    for genre, data in report_data["genre_comparison"].items():
        md_content += f"""
### {genre.title()}

- **é…ç½®æ•°**: {data["count"]}
- **å¹³å‡æƒ…æ„Ÿåˆ†æ•°**: {data["emotional_metrics"]["avg_roberta_score"]:.4f}
- **å¹³å‡è¿è´¯æ€§**: {data["coherence_metrics"]["avg_hred_coherence"]:.4f}
- **å¹³å‡LiåŠŸèƒ½å¤šæ ·æ€§**: {data["structure_metrics"]["avg_li_diversity"]:.1f}
- **å¹³å‡å­—æ•°**: {data["text_metrics"]["avg_words"]:.0f}
- **å¹³å‡ç« èŠ‚æ•°**: {data["text_metrics"]["avg_chapters"]:.1f}

**Reaganåˆ†ç±»åˆ†å¸ƒ**:
"""
        for classification, count in data["emotional_metrics"]["reagan_classifications"].items():
            md_content += f"- {classification}: {count}ä¸ª\n"
    
    return md_content

def main():
    print("="*80)
    print("æ›´æ–°ç»¼åˆåˆ†ææŠ¥å‘Šï¼ˆåŒ…å«normal_baselineï¼‰")
    print("="*80)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_comprehensive_report()
    
    # ä¿å­˜JSONæ ¼å¼
    json_file = '/Users/haha/Story/data/comprehensive_analysis_report.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_content = generate_markdown_report(report)
    md_file = '/Users/haha/Story/data/comprehensive_analysis_report.md'
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"âœ… JSONæŠ¥å‘Šå·²æ›´æ–°: {json_file}")
    print(f"âœ… MarkdownæŠ¥å‘Šå·²æ›´æ–°: {md_file}")
    
    # æ‰“å°å…³é”®ç»Ÿè®¡
    print(f"\nğŸ“Š æ›´æ–°åçš„å…³é”®ç»Ÿè®¡:")
    print(f"   æ€»é…ç½®æ•°: {report['analysis_summary']['total_configurations']}")
    print(f"   ç±»å‹åˆ†å¸ƒ: {report['analysis_summary']['genres']}")
    print(f"   å®Œæ•´æ€§æ£€æŸ¥: æƒ…æ„Ÿåˆ†æ {report['analysis_summary']['data_completeness']['emotional_analysis']}")
    
    return True

if __name__ == "__main__":
    main()
