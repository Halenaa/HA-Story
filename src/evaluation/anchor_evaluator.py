
import os
import re
import json
import argparse
import numpy as np
from tqdm import tqdm
from src.utils.utils import generate_response, load_json, save_json_absolute

try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    COHERENCE_AVAILABLE = True
except ImportError:
    COHERENCE_AVAILABLE = False
    print("âš ï¸  æœªå®‰è£… sentence-transformersï¼Œå°†è·³è¿‡è¿è´¯æ€§åˆ†æ")


def extract_chapter_texts_from_md(md_path):
    with open(md_path, "r", encoding="utf-8") as f:
        content = f.read()
    pattern = r"# Chapter \d+[:ï¼š][^\n]*"
    matches = list(re.finditer(pattern, content))
    chapters = []
    for i in range(len(matches)):
        start = matches[i].start()
        end = matches[i+1].start() if i+1 < len(matches) else len(content)
        title_line = content[start:matches[i].end()].strip()
        body = content[matches[i].end():end].strip()
        chapters.append({"chapter_id": title_line, "content": body})
    return chapters


def llm_extract_events(text, model="gpt-4.1"):
    """æ­¥éª¤1ï¼šæå–å…³é”®äº‹ä»¶"""
    prompt = f"""
è¯·ä»ä»¥ä¸‹æ•…äº‹ä¸­æå–æ‰€æœ‰å…³é”®äº‹ä»¶ã€‚è¦æ±‚ï¼š
1. æ¯ä¸ªäº‹ä»¶ç”¨ä¸€å¥è¯æè¿°ï¼ˆ10-20å­—ï¼‰
2. åªæå–æ¨åŠ¨æƒ…èŠ‚å‘å±•çš„äº‹ä»¶ï¼Œå¿½ç•¥çº¯æè¿°
3. æŒ‰æ—¶é—´é¡ºåºæ’åˆ—
4. è¾“å‡ºæ ¼å¼ï¼š["äº‹ä»¶1", "äº‹ä»¶2", "äº‹ä»¶3", ...]

æ•…äº‹æ–‡æœ¬ï¼š
{text}
"""
    
    response = generate_response([{"role": "user", "content": prompt}], model=model)
    try:
        if "```json" in response:
            response = response.split("```json")[1].split("```")[0].strip()
        elif "```" in response:
            response = response.split("```")[1].strip()
        return json.loads(response)
    except Exception as e:
        print(f"âš ï¸ äº‹ä»¶æå–å¤±è´¥ï¼š{e}")
        return []


def llm_papalampidi_annotate(events, model="gpt-4.1"):
    """æ­¥éª¤2Aï¼šPapalampidiæ ‡æ³¨"""
    events_str = json.dumps(events, ensure_ascii=False, indent=2)
    
    prompt = f"""
ç»™å®šäº‹ä»¶åˆ—è¡¨ï¼Œè¯·æ ‡æ³¨5ä¸ªå…³é”®è½¬æŠ˜ç‚¹(TP)å’Œ6ä¸ªé˜¶æ®µï¼š

äº‹ä»¶åˆ—è¡¨ï¼š{events_str}

è½¬æŠ˜ç‚¹å®šä¹‰ï¼š
- TP1 (Opportunity): å¼•å…¥ä¸»è¦æƒ…èŠ‚çš„å…³é”®äº‹ä»¶
- TP2 (Change of Plans): ç›®æ ‡/è®¡åˆ’å‘ç”Ÿæ”¹å˜çš„æ—¶åˆ»  
- TP3 (Point of No Return): è§’è‰²å…¨åŠ›æŠ•å…¥ã€æ— æ³•åé€€çš„æ‰¿è¯ºç‚¹
- TP4 (Major Setback): æœ€å¤§å±æœº/æŒ«æŠ˜æ—¶åˆ»
- TP5 (Climax): ä¸»è¦å†²çªçš„æœ€ç»ˆè§£å†³æ—¶åˆ»

é˜¶æ®µå®šä¹‰ï¼š
- Setup: èƒŒæ™¯è®¾å®šå’Œè§’è‰²ä»‹ç»
- New Situation: æ–°ç¯å¢ƒ/æ–°æŒ‘æˆ˜çš„å»ºç«‹
- Progress: æœç›®æ ‡å‰è¿›çš„è¿‡ç¨‹
- Complications: å†²çªå’Œå›°éš¾çš„å‡çº§
- Final Push: æœ€ç»ˆçš„å†³å®šæ€§è¡ŒåŠ¨
- Aftermath: ç»“æœå’Œé•¿æœŸå½±å“

è¾“å‡ºæ ¼å¼ï¼š
{{
  "è½¬æŠ˜ç‚¹": {{
    "TP1": "äº‹ä»¶X",
    "TP2": "äº‹ä»¶Y", 
    "TP3": "äº‹ä»¶Z",
    "TP4": "äº‹ä»¶A",
    "TP5": "äº‹ä»¶B"
  }},
  "é˜¶æ®µåˆ’åˆ†": {{
    "Setup": ["äº‹ä»¶1", "äº‹ä»¶2"],
    "New Situation": ["äº‹ä»¶3"],
    "Progress": ["äº‹ä»¶4", "äº‹ä»¶5"],
    "Complications": ["äº‹ä»¶6", "äº‹ä»¶7", "äº‹ä»¶8"],
    "Final Push": ["äº‹ä»¶9"],
    "Aftermath": ["äº‹ä»¶10"]
  }}
}}
"""
    
    response = generate_response([{"role": "user", "content": prompt}], model=model)
    try:
        if "```json" in response:
            response = response.split("```json")[1].split("```")[0].strip()
        elif "```" in response:
            response = response.split("```")[1].strip()
        return json.loads(response)
    except Exception as e:
        print(f"âš ï¸ Papalampidiæ ‡æ³¨å¤±è´¥ï¼š{e}")
        return {"è½¬æŠ˜ç‚¹": {}, "é˜¶æ®µåˆ’åˆ†": {}}


def llm_li_annotate(events, model="gpt-4.1"):
    """æ­¥éª¤2Bï¼šLiæ ‡æ³¨"""
    events_str = json.dumps(events, ensure_ascii=False, indent=2)
    
    prompt = f"""
ç»™å®šäº‹ä»¶åˆ—è¡¨ï¼Œè¯·ç”¨10ä¸ªåŠŸèƒ½æ ‡ç­¾æ ‡æ³¨æ¯ä¸ªäº‹ä»¶ï¼š

äº‹ä»¶åˆ—è¡¨ï¼š{events_str}

åŠŸèƒ½æ ‡ç­¾å®šä¹‰ï¼š
- Abstract: æ•…äº‹è¦ç‚¹çš„æ€»ç»“
- Orientation: èƒŒæ™¯è®¾å®šï¼ˆæ—¶é—´ã€åœ°ç‚¹ã€äººç‰©ï¼‰
- Complicating Action: å¢åŠ å¼ åŠ›ã€æ¨åŠ¨æƒ…èŠ‚çš„äº‹ä»¶
- MRE (Most Reportable Event): æœ€é‡è¦/æœ€å€¼å¾—æŠ¥å‘Šçš„äº‹ä»¶
- Minor Resolution: éƒ¨åˆ†ç¼“è§£å¼ åŠ›çš„äº‹ä»¶
- Return of MRE: MREä¸»é¢˜çš„é‡æ–°å‡ºç°
- Resolution: è§£å†³ä¸»è¦å†²çªçš„äº‹ä»¶  
- Aftermath: ä¸»è¦äº‹ä»¶åçš„é•¿æœŸå½±å“
- Evaluation: å™è¿°è€…å¯¹æ•…äº‹æ„ä¹‰çš„è¯„è®º
- Direct Comment: å¯¹è§‚ä¼—çš„ç›´æ¥è¯„è®º

è¾“å‡ºæ ¼å¼ï¼š
{{
  "äº‹ä»¶1": "Orientation",
  "äº‹ä»¶2": "Complicating Action",
  "äº‹ä»¶3": "MRE",
  ...
}}
"""
    
    response = generate_response([{"role": "user", "content": prompt}], model=model)
    try:
        if "```json" in response:
            response = response.split("```json")[1].split("```")[0].strip()
        elif "```" in response:
            response = response.split("```")[1].strip()
        return json.loads(response)
    except Exception as e:
        print(f"âš ï¸ Liæ ‡æ³¨å¤±è´¥ï¼š{e}")
        return {}


def score_papalampidi(result):
    """Papalampidiè¯„åˆ†ï¼ˆ10åˆ†åˆ¶ï¼‰"""
    score = 0
    
    # TPæ¸…æ™°åº¦ (5åˆ†)
    tp_count = len([tp for tp in result.get("è½¬æŠ˜ç‚¹", {}).values() if tp and tp.strip()])
    score += tp_count  # æ¯ä¸ªæ¸…æ™°TPå¾—1åˆ†
    
    # é˜¶æ®µå®Œæ•´æ€§ (5åˆ†) 
    stages = result.get("é˜¶æ®µåˆ’åˆ†", {})
    stage_count = len([stage for stage in stages.values() if stage and len(stage) > 0])
    score += min(stage_count, 5)  # æœ€å¤š5åˆ†
    
    return min(score, 10)


def score_li(result):
    """Liè¯„åˆ†ï¼ˆ10åˆ†åˆ¶ï¼‰"""
    score = 0
    labels = list(result.values())
    
    # æ ¸å¿ƒåŠŸèƒ½æ£€æŸ¥ (6åˆ†)
    core_functions = ["Orientation", "Complicating Action", "MRE", "Resolution"]
    for func in core_functions:
        if func in labels:
            score += 1.5  # æ¯ä¸ªæ ¸å¿ƒåŠŸèƒ½1.5åˆ†
    
    # åŠŸèƒ½å¤šæ ·æ€§ (4åˆ†)
    unique_labels = len(set(labels))
    score += min(unique_labels * 0.4, 4)  # æ ‡ç­¾ç§ç±»è¶Šå¤šåˆ†è¶Šé«˜
    
    return min(score, 10)


def calculate_coherence(events):
    """è®¡ç®—è¿è´¯æ€§åˆ†æ•°"""
    if not COHERENCE_AVAILABLE or len(events) < 2:
        return {"å¹³å‡è¿è´¯æ€§": 0, "è¿è´¯æ€§ç­‰çº§": "æ— æ³•è®¡ç®—"}
    
    try:
        model = SentenceTransformer('all-MiniLM-L6-v2')
        event_embeddings = model.encode(events)
        
        coherence_scores = []
        for i in range(len(event_embeddings) - 1):
            similarity = cosine_similarity([event_embeddings[i]], [event_embeddings[i+1]])[0][0]
            coherence_scores.append(similarity)
        
        avg_coherence = np.mean(coherence_scores)
        min_coherence = np.min(coherence_scores)
        worst_gap = np.argmin(coherence_scores)
        
        if avg_coherence >= 0.7:
            level = "ä¼˜ç§€"
        elif avg_coherence >= 0.5:
            level = "è‰¯å¥½" 
        elif avg_coherence >= 0.3:
            level = "åŠæ ¼"
        else:
            level = "éœ€æ”¹è¿›"
        
        return {
            "å¹³å‡è¿è´¯æ€§": round(avg_coherence, 3),
            "æœ€ä½è¿è´¯æ€§": round(min_coherence, 3),
            "æœ€å¤§è·³è·ƒä½ç½®": worst_gap,
            "è¿è´¯æ€§ç­‰çº§": level,
            "è¯¦ç»†åˆ†æ•°": [round(s, 3) for s in coherence_scores]
        }
    except Exception as e:
        print(f"âš ï¸ è¿è´¯æ€§è®¡ç®—å¤±è´¥ï¼š{e}")
        return {"å¹³å‡è¿è´¯æ€§": 0, "è¿è´¯æ€§ç­‰çº§": "è®¡ç®—å¤±è´¥"}


def evaluate_story_dual_track(input_path, output_path, model="gpt-4.1"):
    """ä¸»å‡½æ•°ï¼šåŒè½¨å¹¶è¡Œæ•…äº‹è¯„ä»·"""
    print(f"\nğŸ” å¼€å§‹åŒè½¨è¯„ä»·ï¼š{input_path}")
    
    # è¯»å–è¾“å…¥æ–‡ä»¶
    if input_path.endswith(".json"):
        story = load_json(input_path)
        contents = [c.get("plot", "") for c in story]
        chapter_ids = [f"Chapter {i+1}" for i in range(len(contents))]
    elif input_path.endswith(".md"):
        chapters = extract_chapter_texts_from_md(input_path)
        chapter_ids = [c["chapter_id"] for c in chapters]
        contents = [c["content"] for c in chapters]
    else:
        raise ValueError("åªæ”¯æŒ .json æˆ– .md æ–‡ä»¶è¾“å…¥")

    # åˆå¹¶æ‰€æœ‰ç« èŠ‚æ–‡æœ¬
    full_text = "\n\n".join([f"{cid}: {content}" for cid, content in zip(chapter_ids, contents)])
    
    print("\nğŸ“‹ æ­¥éª¤1ï¼šæå–å…³é”®äº‹ä»¶...")
    events = llm_extract_events(full_text, model=model)
    
    print(f"âœ… æå–åˆ° {len(events)} ä¸ªäº‹ä»¶")
    
    print("\nğŸ“Š æ­¥éª¤2Aï¼šPapalampidiç»“æ„åˆ†æ...")
    papalampidi_result = llm_papalampidi_annotate(events, model=model)
    papalampidi_score = score_papalampidi(papalampidi_result)
    
    print("\nğŸ·ï¸  æ­¥éª¤2Bï¼šLiåŠŸèƒ½åˆ†æ...")
    li_result = llm_li_annotate(events, model=model)
    li_score = score_li(li_result)
    
    print("\nğŸ”— æ­¥éª¤3ï¼šè¿è´¯æ€§åˆ†æ...")
    coherence_result = calculate_coherence(events)
    
    # æ±‡æ€»ç»“æœ
    evaluation_result = {
        "åŸºæœ¬ä¿¡æ¯": {
            "æ–‡ä»¶è·¯å¾„": input_path,
            "ç« èŠ‚æ•°é‡": len(contents),
            "äº‹ä»¶æ•°é‡": len(events)
        },
        "äº‹ä»¶åˆ—è¡¨": events,
        "Papalampidiè¯„ä»·": {
            "å¾—åˆ†": f"{papalampidi_score}/10",
            "è¯¦ç»†ç»“æœ": papalampidi_result
        },
        "Liè¯„ä»·": {
            "å¾—åˆ†": f"{li_score}/10", 
            "è¯¦ç»†ç»“æœ": li_result
        },
        "è¿è´¯æ€§è¯„ä»·": coherence_result,
        "ç»¼åˆè¯Šæ–­": {
            "ç»“æ„è´¨é‡": "ä¼˜ç§€" if papalampidi_score >= 8 else "è‰¯å¥½" if papalampidi_score >= 6 else "éœ€æ”¹è¿›",
            "åŠŸèƒ½è´¨é‡": "ä¼˜ç§€" if li_score >= 8 else "è‰¯å¥½" if li_score >= 6 else "éœ€æ”¹è¿›",
            "è¿è´¯æ€§è´¨é‡": coherence_result.get("è¿è´¯æ€§ç­‰çº§", "æ— æ³•è®¡ç®—")
        }
    }
    
    # ä¿å­˜ç»“æœ
    save_json_absolute(evaluation_result, output_path)
    
    # æ‰“å°æ‘˜è¦
    print(f"""
ğŸ“Š è¯„ä»·å®Œæˆï¼ç»“æœæ‘˜è¦ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ˆ ç»“æ„è´¨é‡ (Papalampidi): {papalampidi_score}/10 ({evaluation_result['ç»¼åˆè¯Šæ–­']['ç»“æ„è´¨é‡']})
ğŸ·ï¸  åŠŸèƒ½è´¨é‡ (Li):        {li_score}/10 ({evaluation_result['ç»¼åˆè¯Šæ–­']['åŠŸèƒ½è´¨é‡']})
ğŸ”— è¿è´¯æ€§è´¨é‡:            {coherence_result.get('å¹³å‡è¿è´¯æ€§', 0)} ({coherence_result.get('è¿è´¯æ€§ç­‰çº§', 'æ— æ³•è®¡ç®—')})
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³ï¼š{output_path}
""")
    
    return evaluation_result


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="åŒè½¨å¹¶è¡Œæ•…äº‹è¯„ä»·ç³»ç»Ÿ")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼š.json æˆ– .md")
    parser.add_argument("--output", type=str, default="story_evaluation_result.json", help="è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", type=str, default="gpt-4.1", help="ä½¿ç”¨çš„ LLM æ¨¡å‹åç§°")

    args = parser.parse_args()
    evaluate_story_dual_track(args.input, args.output, args.model)

# import os
# import json
# import argparse
# from tqdm import tqdm
# from src.utils.utils import load_json, generate_response
# from src.utils.utils import convert_json_safe as convert_json


# # ğŸ‘‡ LLM æ¨¡å‹åˆ¤æ–­ä¸¤é”šç‚¹æ˜¯å¦åŒä¹‰ï¼ˆæ”¯æŒ markdown åŒ…è£¹ï¼‰
# def is_similar(a: str, b: str, threshold=0.85, model="gpt-4.1") -> bool:
#     prompt = f"""
# ä½ æ˜¯ä¸€ä¸ªå™äº‹ç»“æ„ä¸“å®¶ï¼Œè¯·åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªé”šç‚¹æ˜¯å¦è¡¨è¾¾ç›¸åŒçš„ç»“æ„åŠŸèƒ½ï¼š

# A: ã€Œ{a}ã€
# B: ã€Œ{b}ã€

# å¦‚æœå®ƒä»¬è¯­ä¹‰ç›¸è¿‘ã€æ‰¿æ‹…ç›¸ä¼¼æƒ…èŠ‚è§’è‰²ï¼ˆå¦‚ä»»åŠ¡å¼€å§‹ã€å±æœºçˆ†å‘ã€åæ´¾å‡ºç°ï¼‰ï¼Œè¯·åˆ¤æ–­ä¸ºåŒä¹‰ã€‚

# è¯·è¿”å›å¦‚ä¸‹æ ¼å¼ï¼ˆä»… JSONï¼‰ï¼š
# {{"is_paraphrase": true æˆ– false, "reason": "ç®€è¦è¯´æ˜"}}
#     """.strip()

#     response = generate_response([{"role": "user", "content": prompt}], model=model)

#     try:
#         result = convert_json(response)
#         return result.get("is_paraphrase", False)
#     except Exception as e:
#         print(f"âŒ æ— æ³•è§£ææ¨¡å‹è¾“å‡º: {e}\nåŸå§‹è¿”å›ï¼š{response[:80]}")
#         return False

# def flatten_dual_anchors(path, level="core"):
#     data = load_json(path)
#     anchors = []
#     key = f"{level}_anchors"
#     for ch in data:
#         for a in ch.get(key, []):
#             anchor = a.copy()
#             anchor["chapter_id"] = ch["chapter_id"]
#             anchors.append(anchor)
#     return anchors

# def load_reference(path):
#     data = load_json(path)
#     if isinstance(data, dict) and "anchors" in data:
#         # æ”¯æŒ { "anchors": ["ä»»åŠ¡1", "ä»»åŠ¡2", ...] }
#         return [{"surface": s, "type": None} for s in data["anchors"]]
#     elif isinstance(data, list):
#         return data
#     else:
#         raise ValueError("ä¸æ”¯æŒçš„å‚è€ƒé”šç‚¹æ ¼å¼")

# def match_surface(ref_surface, generated_surfaces, threshold=0.85):
#     for g in generated_surfaces:
#         if is_similar(ref_surface, g, threshold=threshold):
#             return g
#     return None

# def match_functional(ref_type, generated_types):
#     return ref_type in generated_types

# def evaluate_anchors(generated_path, reference_path, use_surface=True, use_functional=True,
#                      explain=False, generated_level=None):
#     # è¯»å–ç”Ÿæˆé”šç‚¹
#     if generated_level:
#         generated = flatten_dual_anchors(generated_path, level=generated_level)
#     else:
#         generated = load_json(generated_path)

#     # è¯»å–å‚è€ƒé”šç‚¹
#     reference = load_reference(reference_path)

#     results = []
#     total = len(reference)
#     surface_hits = 0
#     type_hits = 0

#     gen_surfaces = [a.get("surface", "") for a in generated]
#     gen_types = [a.get("type", "") for a in generated]

#     for ref in tqdm(reference, desc="ğŸ” åŒ¹é…é”šç‚¹"):
#         match = {"ref": ref, "surface_match": False, "type_match": False, "matched_to": None}

#         if use_surface and ref.get("surface"):
#             matched_surface = match_surface(ref["surface"], gen_surfaces)
#             if matched_surface:
#                 match["surface_match"] = True
#                 match["matched_to"] = matched_surface
#                 surface_hits += 1

#         if use_functional and ref.get("type"):
#             if match_functional(ref["type"], gen_types):
#                 match["type_match"] = True
#                 type_hits += 1

#         if explain and not match["surface_match"]:
#             prompt = f"å‚è€ƒé”šç‚¹ï¼š{ref['surface']}\næœªåŒ¹é…ç”Ÿæˆé”šç‚¹ï¼Œè¯·åˆ¤æ–­ä¸ºä½•æœªå‘½ä¸­ï¼Ÿæ˜¯å¦è¡¨è¾¾æ–¹å¼å˜åŒ–æˆ–ç»“æ„é—æ¼ï¼Ÿ"
#             match["llm_reason"] = generate_response([{"role": "user", "content": prompt}])

#         results.append(match)

#     print("\nâœ… é”šç‚¹å¯¹é½ç»Ÿè®¡ç»“æœï¼š")
#     if use_surface:
#         print(f"ğŸ”¹ Surface å‘½ä¸­æ•°ï¼š{surface_hits} / {total}ï¼ˆå‘½ä¸­ç‡ {surface_hits / total:.2%}ï¼‰")
#     if use_functional:
#         print(f"ğŸ”¹ Functional ç±»å‹åŒ¹é…ï¼š{type_hits} / {total}ï¼ˆå‘½ä¸­ç‡ {type_hits / total:.2%}ï¼‰")

#     return results

# def save_result(results, output_path="anchor_eval_result.json"):
#     with open(output_path, "w", encoding="utf-8") as f:
#         json.dump(results, f, ensure_ascii=False, indent=2)
#     print(f"\nğŸ“„ åŒ¹é…è¯¦æƒ…å·²ä¿å­˜è‡³ï¼š{output_path}")

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--generated", type=str, required=True, help="ç”Ÿæˆçš„ anchors è·¯å¾„")
#     parser.add_argument("--reference", type=str, required=True, help="å‚è€ƒ anchors è·¯å¾„")
#     parser.add_argument("--surface", action="store_true", help="æ˜¯å¦å¯ç”¨ surface åŒ¹é…")
#     parser.add_argument("--functional", action="store_true", help="æ˜¯å¦å¯ç”¨ functional ç±»å‹åŒ¹é…")
#     parser.add_argument("--explain", action="store_true", help="æ˜¯å¦å¯ç”¨ LLM é”™è¯¯è§£é‡Š")
#     parser.add_argument("--generated_level", choices=["core", "fine"], help="dualç»“æ„ä¸­æå–çš„å±‚çº§")
#     parser.add_argument("--output", type=str, default="anchor_eval_result.json")

#     args = parser.parse_args()
#     results = evaluate_anchors(
#         generated_path=args.generated,
#         reference_path=args.reference,
#         use_surface=args.surface,
#         use_functional=args.functional,
#         explain=args.explain,
#         generated_level=args.generated_level
#     )
#     save_result(results, args.output)
