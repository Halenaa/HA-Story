import os
import json
import numpy as np
from collections import Counter
from src.utils.utils import generate_response, save_json, load_json


# def extract_events_from_story(story_data, model="gpt-4.1"):
#     """
#     æ”¹è¿›ç‰ˆï¼šä¸¤æ­¥æ³•æå–äº‹ä»¶
#     ç¬¬ä¸€æ­¥ï¼šæå–äº‹ä»¶æè¿°
#     ç¬¬äºŒæ­¥ï¼šéªŒè¯åŸæ–‡ä¾æ®
#     """
#     # å‡†å¤‡åŸæ–‡
#     chapters_with_ids = []
#     for i, ch in enumerate(story_data):
#         chapter_text = f"ã€{ch.get('chapter_id', f'Chapter {i+1}')}ã€‘{ch.get('plot', '')}"
#         chapters_with_ids.append(chapter_text)
    
#     full_plot = "\n\n".join(chapters_with_ids)
    
#     print("  ç¬¬ä¸€æ­¥ï¼šæå–äº‹ä»¶æè¿°...")
    
#     # ç¬¬ä¸€æ­¥ï¼šåªæå–äº‹ä»¶ï¼Œä¸è¦æ±‚åŸæ–‡ä¾æ®
#     step1_prompt = f"""
# è¯·ä»ä»¥ä¸‹æ•…äº‹ä¸­æå–æ‰€æœ‰å…³é”®äº‹ä»¶ã€‚

# è¦æ±‚ï¼š
# 1. æ¯ä¸ªäº‹ä»¶ç”¨10-20å­—ç®€æ´æè¿°
# 2. æŒ‰æ—¶é—´é¡ºåºæ’åˆ—
# 3. åªè¾“å‡ºäº‹ä»¶æè¿°ï¼Œä¸éœ€è¦åŸæ–‡ä¾æ®

# è¾“å‡ºæ ¼å¼ï¼š
# [
#   "äº‹ä»¶æè¿°1",
#   "äº‹ä»¶æè¿°2", 
#   "äº‹ä»¶æè¿°3"
# ]

# æ•…äº‹æ–‡æœ¬ï¼š
# {full_plot}
# """
    
#     response1 = generate_response([{"role": "user", "content": step1_prompt}], model=model)
    
#     try:
#         from src.utils.utils import convert_json
#         events_only = convert_json(response1)
#         if not isinstance(events_only, list):
#             print(f"âš ï¸ ç¬¬ä¸€æ­¥äº‹ä»¶æå–æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›listï¼Œå¾—åˆ°{type(events_only)}")
#             return []
#     except Exception as e:
#         print(f"âš ï¸ ç¬¬ä¸€æ­¥äº‹ä»¶æå–å¤±è´¥ï¼š{e}")
#         return []
    
#     print(f"  âœ… ç¬¬ä¸€æ­¥æå–åˆ° {len(events_only)} ä¸ªäº‹ä»¶")
#     print("  ç¬¬äºŒæ­¥ï¼šéªŒè¯åŸæ–‡ä¾æ®...")
    
#     # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªäº‹ä»¶æ‰¾åŸæ–‡ä¾æ®
#     step2_prompt = f"""
# ç»™å®šäº‹ä»¶åˆ—è¡¨å’ŒåŸæ–‡ï¼Œè¯·ä¸ºæ¯ä¸ªäº‹ä»¶æ‰¾åˆ°æœ€åŒ¹é…çš„åŸæ–‡å¥å­ä½œä¸ºä¾æ®ã€‚

# äº‹ä»¶åˆ—è¡¨ï¼š
# {json.dumps(events_only, ensure_ascii=False, indent=2)}

# åŸæ–‡ï¼š
# {full_plot}

# è¦æ±‚ï¼š
# 1. ä¸ºæ¯ä¸ªäº‹ä»¶æ‰¾åˆ°åŸæ–‡ä¸­æœ€ç›¸å…³çš„å¥å­
# 2. å¦‚æœæ‰¾ä¸åˆ°æ”¯æ’‘å¥å­ï¼Œsourceå­—æ®µå¡«å†™"æ— æ³•æ‰¾åˆ°åŸæ–‡ä¾æ®"
# 3. ç¡®å®šè¯¥äº‹ä»¶æ¥è‡ªå“ªä¸ªç« èŠ‚

# è¾“å‡ºæ ¼å¼ï¼š
# [
#   {{
#     "event": "äº‹ä»¶æè¿°",
#     "source": "åŸæ–‡ä¸­çš„å…·ä½“å¥å­æˆ–'æ— æ³•æ‰¾åˆ°åŸæ–‡ä¾æ®'",
#     "chapter": "Chapter X"
#   }}
# ]
# """
    
#     response2 = generate_response([{"role": "user", "content": step2_prompt}], model=model)
    
#     try:
#         events_with_source = convert_json(response2)
#         if not isinstance(events_with_source, list):
#             print(f"âš ï¸ ç¬¬äºŒæ­¥åŒ¹é…æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›listï¼Œå¾—åˆ°{type(events_with_source)}")
#             return []
            
#         # è¿‡æ»¤æ‰æ— æ³•æ‰¾åˆ°åŸæ–‡ä¾æ®çš„äº‹ä»¶
#         valid_events = []
#         filtered_count = 0
        
#         for event in events_with_source:
#             if event.get("source") != "æ— æ³•æ‰¾åˆ°åŸæ–‡ä¾æ®":
#                 valid_events.append(event)
#             else:
#                 filtered_count += 1
#                 print(f"  âš ï¸ è¿‡æ»¤äº‹ä»¶ï¼ˆæ— åŸæ–‡ä¾æ®ï¼‰ï¼š{event.get('event', '')}")
        
#         print(f"  âœ… ç¬¬äºŒæ­¥éªŒè¯å®Œæˆï¼Œè¿‡æ»¤äº† {filtered_count} ä¸ªæ— ä¾æ®äº‹ä»¶")
#         return valid_events
        
#     except Exception as e:
#         print(f"âš ï¸ ç¬¬äºŒæ­¥éªŒè¯å¤±è´¥ï¼š{e}")
#         return []


def extract_events_no_hallucination(story_data, model="gpt-4.1", temperature=None):
    """
    æ— å¹»è§‰ç‰ˆæœ¬ï¼šå…ˆåˆ†å¥ç¼–å·ï¼Œå†è®©LLMé€‰æ‹©
    temperature: Noneä½¿ç”¨é»˜è®¤å€¼ï¼Œ0è¡¨ç¤ºå›ºå®šæ¨¡å¼ï¼Œ>0è¡¨ç¤ºéšæœºæ¨¡å¼
    """
    from src.utils.utils import split_plot_into_sentences
    
    # ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†åŸæ–‡ï¼Œç»™æ¯ä¸ªå¥å­ç¼–å·
    all_sentences = []
    sentence_map = {}
    sentence_counter = 0
    
    for ch in story_data:
        chapter_id = ch.get('chapter_id', '')
        plot = ch.get('plot', '')
        sentences = split_plot_into_sentences(plot)
        
        for sent in sentences:
            sentence_map[sentence_counter] = {
                "sentence": sent,
                "chapter": chapter_id
            }
            all_sentences.append(f"{sentence_counter}: {sent}")
            sentence_counter += 1

    concat_story_plot = ""
    for i in story_data:
        concat_story_plot += i.get('chapter_id', '') + "\n" + i.get('plot', '') + "\n\n"
        
    # å‡†å¤‡ç¼–å·å¥å­åˆ—è¡¨
    numbered_sentences = "\n".join(all_sentences)
    double_newline = "\n\n"

    print("  ç¬¬ä¸€æ­¥ï¼šæå–äº‹ä»¶...")
    # ç¬¬äºŒæ­¥ï¼šæå–äº‹ä»¶
    step1_prompt = f"""
è¯·ä»ä»¥ä¸‹æ•…äº‹ä¸­æå–æ‰€æœ‰å…³é”®äº‹ä»¶ã€‚

è¦æ±‚ï¼š
1. æ¯ä¸ªäº‹ä»¶ç”¨10-20å­—ç®€æ´æè¿°
2. æŒ‰æ—¶é—´é¡ºåºæ’åˆ—

é‡è¦é™åˆ¶ï¼š
1. åªæå–æ¯ä¸ªç« èŠ‚å†…æ˜ç¡®æè¿°å‘ç”Ÿçš„äº‹ä»¶, ä¸è¦æ·»åŠ ä»»ä½•å‡è®¾æˆ–æœªæ˜ç¡®æè¿°çš„äº‹ä»¶, ä¸è¦æ¨æ–­ç« èŠ‚ä¹‹é—´å‘ç”Ÿäº†ä»€ä¹ˆ
2. å‡†ç¡®æè¿°åŠ¨ä½œçš„æ€§è´¨, ä¿æŒåŠ¨ä½œæè¿°çš„å‡†ç¡®æ€§ï¼Œä¸è¦å¤¸å¤§æˆ–æ”¹å˜å…¶æ€§è´¨,æ¯”å¦‚ä¸è¦æŠŠ"å‡†å¤‡åšX"ç†è§£ä¸º"å·²ç»å®ŒæˆX"ä¹Ÿä¸è¦æŠŠ"å¨èƒè¦åšY"ç†è§£ä¸º"å·²ç»åšäº†Y"
3. æå–äº‹ä»¶çš„åŒæ—¶éœ€è¦æ ‡æ³¨äº‹ä»¶æ¥æº

è¾“å‡ºæ ¼å¼ï¼š
[
  {{"description":"äº‹ä»¶æè¿°1","reference":"chapter 1:æ–‡æœ¬åŸæ–‡"}},
  {{"description":"äº‹ä»¶æè¿°2","reference":"chapter 2:æ–‡æœ¬åŸæ–‡"}},
  ...
]
"""
    
    # æ ¹æ®temperatureå‚æ•°è°ƒç”¨
    if temperature is not None:
        response1 = generate_response([{"role": "user", "content": step1_prompt}], model=model, temperature=temperature)
        response1 = generate_response([
            {"role": "user", "content":step1_prompt},
        {"role": "user", "content": f"extract the key events base on the following story plot:\n\n{concat_story_plot}"}
        ], model=model, temperature=temperature)

    else:
        response1 = generate_response([{"role": "user", "content": step1_prompt}], model=model)
        response1 = generate_response([
            {"role": "user", "content":step1_prompt},
        {"role": "user", "content": f"extract the key events base on the following story plot:\n\n{concat_story_plot}"}
        ], model=model)
    try:
        from src.utils.utils import convert_json
        events_only = convert_json(response1)
        if not isinstance(events_only, list):
            print(f"âš ï¸ äº‹ä»¶æå–å¤±è´¥")
            return []
    except Exception as e:
        print(f"âš ï¸ äº‹ä»¶æå–å¤±è´¥ï¼š{e}")
        return []
    
    print(f"  âœ… æå–åˆ° {len(events_only)} ä¸ªäº‹ä»¶")
    print("  ç¬¬äºŒæ­¥ï¼šåŒ¹é…åŸæ–‡å¥å­...")
    
    # ç¬¬ä¸‰æ­¥ï¼šè®©LLMä¸ºæ¯ä¸ªäº‹ä»¶é€‰æ‹©å¥å­ç¼–å·
    step2_prompt = f"""
ç»™å®šäº‹ä»¶åˆ—è¡¨å’Œç¼–å·çš„åŸæ–‡å¥å­ï¼Œè¯·ä¸ºæ¯ä¸ªäº‹ä»¶é€‰æ‹©æœ€åŒ¹é…çš„å¥å­ç¼–å·ã€‚

äº‹ä»¶åˆ—è¡¨ï¼š
{json.dumps(events_only, ensure_ascii=False, indent=2)}

ç¼–å·çš„åŸæ–‡å¥å­ï¼š
{numbered_sentences}

è¦æ±‚ï¼š
1. ä¸ºæ¯ä¸ªäº‹ä»¶é€‰æ‹©ä¸€ä¸ªæœ€åŒ¹é…çš„å¥å­ç¼–å·
2. å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…çš„å¥å­ï¼Œç¼–å·å¡«å†™-1
3. åªèƒ½é€‰æ‹©å·²ç»™å‡ºçš„ç¼–å·ï¼Œä¸èƒ½å¡«å†™å…¶ä»–æ•°å­—

è¾“å‡ºæ ¼å¼ï¼š
[
  {{
    "event": "äº‹ä»¶æè¿°",
    "sentence_number": ç¼–å·æ•°å­—,
    "confidence": "high/medium/low"
  }}
]
"""
    
    # æ ¹æ®temperatureå‚æ•°è°ƒç”¨
    if temperature is not None:
        response2 = generate_response([{"role": "user", "content": step2_prompt}], model=model, temperature=temperature)
    else:
        response2 = generate_response([{"role": "user", "content": step2_prompt}], model=model)
    
    try:
        matches = convert_json(response2)
        if not isinstance(matches, list):
            print(f"âš ï¸ åŒ¹é…å¤±è´¥")
            return []
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        final_events = []
        filtered_count = 0
        
        for match in matches:
            sentence_num = match.get("sentence_number", -1)
            
            if sentence_num == -1 or sentence_num not in sentence_map:
                filtered_count += 1
                print(f"  âš ï¸ è¿‡æ»¤äº‹ä»¶ï¼ˆæ— åŒ¹é…å¥å­ï¼‰ï¼š{match.get('event', '')}")
                continue
            
            sentence_info = sentence_map[sentence_num]
            final_events.append({
                "event": match.get("event", ""),
                "source": sentence_info["sentence"],
                "chapter": sentence_info["chapter"],
                "confidence": match.get("confidence", "unknown")
            })
        
        print(f"  âœ… åŒ¹é…å®Œæˆï¼Œè¿‡æ»¤äº† {filtered_count} ä¸ªæ— åŒ¹é…äº‹ä»¶")

        validated_events = validate_events_against_source(final_events, model=model, temperature=temperature)
        return validated_events

        
    except Exception as e:
        print(f"âš ï¸ åŒ¹é…å¤±è´¥ï¼š{e}")
        return []

def validate_events_against_source(events, model="gpt-4.1", temperature=None):
    """
    ç¬¬ä¸‰æ­¥ï¼šéªŒè¯äº‹ä»¶ä¸åŸæ–‡çš„ä¸€è‡´æ€§ï¼Œæ¶ˆé™¤å¹»è§‰
    """
    if not events:
        return []
    
    print("  ç¬¬ä¸‰æ­¥ï¼šéªŒè¯äº‹ä»¶å‡†ç¡®æ€§...")
    
    # æ ¼å¼åŒ–äº‹ä»¶å’ŒåŸæ–‡å¯¹ç…§
    event_source_pairs = []
    for i, event in enumerate(events):
        event_source_pairs.append(f"""
äº‹ä»¶{i+1}: {event['event']}
åŸæ–‡: {event['source']}
ç« èŠ‚: {event['chapter']}
---""")
    
    formatted_pairs = "\n".join(event_source_pairs)
    
    validation_prompt = f"""
ä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„äº‹å®æ£€æŸ¥å‘˜ï¼Œéœ€è¦éªŒè¯äº‹ä»¶æè¿°ä¸åŸæ–‡æ˜¯å¦å®Œå…¨åŒ¹é…ã€‚

ç‰¹åˆ«è¦å…³æ³¨ä»¥ä¸‹é«˜å±å¹»è§‰è¡Œä¸ºï¼š
1. åŸæ–‡è¯´â€œå‡†å¤‡/æ‰“ç®—â€ï¼Œä¸èƒ½è¯´æˆâ€œå·²å®Œæˆâ€ï¼›
2. åŸæ–‡è¯´â€œå¨èƒ/è¯•å›¾â€ï¼Œä¸èƒ½è¯´æˆâ€œå·²ç»åšåˆ°â€ï¼›
3. åŸæ–‡ä¸­æ²¡æœ‰æ˜ç¡®æè¿°â€œäº¤ä»˜/æŠµè¾¾/å®Œæˆâ€ï¼Œä¸èƒ½å‡è®¾å·²ç»å®Œæˆï¼›
4. ä¸€åˆ‡æœªç»è¯å®çš„â€œè·å¾—/æ‹¿åˆ°/å®æ–½æˆåŠŸâ€è¡Œä¸ºéƒ½è¦è´¨ç–‘ã€‚

è¯·é€å¥å¯¹æ¯”æ¯ä¸ªäº‹ä»¶ä¸åŸå§‹ç« èŠ‚å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨â€œå®ŒæˆçŠ¶æ€å¹»è§‰â€ã€‚å¦‚æœå‘ç°éœ€è¦ä¿®æ”¹æˆæ­£ç¡®çš„äº‹ä»¶æè¿°.

{formatted_pairs}

è¾“å‡ºæ ¼å¼ï¼š
[
  {{
    "original_event": "åŸå§‹äº‹ä»¶æè¿°",
    "corrected_event": "ä¿®æ­£åçš„äº‹ä»¶æè¿°ï¼ˆå¦‚æ— é—®é¢˜åˆ™ä¸åŸå§‹ç›¸åŒï¼‰",
    "has_issue": true/false,
    "issue_type": "é—®é¢˜ç±»å‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰"
  }}
]

é‡è¦ï¼šåªè¾“å‡ºçº¯JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•æ³¨é‡Šã€‚
"""
    
    # æ ¹æ®temperatureå‚æ•°è°ƒç”¨
    if temperature is not None:
        response = generate_response([{"role": "user", "content": validation_prompt}], model=model, temperature=temperature)
    else:
        response = generate_response([{"role": "user", "content": validation_prompt}], model=model)
    
    try:
        from src.utils.utils import convert_json
        validation_results = convert_json(response)
        if not isinstance(validation_results, list):
            print("âš ï¸ éªŒè¯æ­¥éª¤æ ¼å¼é”™è¯¯ï¼Œä¿æŒåŸäº‹ä»¶")
            return events
        
        # åº”ç”¨éªŒè¯ç»“æœ
        validated_events = []
        corrected_count = 0
        
        for i, (original_event, validation) in enumerate(zip(events, validation_results)):
            if validation.get("has_issue", False):
                corrected_count += 1
                print(f"  ğŸ”§ ä¿®æ­£äº‹ä»¶{i+1}: {validation.get('original_event', '')} â†’ {validation.get('corrected_event', '')}")
                # ä½¿ç”¨ä¿®æ­£åçš„äº‹ä»¶æè¿°
                corrected_event = original_event.copy()
                corrected_event["event"] = validation.get("corrected_event", original_event["event"])
                validated_events.append(corrected_event)
            else:
                validated_events.append(original_event)
        
        print(f"  âœ… éªŒè¯å®Œæˆï¼Œä¿®æ­£äº† {corrected_count} ä¸ªäº‹ä»¶")
        return validated_events
        
    except Exception as e:
        print(f"âš ï¸ éªŒè¯æ­¥éª¤å¤±è´¥ï¼š{e}ï¼Œä¿æŒåŸäº‹ä»¶")
        return events

def extract_events_fixed_mode(story_data, model="gpt-4.1"):
    """
    å›ºå®šæ¨¡å¼ï¼šå®Œå…¨å¯é‡ç°çš„äº‹ä»¶æå–
    """
    print("  ğŸ”’ å›ºå®šæ¨¡å¼ï¼ˆtemperature=0ï¼‰")
    return extract_events_no_hallucination(story_data, model=model, temperature=0)


def extract_events_statistical_mode(story_data, model="gpt-4.1", runs=3):
    """
    ç»Ÿè®¡æ¨¡å¼ï¼šå¤šæ¬¡è¿è¡Œï¼Œå¸¦ç»Ÿè®¡åˆ†æ
    """
    print(f"  ğŸ“Š ç»Ÿè®¡æ¨¡å¼ï¼šè¿è¡Œ {runs} æ¬¡...")
    
    all_results = []
    all_event_counts = []
    
    for i in range(runs):
        print(f"    ğŸ”„ ç¬¬ {i+1}/{runs} æ¬¡è¿è¡Œ...")
        
        # æ¯æ¬¡ä½¿ç”¨éšæœºæ€§ï¼ˆä¸è®¾ç½®temperature=0ï¼‰
        events = extract_events_no_hallucination(story_data, model=model, temperature=0.3)
        
        if events:
            all_results.append(events)
            all_event_counts.append(len(events))
            print(f"      âœ… ç¬¬{i+1}æ¬¡ï¼š{len(events)}ä¸ªäº‹ä»¶")
        else:
            print(f"      âŒ ç¬¬{i+1}æ¬¡ï¼šæå–å¤±è´¥")
    
    if not all_results:
        print("âš ï¸ æ‰€æœ‰è¿è¡Œéƒ½å¤±è´¥äº†")
        return []
    
    # ç»Ÿè®¡åˆ†æ
    event_counts = np.array(all_event_counts)
    avg_count = np.mean(event_counts)
    std_count = np.std(event_counts)
    min_count = np.min(event_counts)
    max_count = np.max(event_counts)
    
    # è®¡ç®—ç½®ä¿¡åŒºé—´ï¼ˆ95%ï¼‰
    if len(event_counts) > 1:
        confidence_interval = 1.96 * std_count / np.sqrt(len(event_counts))
        ci_lower = avg_count - confidence_interval
        ci_upper = avg_count + confidence_interval
    else:
        ci_lower = ci_upper = avg_count
    
    # è®¡ç®—ç¨³å®šæ€§åˆ†æ•°
    stability_score = 1 - (std_count / avg_count) if avg_count > 0 else 0
    
    print(f"  ğŸ“ˆ ç»Ÿè®¡åˆ†æç»“æœï¼š")
    print(f"    - äº‹ä»¶æ•°é‡èŒƒå›´ï¼š{min_count}-{max_count}ä¸ª")
    print(f"    - å¹³å‡äº‹ä»¶æ•°é‡ï¼š{avg_count:.1f} Â± {std_count:.1f}")
    print(f"    - 95%ç½®ä¿¡åŒºé—´ï¼š[{ci_lower:.1f}, {ci_upper:.1f}]")
    print(f"    - ç¨³å®šæ€§è¯„åˆ†ï¼š{stability_score:.3f} (0-1ï¼Œè¶Šé«˜è¶Šç¨³å®š)")
    
    # é€‰æ‹©æœ€æ¥è¿‘å¹³å‡å€¼çš„ç»“æœä½œä¸ºä»£è¡¨
    best_idx = np.argmin(np.abs(event_counts - avg_count))
    representative_result = all_results[best_idx]
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯åˆ°æ¯ä¸ªäº‹ä»¶
    for event in representative_result:
        event["statistical_info"] = {
            "run_index": int(best_idx + 1),
            "total_runs": runs,
            "avg_event_count": round(float(avg_count), 1),
            "std_event_count": round(float(std_count), 1),
            "stability_score": round(float(stability_score), 3),
            "confidence_interval_95": [round(float(ci_lower), 1), round(float(ci_upper), 1)],
            "event_count_range": [int(min_count), int(max_count)]
        }
    
    # æ·»åŠ æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
    statistical_summary = {
        "runs_completed": len(all_results),
        "runs_failed": runs - len(all_results),
        "success_rate": len(all_results) / runs,
        "event_counts_by_run": all_event_counts,
        "statistical_metrics": {
            "mean": round(float(avg_count), 1),
            "std": round(float(std_count), 1),
            "min": int(min_count),
            "max": int(max_count),
            "stability_score": round(float(stability_score), 3),
            "confidence_interval_95": [round(float(ci_lower), 1), round(float(ci_upper), 1)]
        }
    }
    
    print(f"  âœ… é€‰æ‹©ç¬¬{best_idx+1}æ¬¡ç»“æœä½œä¸ºä»£è¡¨ï¼ˆæœ€æ¥è¿‘å¹³å‡å€¼ï¼‰")
    
    return representative_result, statistical_summary


def analyze_papalampidi_structure(events, model="gpt-4.1", temperature=None):
    """
    ä½¿ç”¨Papalampidiæ¡†æ¶åˆ†ææ•…äº‹ç»“æ„
    """
    # åªä¼ é€’äº‹ä»¶æè¿°ç»™åˆ†æ
    event_descriptions = [event.get("event", "") if isinstance(event, dict) else str(event) for event in events]
    events_str = json.dumps(event_descriptions, ensure_ascii=False, indent=2)
    
    prompt = f"""
ç»™å®šäº‹ä»¶åˆ—è¡¨ï¼Œè¯·æ ‡æ³¨5ä¸ªå…³é”®è½¬æŠ˜ç‚¹(TP)å’Œ6ä¸ªé˜¶æ®µï¼š

äº‹ä»¶åˆ—è¡¨ï¼š{events_str}

è½¬æŠ˜ç‚¹å®šä¹‰ï¼š
- TP1 (Opportunity): å¼•å…¥ä¸»è¦æƒ…èŠ‚çš„å…³é”®äº‹ä»¶
- TP2 (Change of Plans): ç›®æ ‡/è®¡åˆ’å‘ç”Ÿæ”¹å˜çš„æ—¶åˆ»  
- TP3 (Point of No Return): è§’è‰²å…¨åŠ›æŠ•å…¥ã€æ— æ³•åé€€çš„æ‰¿è¯ºç‚¹
- TP4 (Major Setback): æœ€å¤§å±æœº/æŒ«æŠ˜æ—¶åˆ»
- TP5 (Climax): ä¸»è¦å†²çªçš„æœ€ç»ˆè§£å†³æ—¶åˆ»

é˜¶æ®µå®šä¹‰ï¼š
- Setup: èƒŒæ™¯è®¾å®šå’Œè§’è‰²ä»‹ç»
- New Situation: æ–°ç¯å¢ƒ/æ–°æŒ‘æˆ˜çš„å»ºç«‹
- Progress: æœç›®æ ‡å‰è¿›çš„è¿‡ç¨‹
- Complications: å†²çªå’Œå›°éš¾çš„å‡çº§
- Final Push: æœ€ç»ˆçš„å†³å®šæ€§è¡ŒåŠ¨
- Aftermath: ç»“æœå’Œé•¿æœŸå½±å“

è¾“å‡ºæ ¼å¼ï¼š
{{
  "è½¬æŠ˜ç‚¹": {{
    "TP1": "å…·ä½“äº‹ä»¶æè¿°",
    "TP2": "å…·ä½“äº‹ä»¶æè¿°", 
    "TP3": "å…·ä½“äº‹ä»¶æè¿°",
    "TP4": "å…·ä½“äº‹ä»¶æè¿°",
    "TP5": "å…·ä½“äº‹ä»¶æè¿°"
  }},
  "é˜¶æ®µåˆ’åˆ†": {{
    "Setup": ["äº‹ä»¶1", "äº‹ä»¶2"],
    "New Situation": ["äº‹ä»¶3"],
    "Progress": ["äº‹ä»¶4", "äº‹ä»¶5"],
    "Complications": ["äº‹ä»¶6", "äº‹ä»¶7", "äº‹ä»¶8"],
    "Final Push": ["äº‹ä»¶9"],
    "Aftermath": ["äº‹ä»¶10"]
  }}
}}

é‡è¦ï¼šåªè¾“å‡ºçº¯JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•æ³¨é‡Šæˆ–è¯´æ˜æ–‡å­—ã€‚
"""
    
    # æ ¹æ®temperatureå‚æ•°è°ƒç”¨
    if temperature is not None:
        response = generate_response([{"role": "user", "content": prompt}], model=model, temperature=temperature)
    else:
        response = generate_response([{"role": "user", "content": prompt}], model=model)
    
    try:
        from src.utils.utils import convert_json
        result = convert_json(response)
        if not isinstance(result, dict):
            print(f"âš ï¸ Papalampidiåˆ†ææ ¼å¼é”™è¯¯")
            return {"è½¬æŠ˜ç‚¹": {}, "é˜¶æ®µåˆ’åˆ†": {}}
        return result
    except Exception as e:
        print(f"âš ï¸ Papalampidiåˆ†æå¤±è´¥ï¼š{e}")
        return {"è½¬æŠ˜ç‚¹": {}, "é˜¶æ®µåˆ’åˆ†": {}}


def analyze_li_functions(events, model="gpt-4.1", temperature=None):
    """
    ä½¿ç”¨Liæ¡†æ¶åˆ†ææ•…äº‹åŠŸèƒ½
    """
    # åªä¼ é€’äº‹ä»¶æè¿°ç»™åˆ†æ
    event_descriptions = [event.get("event", "") if isinstance(event, dict) else str(event) for event in events]
    events_str = json.dumps(event_descriptions, ensure_ascii=False, indent=2)
    
    prompt = f"""
ç»™å®šäº‹ä»¶åˆ—è¡¨ï¼Œè¯·ç”¨10ä¸ªåŠŸèƒ½æ ‡ç­¾æ ‡æ³¨æ¯ä¸ªäº‹ä»¶ï¼š

äº‹ä»¶åˆ—è¡¨ï¼š{events_str}

åŠŸèƒ½æ ‡ç­¾å®šä¹‰ï¼š
- Abstract: æ•…äº‹è¦ç‚¹çš„æ€»ç»“
- Orientation: èƒŒæ™¯è®¾å®šï¼ˆæ—¶é—´ã€åœ°ç‚¹ã€äººç‰©ï¼‰
- Complicating Action: å¢åŠ å¼ åŠ›ã€æ¨åŠ¨æƒ…èŠ‚çš„äº‹ä»¶
- MRE : æœ€é‡è¦/æœ€å€¼å¾—æŠ¥å‘Šçš„äº‹ä»¶ (Most Reportable Event)
- Minor Resolution: éƒ¨åˆ†ç¼“è§£å¼ åŠ›çš„äº‹ä»¶
- Return of MRE: MREä¸»é¢˜çš„é‡æ–°å‡ºç°
- Resolution: è§£å†³ä¸»è¦å†²çªçš„äº‹ä»¶  
- Aftermath: ä¸»è¦äº‹ä»¶åçš„é•¿æœŸå½±å“
- Evaluation: å™è¿°è€…å¯¹æ•…äº‹æ„ä¹‰çš„è¯„è®º
- Direct Comment: å¯¹è§‚ä¼—çš„ç›´æ¥è¯„è®º

è¾“å‡ºæ ¼å¼ï¼š
{{
  "äº‹ä»¶1": "æ ‡ç­¾å",
  "äº‹ä»¶2": "æ ‡ç­¾å",
  "äº‹ä»¶3": "æ ‡ç­¾å",
  ...
}}

é‡è¦ï¼šåªè¾“å‡ºçº¯JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•æ³¨é‡Šæˆ–è¯´æ˜æ–‡å­—ã€‚
"""
    
    # æ ¹æ®temperatureå‚æ•°è°ƒç”¨
    if temperature is not None:
        response = generate_response([{"role": "user", "content": prompt}], model=model, temperature=temperature)
    else:
        response = generate_response([{"role": "user", "content": prompt}], model=model)
    
    try:
        from src.utils.utils import convert_json
        result = convert_json(response)
        if not isinstance(result, dict):
            print(f"âš ï¸ Liåˆ†ææ ¼å¼é”™è¯¯")
            return {}
        return result
    except Exception as e:
        print(f"âš ï¸ Liåˆ†æå¤±è´¥ï¼š{e}")
        return {}


def analyze_story_structure(events, papalampidi_result, li_result, mode="default", statistical_summary=None):
    """
    å®¢è§‚åˆ†ææ•…äº‹ç»“æ„
    """
    analysis = {
        "åŸºæœ¬ä¿¡æ¯": {
            "äº‹ä»¶æ€»æ•°": len(events),
            "åˆ†ææ¨¡å¼": mode,
            "åˆ†ææ—¶é—´": None
        },
        "Papalampidiç»“æ„åˆ†æ": {
            "è½¬æŠ˜ç‚¹å®Œæ•´æ€§": {},
            "é˜¶æ®µå®Œæ•´æ€§": {}
        },
        "LiåŠŸèƒ½åˆ†æ": {
            "æ ¸å¿ƒåŠŸèƒ½æ£€æŸ¥": {},
            "åŠŸèƒ½åˆ†å¸ƒ": {},
            "åŠŸèƒ½å¤šæ ·æ€§": 0
        }
    }
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæ˜¯ç»Ÿè®¡æ¨¡å¼ï¼‰
    if statistical_summary:
        analysis["ç»Ÿè®¡åˆ†ææ‘˜è¦"] = statistical_summary
    
    # åˆ†æPapalampidiç»“æ„
    required_tps = ["TP1", "TP2", "TP3", "TP4", "TP5"]
    found_tps = [tp for tp in required_tps if papalampidi_result.get("è½¬æŠ˜ç‚¹", {}).get(tp)]
    missing_tps = [tp for tp in required_tps if tp not in found_tps]
    
    analysis["Papalampidiç»“æ„åˆ†æ"]["è½¬æŠ˜ç‚¹å®Œæ•´æ€§"] = {
        "è¯†åˆ«åˆ°çš„TP": found_tps,
        "ç¼ºå¤±çš„TP": missing_tps,
        "TPè¦†ç›–ç‡": f"{len(found_tps)}/5"
    }
    
    required_stages = ["Setup", "New Situation", "Progress", "Complications", "Final Push", "Aftermath"]
    found_stages = [stage for stage in required_stages if papalampidi_result.get("é˜¶æ®µåˆ’åˆ†", {}).get(stage)]
    missing_stages = [stage for stage in required_stages if stage not in found_stages]
    
    analysis["Papalampidiç»“æ„åˆ†æ"]["é˜¶æ®µå®Œæ•´æ€§"] = {
        "è¯†åˆ«åˆ°çš„é˜¶æ®µ": found_stages,
        "ç¼ºå¤±çš„é˜¶æ®µ": missing_stages,
        "é˜¶æ®µè¦†ç›–ç‡": f"{len(found_stages)}/6"
    }
    
    # åˆ†æLiåŠŸèƒ½
    core_functions = ["Orientation", "Complicating Action", "MRE", "Resolution"]
    for func in core_functions:
        exists = any(func in value for value in li_result.values())
        analysis["LiåŠŸèƒ½åˆ†æ"]["æ ¸å¿ƒåŠŸèƒ½æ£€æŸ¥"][func] = "å­˜åœ¨" if exists else "ç¼ºå¤±"

    # åŠŸèƒ½åˆ†å¸ƒç»Ÿè®¡
    func_counts = Counter(li_result.values())
    analysis["LiåŠŸèƒ½åˆ†æ"]["åŠŸèƒ½åˆ†å¸ƒ"] = dict(func_counts)
    analysis["LiåŠŸèƒ½åˆ†æ"]["åŠŸèƒ½å¤šæ ·æ€§"] = len(set(li_result.values()))
    
    return analysis


def run_story_evaluation(version, mode="default", runs=3, story_file="story_updated.json", model="gpt-4.1"):
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„æ•…äº‹è¯„ä»·
    mode: "default", "fixed", "statistical"
    """
    from src.constant import output_dir
    
    print(f"\nğŸ” å¼€å§‹æ•…äº‹ç»“æ„è¯„ä»·ï¼š{version}")
    
    # è¯»å–æ•…äº‹æ•°æ®
    story_path = os.path.join(output_dir, version, story_file)
    if not os.path.exists(story_path):
        print(f"âš ï¸ æ•…äº‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{story_path}")
        return None
    
    story_data = load_json(story_path)
    
    # Step 1: æ ¹æ®æ¨¡å¼æå–å…³é”®äº‹ä»¶
    statistical_summary = None
    
    if mode == "fixed":
        print("ğŸ“‹ æ­¥éª¤1ï¼šå›ºå®šæ¨¡å¼æå–å…³é”®äº‹ä»¶...")
        events = extract_events_fixed_mode(story_data, model=model)
        temp_for_analysis = 0  # åˆ†æé˜¶æ®µä¹Ÿä½¿ç”¨å›ºå®šæ¨¡å¼

    elif mode == "statistical":
        print(f"ğŸ“‹ æ­¥éª¤1ï¼šç»Ÿè®¡æ¨¡å¼æå–å…³é”®äº‹ä»¶ï¼ˆ{runs}æ¬¡è¿è¡Œï¼‰...")
        result = extract_events_statistical_mode(story_data, model=model, runs=runs)
        if isinstance(result, tuple):
            events, statistical_summary = result
        else:
            events = result
        temp_for_analysis = 0  # åˆ†æé˜¶æ®µä½¿ç”¨å›ºå®šæ¨¡å¼ï¼Œç¡®ä¿åŒæ ·äº‹ä»¶å¾—åˆ°ä¸€è‡´åˆ†æ

    # elif mode == "statistical":
    #     print(f"ğŸ“‹ æ­¥éª¤1ï¼šç»Ÿè®¡æ¨¡å¼æå–å…³é”®äº‹ä»¶ï¼ˆ{runs}æ¬¡è¿è¡Œï¼‰...")
    #     result = extract_events_statistical_mode(story_data, model=model, runs=runs)
    #     if isinstance(result, tuple):
    #         events, statistical_summary = result
    #     else:
    #         events = result
    #     temp_for_analysis = 0  
    else:
        print("ğŸ“‹ æ­¥éª¤1ï¼šé»˜è®¤æ¨¡å¼æå–å…³é”®äº‹ä»¶...")
        events = extract_events_no_hallucination(story_data, model=model)
        temp_for_analysis = None  # ä½¿ç”¨é»˜è®¤temperature
    
    print(f"âœ… æœ€ç»ˆæå–åˆ° {len(events)} ä¸ªæœ‰æ•ˆäº‹ä»¶")
    
    if len(events) == 0:
        print("âš ï¸ æ²¡æœ‰æå–åˆ°æœ‰æ•ˆäº‹ä»¶ï¼Œç»ˆæ­¢åˆ†æ")
        return None
    
    # Step 2: Papalampidiç»“æ„åˆ†æ
    print("ğŸ“Š æ­¥éª¤2ï¼šPapalampidiç»“æ„åˆ†æ...")
    papalampidi_result = analyze_papalampidi_structure(events, model=model, temperature=temp_for_analysis)
    
    # Step 3: LiåŠŸèƒ½åˆ†æ
    print("ğŸ·ï¸ æ­¥éª¤3ï¼šLiåŠŸèƒ½åˆ†æ...")
    li_result = analyze_li_functions(events, model=model, temperature=temp_for_analysis)
    
    # Step 4: ç»¼åˆåˆ†æ
    print("ğŸ“ˆ æ­¥éª¤4ï¼šç»¼åˆåˆ†æ...")
    structure_analysis = analyze_story_structure(events, papalampidi_result, li_result, mode, statistical_summary)
    
    # æ±‡æ€»ç»“æœ
    evaluation_result = {
        "è¯„ä»·æ¨¡å¼": f"{mode}æ¨¡å¼" + (f"ï¼ˆ{runs}æ¬¡è¿è¡Œï¼‰" if mode == "statistical" else ""),
        "äº‹ä»¶åˆ—è¡¨": events,
        "Papalampidiè¯¦ç»†ç»“æœ": papalampidi_result,
        "Liè¯¦ç»†ç»“æœ": li_result,
        "ç»“æ„åˆ†æ": structure_analysis
    }
    
    # ä¿å­˜ç»“æœ
    output_filename = f"story_structure_analysis_{mode}.json"
    if mode == "statistical":
        output_filename = f"story_structure_analysis_{mode}_{runs}runs.json"
    save_json(evaluation_result, version, output_filename)
    
    # æ‰“å°æ‘˜è¦
    print_evaluation_summary(structure_analysis, mode, statistical_summary)
    
    return evaluation_result


def print_evaluation_summary(structure_analysis, mode, statistical_summary=None):
    """æ‰“å°è¯„ä»·æ‘˜è¦"""
    tp_coverage = structure_analysis["Papalampidiç»“æ„åˆ†æ"]["è½¬æŠ˜ç‚¹å®Œæ•´æ€§"]["TPè¦†ç›–ç‡"]
    stage_coverage = structure_analysis["Papalampidiç»“æ„åˆ†æ"]["é˜¶æ®µå®Œæ•´æ€§"]["é˜¶æ®µè¦†ç›–ç‡"]
    core_missing = [k for k, v in structure_analysis["LiåŠŸèƒ½åˆ†æ"]["æ ¸å¿ƒåŠŸèƒ½æ£€æŸ¥"].items() if v == "ç¼ºå¤±"]
    function_diversity = structure_analysis["LiåŠŸèƒ½åˆ†æ"]["åŠŸèƒ½å¤šæ ·æ€§"]
    
    mode_display = f"{mode}æ¨¡å¼"
    
    print(f"""
ğŸ“Š {mode_display}è¯„ä»·å®Œæˆï¼ç»“æœæ‘˜è¦ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ˆ Papalampidiç»“æ„:
   - è½¬æŠ˜ç‚¹è¦†ç›–: {tp_coverage}
   - é˜¶æ®µè¦†ç›–: {stage_coverage}
ğŸ·ï¸ LiåŠŸèƒ½åˆ†æ:
   - ç¼ºå¤±æ ¸å¿ƒåŠŸèƒ½: {core_missing if core_missing else 'æ— '}
   - åŠŸèƒ½å¤šæ ·æ€§: {function_diversity}ç§""")
    
    # å¦‚æœæ˜¯ç»Ÿè®¡æ¨¡å¼ï¼Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if statistical_summary and mode == "statistical":
        metrics = statistical_summary["statistical_metrics"]
        print(f"""ğŸ“Š ç»Ÿè®¡åˆ†ææ‘˜è¦:
   - æˆåŠŸç‡: {statistical_summary['success_rate']:.1%}
   - äº‹ä»¶æ•°é‡: {metrics['mean']} Â± {metrics['std']} (èŒƒå›´: {metrics['min']}-{metrics['max']})
   - 95%ç½®ä¿¡åŒºé—´: [{metrics['confidence_interval_95'][0]}, {metrics['confidence_interval_95'][1]}]
   - ç¨³å®šæ€§è¯„åˆ†: {metrics['stability_score']:.3f}/1.000""")
    
    print(f"""â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜
""")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆæ•…äº‹ç»“æ„è¯„ä»·å·¥å…·")
    parser.add_argument("--version", type=str, required=True, help="ç‰ˆæœ¬æ–‡ä»¶å¤¹å")
    parser.add_argument("--mode", type=str, choices=["default", "fixed", "statistical"], default="default", 
                       help="è¯„ä»·æ¨¡å¼ï¼šdefaultï¼ˆé»˜è®¤ï¼‰ã€fixedï¼ˆå›ºå®šå¯é‡ç°ï¼‰æˆ– statisticalï¼ˆç»Ÿè®¡åˆ†æï¼‰")
    parser.add_argument("--runs", type=int, default=3, help="ç»Ÿè®¡æ¨¡å¼çš„è¿è¡Œæ¬¡æ•°")
    parser.add_argument("--story-file", type=str, default="story_updated.json", help="æ•…äº‹æ–‡ä»¶å")
    parser.add_argument("--model", type=str, default="gpt-4.1", help="ä½¿ç”¨çš„LLMæ¨¡å‹")
    
    args = parser.parse_args()
    run_story_evaluation(args.version, args.mode, args.runs, args.story_file, args.model)