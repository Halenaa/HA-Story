"""
ä¸»æ¬¡æ³•æƒ…æ„Ÿå¼§çº¿åˆ†æå™¨ (æ›´æ–°ç‰ˆ)
ä¸»è¦æ–¹æ³•ï¼šRoBERTaï¼ˆç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰
éªŒè¯æ–¹æ³•ï¼šLabMTï¼ˆç»å…¸æ–¹æ³•ï¼Œä¸ReaganåŸæ–‡ä¸€è‡´ï¼‰
"""

import re
import numpy as np
import matplotlib.pyplot as plt
import json
import os
import requests
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
from scipy.stats import pearsonr, spearmanr

class DualMethodEmotionalAnalyzer:
    """
    ä¸»æ¬¡æ³•æƒ…æ„Ÿåˆ†æå™¨ (æ›´æ–°ç‰ˆ)
    ä¸»è¦ï¼šRoBERTaåˆ†æ
    éªŒè¯ï¼šLabMTåˆ†æ
    """
    
    def __init__(self, labmt_version='v1'):
        """
        åˆå§‹åŒ–åŒæ–¹æ³•åˆ†æå™¨
        
        Args:
            labmt_version: 'v1' (2011, ä¸Reaganä¸€è‡´) æˆ– 'v2' (2020, æ›´ç°ä»£)
        """
        print("ğŸš€ åˆå§‹åŒ–ä¸»æ¬¡æ³•æƒ…æ„Ÿåˆ†æå™¨ï¼ˆRoBERTaç‰ˆï¼‰...")
        
        self.labmt_version = labmt_version
        print(f"ğŸ“‹ é€‰æ‹©LabMTç‰ˆæœ¬: {labmt_version} ({'ä¸ReaganåŸæ–‡ä¸€è‡´' if labmt_version == 'v1' else 'æ›´ç°ä»£çš„ç‰ˆæœ¬'})")
        
        # åˆå§‹åŒ–RoBERTaï¼ˆä¸»è¦æ–¹æ³•ï¼‰
        self._init_roberta()
        
        # åˆå§‹åŒ–LabMTï¼ˆéªŒè¯æ–¹æ³•ï¼‰
        self._init_labmt()
        
        print("âœ… åŒæ–¹æ³•åˆ†æå™¨å‡†å¤‡å°±ç»ª")
    
    def _init_roberta(self):
        """åˆå§‹åŒ–RoBERTaåˆ†æå™¨"""
        try:
            from transformers import pipeline
            
            print("ğŸ“¥ åŠ è½½RoBERTaæ¨¡å‹...")
            # ä½¿ç”¨ä½ æµ‹è¯•è¿‡çš„RoBERTaæ¨¡å‹
            self.roberta_classifier = pipeline(
                "sentiment-analysis", 
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                truncation=True,
                max_length=512
            )
            print("âœ… RoBERTaåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸï¼ˆä¸»è¦æ–¹æ³•ï¼‰")
            
        except ImportError:
            print("âŒ RoBERTaåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·å®‰è£…transformers")
            raise ImportError("è¯·è¿è¡Œ: pip install transformers torch")
        except Exception as e:
            print(f"âŒ RoBERTaæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å°è¯•å¤‡é€‰æ¨¡å‹
            try:
                print("ğŸ”„ å°è¯•å¤‡é€‰æ¨¡å‹...")
                self.roberta_classifier = pipeline(
                    "sentiment-analysis",
                    model="distilbert-base-uncased-finetuned-sst-2-english",
                    truncation=True,
                    max_length=512
                )
                print("âœ… å¤‡é€‰æ¨¡å‹DistilBERTåŠ è½½æˆåŠŸ")
            except:
                raise ImportError("æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
    
    def _init_labmt(self):
        """åˆå§‹åŒ–LabMTè¯å…¸"""
        # å°è¯•åŠ è½½æœ¬åœ°LabMTæ–‡ä»¶
        possible_paths = [
            '/Users/haha/Story/data/Hedonometer.csv',
            './labMT.csv',
            './data/labMT.csv',
            '../data/labMT.csv'
        ]
        
        labmt_found = False
        for path in possible_paths:
            if os.path.exists(path):
                print(f"ğŸ“ å‘ç°æœ¬åœ°LabMTè¯å…¸æ–‡ä»¶: {path}")
                self._load_labmt_local(path)
                labmt_found = True
                break
        
        if not labmt_found:
            print("ğŸ“¥ ä¸‹è½½LabMTè¯å…¸...")


    
    def _load_labmt_local(self, file_path='/Users/haha/Story/data/Hedonometer.csv'):
        """åŠ è½½æœ¬åœ°LabMTæ–‡ä»¶ - ä¿®å¤ç‰ˆ"""
        import pandas as pd
        
        try:
            print(f"ğŸ” è¯»å–LabMTæ–‡ä»¶: {file_path}")
            
            # ä½¿ç”¨é€—å·åˆ†éš”ç¬¦è¯»å–
            df = pd.read_csv(file_path, sep=',')
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
            
            # ä½¿ç”¨æ­£ç¡®çš„åˆ—å
            word_col = 'Word'
            happiness_col = 'Happiness Score'
            
            # æ„å»ºè¯å…¸
            self.labmt_dict = {}
            valid_count = 0
            
            for _, row in df.iterrows():
                try:
                    word = str(row[word_col]).lower().strip()
                    score = float(row[happiness_col])
                    
                    # è¿‡æ»¤ä¸­æ€§è¯
                    if len(word) > 1 and 1.0 <= score <= 9.0 and not (4.0 <= score <= 6.0):
                        self.labmt_dict[word] = score
                        valid_count += 1
                except:
                    continue
            
            print(f"âœ… LabMTè¯å…¸åŠ è½½æˆåŠŸï¼Œæœ‰æ•ˆè¯æ±‡: {valid_count}")
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            self._init_simple_labmt()
        

    def analyze_with_roberta_correct(self, text: str) -> float:
        """RoBERTaæƒ…æ„Ÿåˆ†æï¼ˆä¸»è¦æ–¹æ³•ï¼‰- ä¿®å¤ç‰ˆ"""
        if not text or not text.strip():
            return 0.0
        
        try:
            if len(text) <= 500:
                result = self.roberta_classifier(text)[0]
                return self.convert_roberta_score(result)
            else:
                # åˆ†å¥åˆ†æï¼ˆä¿®å¤ç‰ˆï¼‰
                sentences = [s.strip() for s in text.split('.') if s.strip() and len(s.strip()) > 10]
                scores = []
                
                for sentence in sentences[:15]:  # åˆ†æå‰15å¥
                    try:
                        result = self.roberta_classifier(sentence)[0]
                        score = self.convert_roberta_score(result)
                        scores.append(score)
                    except:
                        continue
                
                return sum(scores) / len(scores) if scores else 0.0
                
        except Exception as e:
            print(f"RoBERTaåˆ†æé”™è¯¯: {e}")
            return 0.0
    
    def convert_roberta_score(self, result):
        """æ­£ç¡®è½¬æ¢RoBERTaåˆ†æ•°"""
        label = result['label'].lower()
        score = result['score']
        
        if label == 'positive':
            return score  # 0åˆ°1çš„æ­£å€¼
        elif label == 'negative':
            return -score  # 0åˆ°-1çš„è´Ÿå€¼
        elif label == 'neutral':
            return 0.0  # ä¸­æ€§ä¸º0
        else:
            print(f"æœªçŸ¥æ ‡ç­¾: {label}")
            return 0.0
    
    def analyze_labmt(self, text: str) -> float:
        """LabMTæƒ…æ„Ÿåˆ†æï¼ˆéªŒè¯æ–¹æ³•ï¼‰"""
        if not text or not text.strip():
            return 0.0  # æ”¹ä¸º0.0ä»¥ä¸RoBERTaä¿æŒä¸€è‡´
        
        # æ–‡æœ¬é¢„å¤„ç†
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        
        if not words:
            return 0.0
        
        # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        total_score = 0
        valid_words = 0
        
        for word in words:
            if word in self.labmt_dict:
                total_score += self.labmt_dict[word]
                valid_words += 1
        
        if valid_words == 0:
            return 0.0
        
        avg_score = total_score / valid_words
        
        # è½¬æ¢ä¸º-1åˆ°1èŒƒå›´ï¼ˆä¸ºäº†ä¸RoBERTaå¯¹æ¯”ï¼‰
        # LabMT: 1-9 -> è½¬æ¢ä¸º -1åˆ°1
        normalized_score = (avg_score - 5.0) / 4.0  # (score-5)/4
        return max(-1, min(1, normalized_score))
    
    def parse_story(self, md_content: str) -> List[Dict]:
        """è§£æstoryæ–‡ä»¶"""
        chapters = []
        
        # æŒ‰ç« èŠ‚åˆ†å‰²
        chapter_pattern = r'# Chapter (\d+)[ï¼š:]([^\n]+)'
        splits = re.split(chapter_pattern, md_content)
        
        if len(splits) < 3:
            print("âš ï¸ æœªæ£€æµ‹åˆ°æ ‡å‡†ç« èŠ‚æ ¼å¼ï¼Œå°è¯•æŒ‰# åˆ†å‰²")
            parts = md_content.split('\n# ')
            for i, part in enumerate(parts[1:], 1):
                lines = part.split('\n')
                title = lines[0].strip()
                content = '\n'.join(lines[1:]).strip()
                if content:
                    chapters.append({
                        'chapter_num': i,
                        'title': title,
                        'content': content
                    })
        else:
            for i in range(1, len(splits), 3):
                if i + 2 < len(splits):
                    chapter_num = int(splits[i])
                    title = splits[i + 1].strip()
                    content = splits[i + 2].strip()
                    
                    # æ¸…ç†å†…å®¹
                    content = re.sub(r'-{20,}', '', content)
                    content = content.strip()
                    
                    chapters.append({
                        'chapter_num': chapter_num,
                        'title': title,
                        'content': content
                    })
        
        print(f"âœ… è§£æåˆ° {len(chapters)} ä¸ªç« èŠ‚")
        return chapters
    
    def analyze_story_dual_method(self, chapters: List[Dict]) -> Dict:
        """åŒæ–¹æ³•åˆ†ææ•…äº‹æƒ…æ„Ÿå¼§çº¿ï¼ˆRoBERTaç‰ˆï¼‰"""
        if not chapters:
            return {"error": "æ²¡æœ‰ç« èŠ‚æ•°æ®"}
        
        print("ğŸ” å¼€å§‹åŒæ–¹æ³•æƒ…æ„Ÿåˆ†æï¼ˆRoBERTa + LabMTï¼‰...")
        
        # 1. å¯¹æ¯ç« è¿›è¡ŒåŒæ–¹æ³•åˆ†æ
        chapter_analysis = []
        roberta_scores = []
        labmt_scores = []
        
        for chapter in chapters:
            print(f"  åˆ†æç¬¬ {chapter['chapter_num']} ç« : {chapter['title'][:30]}...")
            
            # RoBERTaåˆ†æï¼ˆä¸»è¦ï¼‰
            roberta_score = self.analyze_with_roberta_correct(chapter['content'])
            
            # LabMTåˆ†æï¼ˆéªŒè¯ï¼‰
            labmt_score = self.analyze_labmt(chapter['content'])
            
            chapter_analysis.append({
                'chapter_num': chapter['chapter_num'],
                'title': chapter['title'],
                'roberta_score': round(roberta_score, 4),
                'labmt_score': round(labmt_score, 4),
                'content_length': len(chapter['content'])
            })
            
            roberta_scores.append(roberta_score)
            labmt_scores.append(labmt_score)
        
        # 2. è®¡ç®—ä¸¤ç§æ–¹æ³•çš„ç›¸å…³æ€§
        correlation_analysis = self._analyze_correlation(roberta_scores, labmt_scores)
        
        # 3. Reaganå…­å‹åˆ†ç±»ï¼ˆåŸºäºä¸»è¦æ–¹æ³•RoBERTaï¼‰
        reagan_classification = self._classify_reagan_arc(roberta_scores, method="RoBERTa")
        
        # 4. éªŒè¯åˆ†ç±»ï¼ˆåŸºäºLabMTï¼‰
        labmt_classification = self._classify_reagan_arc(labmt_scores, method="LabMT")
        
        # 5. ç”Ÿæˆå¯¹æ¯”åˆ†æ
        comparison_analysis = self._generate_comparison_analysis(
            roberta_scores, labmt_scores, reagan_classification, labmt_classification
        )
        
        # 6. ç»„è£…ç»“æœï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
        result = {
            'metadata': {
                'total_chapters': len(chapters),
                'primary_method': 'RoBERTa',
                'validation_method': f'LabMT-en-{getattr(self, "actual_labmt_version", self.labmt_version)}',
                'analysis_timestamp': datetime.now().isoformat()
            },
            'chapter_analysis': chapter_analysis,
            'primary_analysis': {
                'method': 'RoBERTa',
                'scores': roberta_scores,
                'reagan_classification': reagan_classification
            },
            'validation_analysis': {
                'method': 'LabMT', 
                'scores': labmt_scores,
                'reagan_classification': labmt_classification
            },
            'correlation_analysis': correlation_analysis,
            'comparison_analysis': comparison_analysis,
            'final_conclusion': self._generate_final_conclusion(correlation_analysis, comparison_analysis)
        }
        
        print("âœ… åŒæ–¹æ³•åˆ†æå®Œæˆï¼")
        print(f"ğŸ“Š ç›¸å…³ç³»æ•°: {correlation_analysis['pearson_correlation']['r']:.3f}")
        print(f"ğŸ¯ RoBERTaåˆ†ç±»: {reagan_classification['best_match']} (ç½®ä¿¡åº¦: {reagan_classification['confidence']:.3f})")
        print(f"ğŸ” LabMTåˆ†ç±»: {labmt_classification['best_match']} (ç½®ä¿¡åº¦: {labmt_classification['confidence']:.3f})")
        
        return result
    
    # ä¿ç•™åŸæœ‰çš„å…¶ä»–æ–¹æ³•ä¸å˜
    def _analyze_correlation(self, scores1: List[float], scores2: List[float]) -> Dict:
        """åˆ†æä¸¤ç§æ–¹æ³•çš„ç›¸å…³æ€§"""
        if len(scores1) != len(scores2) or len(scores1) < 2:
            return {"error": "æ•°æ®ä¸è¶³"}
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        pearson_r, pearson_p = pearsonr(scores1, scores2)
        spearman_r, spearman_p = spearmanr(scores1, scores2)
        
        # è®¡ç®—æ–¹å‘ä¸€è‡´æ€§
        direction_agreement = 0
        for i in range(1, len(scores1)):
            roberta_direction = np.sign(scores1[i] - scores1[i-1])
            labmt_direction = np.sign(scores2[i] - scores2[i-1])
            if roberta_direction == labmt_direction:
                direction_agreement += 1
        
        direction_consistency = direction_agreement / max(1, len(scores1) - 1)
        
        return {
            'pearson_correlation': {
                'r': round(pearson_r, 4),
                'p_value': round(pearson_p, 4),
                'significance': 'Significant' if pearson_p < 0.05 else 'Not Significant'
            },
            'spearman_correlation': {
                'r': round(spearman_r, 4),
                'p_value': round(spearman_p, 4)
            },
            'direction_consistency': round(direction_consistency, 4),
            'consistency_level': 'High' if abs(pearson_r) > 0.7 else 'Medium' if abs(pearson_r) > 0.5 else 'Low',
            'interpretation': f"RoBERTaä¸LabMTç›¸å…³ç³»æ•°ä¸º{pearson_r:.3f}ï¼Œå±äº{'å¼ºç›¸å…³' if abs(pearson_r) > 0.7 else 'ä¸­ç­‰ç›¸å…³' if abs(pearson_r) > 0.5 else 'å¼±ç›¸å…³'}"
        }
    
    def _classify_reagan_arc(self, scores: List[float], method: str) -> Dict:
        """Reaganå…­ç§å¼§çº¿åˆ†ç±»"""
        if len(scores) < 3:
            return {
                'best_match': 'Unknown',
                'confidence': 0.0,
                'method': method,
                'all_similarities': {}
            }
        
        # ç”Ÿæˆæ ‡å‡†å¼§çº¿æ¨¡å¼
        standard_arcs = self._generate_standard_arcs(len(scores))
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = {}
        for arc_name, arc_pattern in standard_arcs.items():
            similarity = self._cosine_similarity(scores, arc_pattern)
            similarities[arc_name] = round(max(0, similarity), 4)
        
        best_match = max(similarities, key=similarities.get)
        confidence = similarities[best_match]
        
        return {
            'method': method,
            'best_match': best_match,
            'confidence': confidence,
            'all_similarities': similarities,
            'reagan_category': self._get_reagan_category(best_match)
        }
    
    def _generate_standard_arcs(self, length: int) -> Dict[str, List[float]]:
        """ç”Ÿæˆæ ‡å‡†å¼§çº¿æ¨¡å¼"""
        x = np.linspace(0, 1, length)
        
        return {
            'Rags to riches': [0.8 * (t - 0.2) for t in x],
            'Tragedy': [0.8 * (0.8 - t) for t in x],
            'Man in a hole': [0.5 * np.sin(np.pi * t - np.pi/2) for t in x],
            'Icarus': [0.5 * np.sin(np.pi * t + np.pi/2) for t in x],
            'Cinderella': [0.5 * np.sin(2 * np.pi * t - np.pi/2) for t in x],
            'Oedipus': [0.5 * np.sin(2 * np.pi * t + np.pi/2) for t in x]
        }
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if len(vec1) != len(vec2):
            return 0.0
        
        v1 = np.array(vec1)
        v2 = np.array(vec2)
        
        dot_product = np.dot(v1, v2)
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return max(0, dot_product / (norm1 * norm2))
    
    def _get_reagan_category(self, arc_type: str) -> str:
        """è·å–ReaganåŸæ–‡åˆ†ç±»ä»£ç """
        categories = {
            "Rags to riches": "RR",
            "Tragedy": "TR", 
            "Man in a hole": "MH",
            "Icarus": "IC",
            "Cinderella": "CN",
            "Oedipus": "OE"
        }
        return categories.get(arc_type, "UK")
    
    def _generate_comparison_analysis(self, roberta_scores, labmt_scores, 
                                    roberta_class, labmt_class) -> Dict:
        """ç”Ÿæˆå¯¹æ¯”åˆ†æ"""
        
        # åˆ†æåˆ†æ­§ç‚¹
        disagreement_points = []
        for i, (r_score, l_score) in enumerate(zip(roberta_scores, labmt_scores)):
            diff = abs(r_score - l_score)
            if diff > 0.3:  # åˆ†æ­§é˜ˆå€¼
                disagreement_points.append({
                    'chapter': i + 1,
                    'roberta_score': r_score,
                    'labmt_score': l_score,
                    'difference': round(diff, 4)
                })
        
        # æ–¹æ³•ä¼˜åŠ¿åˆ†æ
        method_advantages = {
            'RoBERTa': {
                'strengths': [
                    "æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯¹ä¸Šä¸‹æ–‡ç†è§£æ›´å¥½",
                    "é€‚åˆç°ä»£æ–‡æœ¬å’Œå¯¹è¯",
                    "å¯¹ç§‘å¹»ã€æŠ€æœ¯ç±»æ–‡æœ¬æ•æ„Ÿåº¦é«˜"
                ],
                'classification': roberta_class['best_match'],
                'confidence': roberta_class['confidence']
            },
            'LabMT': {
                'strengths': [
                    "ä¸ReaganåŸæ–‡æ–¹æ³•å®Œå…¨ä¸€è‡´",
                    "åŸºäºå¤§è§„æ¨¡äººå·¥æ ‡æ³¨",
                    "é€‚åˆä¼ ç»Ÿæ–‡å­¦åˆ†æ"
                ],
                'classification': labmt_class['best_match'],
                'confidence': labmt_class['confidence']
            }
        }
        
        # ä¸€è‡´æ€§è¯„ä¼°
        consistency_assessment = {
            'classification_agreement': roberta_class['best_match'] == labmt_class['best_match'],
            'major_disagreements': len(disagreement_points),
            'recommendation': self._generate_recommendation(roberta_class, labmt_class, disagreement_points)
        }
        
        return {
            'disagreement_points': disagreement_points,
            'method_advantages': method_advantages,
            'consistency_assessment': consistency_assessment
        }
    
    def _generate_recommendation(self, roberta_class, labmt_class, disagreements) -> str:
        """ç”Ÿæˆä½¿ç”¨å»ºè®®"""
        if roberta_class['best_match'] == labmt_class['best_match']:
            return f"ä¸¤ç§æ–¹æ³•éƒ½è¯†åˆ«ä¸º{roberta_class['best_match']}ï¼Œç»“æœé«˜åº¦ä¸€è‡´ï¼Œå»ºè®®ä»¥RoBERTaä¸ºä¸»è¦åˆ†æç»“æœ"
        elif len(disagreements) <= 2:
            return f"åˆ†ç±»ç•¥æœ‰å·®å¼‚ä½†æ•´ä½“ä¸€è‡´ï¼Œå»ºè®®ä»¥RoBERTaä¸ºä¸»ï¼ŒLabMTä¸ºéªŒè¯ï¼Œä½“ç°äº†ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¼˜åŠ¿"
        else:
            return f"ä¸¤ç§æ–¹æ³•å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®è¯¦ç»†åˆ†æå·®å¼‚åŸå› ï¼Œå¯èƒ½éœ€è¦æ›´æ·±å…¥çš„æ–‡æœ¬ç‰¹å¾åˆ†æ"
    
    def _generate_final_conclusion(self, correlation_analysis, comparison_analysis) -> str:
        """ç”Ÿæˆæœ€ç»ˆç»“è®º"""
        pearson_r = correlation_analysis['pearson_correlation']['r']
        consistency_level = correlation_analysis['consistency_level']
        
        conclusion = f"""
ğŸ“Š RoBERTa + LabMT åŒæ–¹æ³•åˆ†æç»“è®ºï¼š

ğŸ“ˆ ç›¸å…³æ€§åˆ†æï¼š
- Pearsonç›¸å…³ç³»æ•°: r = {pearson_r:.3f} ({consistency_level})
- ä¸€è‡´æ€§æ°´å¹³: {correlation_analysis['interpretation']}

ğŸ¯ åˆ†ç±»ç»“æœï¼š
- RoBERTa (ä¸»è¦): {comparison_analysis['method_advantages']['RoBERTa']['classification']}
- LabMT (éªŒè¯): {comparison_analysis['method_advantages']['LabMT']['classification']}

ğŸ“ æ–¹æ³•å­¦è¯´æ˜ï¼š
æœ¬ç ”ç©¶é‡‡ç”¨RoBERTaä½œä¸ºä¸»è¦åˆ†ææ–¹æ³•ï¼ˆç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰ï¼Œ
ä½¿ç”¨LabMTè¿›è¡Œäº¤å‰éªŒè¯ï¼ˆä¸Reagan et al. 2016ä¿æŒä¸€è‡´ï¼‰ã€‚
ç›¸å…³ç³»æ•°ä¸º{pearson_r:.3f}ï¼Œä¸¤ç§æ–¹æ³•{consistency_level.lower()}ä¸€è‡´ã€‚

å»ºè®®ï¼šä»¥RoBERTaç»“æœä¸ºä¸»ï¼ŒLabMTä½œä¸ºå­¦æœ¯åŸºå‡†éªŒè¯ã€‚
        """.strip()
        
        return conclusion
    
    def save_results(self, result: Dict, output_dir: str = "./output/") -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"roberta_labmt_analysis_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜è‡³ï¼š{filepath}")
        return filepath


def analyze_story_dual_method(file_path: str, output_dir: str = "./output/") -> Dict:
    """
    RoBERTa + LabMT åŒæ–¹æ³•æ•…äº‹åˆ†æå®Œæ•´æµç¨‹
    
    Args:
        file_path: æ•…äº‹æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        æ ‡å‡†æ ¼å¼çš„å®Œæ•´åˆ†æç»“æœ
    """
    print("ğŸš€ å¼€å§‹RoBERTa + LabMTåŒæ–¹æ³•æƒ…æ„Ÿå¼§çº¿åˆ†æ...")
    print(f"ğŸ“„ ä¸»è¦æ–¹æ³•: RoBERTa (æ·±åº¦å­¦ä¹ )")
    print(f"ğŸ” éªŒè¯æ–¹æ³•: LabMT (ReaganåŸæ–‡ä¸€è‡´)")
    
    # 1. è¯»å–æ–‡ä»¶
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        print(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼š{file_path}")
    except Exception as e:
        return {"error": f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{e}"}
    
    # 2. åˆå§‹åŒ–åˆ†æå™¨
    analyzer = DualMethodEmotionalAnalyzer()
    
    # 3. è§£ææ•…äº‹
    chapters = analyzer.parse_story(content)
    if not chapters:
        return {"error": "æœªèƒ½è§£æåˆ°ç« èŠ‚å†…å®¹"}
    
    # 4. åŒæ–¹æ³•åˆ†æ
    result = analyzer.analyze_story_dual_method(chapters)
    if "error" in result:
        return result
    
    # 5. ä¿å­˜ç»“æœ
    try:
        json_path = analyzer.save_results(result, output_dir)
        result['output_files'] = {'analysis_json': json_path}
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ç»“æœå¤±è´¥ï¼š{e}")
        result['output_files'] = {}
    
    # 6. æ‰“å°ç»“æœæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š RoBERTa + LabMT åŒæ–¹æ³•åˆ†æå®Œæˆï¼")
    print("="*60)
    
    # æ‰“å°ä¸»è¦ç»“æœ
    primary = result['primary_analysis']
    validation = result['validation_analysis']
    correlation = result['correlation_analysis']
    
    print(f"ğŸ¯ ä¸»è¦åˆ†æ (RoBERTa): {primary['reagan_classification']['best_match']}")
    print(f"ğŸ” éªŒè¯åˆ†æ (LabMT): {validation['reagan_classification']['best_match']}")
    print(f"ğŸ“ˆ ç›¸å…³ç³»æ•°: r={correlation['pearson_correlation']['r']:.3f} ({correlation['consistency_level']})")
    
    print(f"\nğŸ“‹ å„ç« èŠ‚åˆ†æ•°å¯¹æ¯”:")
    for ch in result['chapter_analysis']:
        print(f"  ç¬¬{ch['chapter_num']}ç« : RoBERTa={ch['roberta_score']:.3f}, LabMT={ch['labmt_score']:.3f}")
    
    print(f"\n{result['final_conclusion']}")
    
    return result


# ç›´æ¥è¿è¡Œç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ­ RoBERTa + LabMT åŒæ–¹æ³•æƒ…æ„Ÿå¼§çº¿åˆ†æå™¨")
    print("ä¸»è¦æ–¹æ³•ï¼šRoBERTaï¼ˆç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰")
    print("éªŒè¯æ–¹æ³•ï¼šLabMTï¼ˆä¸ReaganåŸæ–‡ä¸€è‡´ï¼‰")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•…äº‹æ–‡ä»¶
    possible_files = [
        '/Users/haha/Story/data/output/å°çº¢å¸½_ç§‘å¹»_linear_T0.7_s1/enhance.md',  # ç”¨æˆ·çš„å…·ä½“æ–‡ä»¶
        'enhanced_story_dialogue_updated.md',
        'data/enhanced_story_dialogue_updated.md',
        './output/å°çº¢å¸½_ç§‘å¹»_linear_T0.7_s1/enhance.md',
        './å°çº¢å¸½_ç§‘å¹»_linear_T0.7_s1/enhance.md'
    ]
    
    story_file = None
    for file_path in possible_files:
        if os.path.exists(file_path):
            story_file = file_path
            break
    
    if story_file:
        print(f"\nğŸ“ æ‰¾åˆ°æ•…äº‹æ–‡ä»¶: {story_file}")
        print("ğŸš€ å¼€å§‹åˆ†æ...")
        
        try:
            result = analyze_story_dual_method(story_file, "./output/")
            
            # ä¿å­˜resultå˜é‡ä¾›åç»­ä½¿ç”¨
            print(f"\nğŸ’¾ resultå˜é‡å·²ç”Ÿæˆï¼Œå¯ç”¨äºå¯è§†åŒ–:")
            print(f"   - ç±»å‹: {type(result)}")
            print(f"   - ç« èŠ‚æ•°: {result['metadata']['total_chapters']}")
            print(f"   - ç›¸å…³æ€§: {result['correlation_analysis']['pearson_correlation']['r']:.3f}")
            
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")
            print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š")
            print("1. æ£€æŸ¥æ•…äº‹æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
            print("2. å®‰è£…ç¼ºå¤±çš„ä¾èµ–: pip install transformers torch nltk pandas requests scipy")
            print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆä¸‹è½½æ¨¡å‹éœ€è¦ï¼‰")
    else:
        print("\nâŒ æœªæ‰¾åˆ°æ•…äº‹æ–‡ä»¶")
        print("ğŸ“ è¯·ç¡®ä¿æ–‡ä»¶åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€ï¼š")
        for fp in possible_files:
            print(f"   - {fp}")
        print("\nğŸ’¡ æˆ–è€…æ‰‹åŠ¨æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼š")
        print("   from emotional_arc_analyzer import analyze_story_dual_method")
        print("   result = analyze_story_dual_method('ä½ çš„æ–‡ä»¶è·¯å¾„.md')")