import os
import sys
import json
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

try:
    from src.utils.utils import load_json, save_json
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥é¡¹ç›®å·¥å…·å‡½æ•°ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    
    def load_json(file_path):
        """ç®€åŒ–ç‰ˆload_json"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def save_json(data, version, filename):
        """ç®€åŒ–ç‰ˆsave_json"""
        # å°è¯•æ¨æ–­outputç›®å½•
        possible_output_dirs = [
            os.path.join(project_root, "output"),
            os.path.join(project_root, "outputs"),
            os.path.join(project_root, "data", "output"),
            os.path.join(project_root, "results")
        ]
        
        output_dir = None
        for dir_path in possible_output_dirs:
            version_path = os.path.join(dir_path, version)
            if os.path.exists(version_path):
                output_dir = dir_path
                break
        
        if not output_dir:
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä½œä¸ºé»˜è®¤
            output_dir = possible_output_dirs[0]
            os.makedirs(os.path.join(output_dir, version), exist_ok=True)
        
        file_path = os.path.join(output_dir, version, filename)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


class HREDCoherenceEvaluator:
    """
    åŸºäºHREDæ€æƒ³çš„è¯­ä¹‰è¿è´¯æ€§è¯„ä»·å™¨
    ä¸“æ³¨äºåŸæ–‡å¥å­çš„è¯­ä¹‰è¿è´¯æ€§åˆ†æ
    """
    
    def __init__(self, model_name="all-mpnet-base-v2"):
        """
        åˆå§‹åŒ–å‘é‡æ¨¡å‹
        
        Args:
            model_name: sentence-transformersæ¨¡å‹åç§°
                      - "all-mpnet-base-v2": æ¨èï¼Œæ€§èƒ½æœ€å¥½
                      - "all-MiniLM-L6-v2": æ›´å¿«ï¼Œç¨ä½ç²¾åº¦
                      - "paraphrase-multilingual-mpnet-base-v2": å¤šè¯­è¨€
        """
        print(f"ğŸ”„ åŠ è½½sentence-transformersæ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.model_name = model_name
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def extract_sentences_from_story(self, story_data):
        """
        ä»åŸæ–‡æ•…äº‹ä¸­æå–æ‰€æœ‰å¥å­
        
        Args:
            story_data: æ•…äº‹æ•°æ®ï¼ŒåŒ…å«chaptersçš„list
        
        Returns:
            List[str]: åŸæ–‡å¥å­åˆ—è¡¨
        """
        try:
            from src.utils.utils import split_plot_into_sentences
        except ImportError:
            # ç®€åŒ–ç‰ˆå¥å­åˆ†å‰²å‡½æ•°
            def split_plot_into_sentences(text):
                """ç®€åŒ–ç‰ˆå¥å­åˆ†å‰²"""
                # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
                import re
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)
                return [s.strip() for s in sentences if s.strip()]
            
        all_sentences = []
        
        for chapter in story_data:
            plot = chapter.get('plot', '')
            if plot.strip():
                try:
                    sentences = split_plot_into_sentences(plot)
                except:
                    # å¦‚æœåˆ†å‰²å‡½æ•°å¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²
                    import re
                    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', plot)
                    sentences = [s.strip() for s in sentences if s.strip()]
                
                all_sentences.extend([s.strip() for s in sentences if s.strip()])
        
        return all_sentences
    
    def compute_embeddings(self, sentences):
        """
        è®¡ç®—å¥å­çš„å‘é‡è¡¨ç¤º
        
        Args:
            sentences: å¥å­åˆ—è¡¨
        
        Returns:
            np.ndarray: å¥å­å‘é‡çŸ©é˜µ (n_sentences, embedding_dim)
        """
        print(f"ğŸ§® è®¡ç®— {len(sentences)} ä¸ªå¥å­çš„å‘é‡è¡¨ç¤º...")
        
        # ä½¿ç”¨sentence-transformersè®¡ç®—åµŒå…¥
        embeddings = self.model.encode(sentences, convert_to_numpy=True)
        
        print(f"âœ… å‘é‡è®¡ç®—å®Œæˆï¼Œç»´åº¦: {embeddings.shape}")
        return embeddings
    
    def compute_adjacent_similarities(self, embeddings):
        """
        è®¡ç®—ç›¸é‚»å¥å­ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            embeddings: å¥å­å‘é‡çŸ©é˜µ
        
        Returns:
            List[float]: ç›¸é‚»å¥å­ç›¸ä¼¼åº¦åˆ—è¡¨
        """
        if len(embeddings) < 2:
            print("âš ï¸ å¥å­æ•°é‡ä¸è¶³2ä¸ªï¼Œæ— æ³•è®¡ç®—ç›¸é‚»ç›¸ä¼¼åº¦")
            return []
        
        similarities = []
        
        for i in range(len(embeddings) - 1):
            # è®¡ç®—ç¬¬iä¸ªå’Œç¬¬i+1ä¸ªå¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦
            sim = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]
            similarities.append(float(sim))
        
        return similarities
    
    def analyze_coherence_patterns(self, similarities, sentences):
        """
        åˆ†æè¿è´¯æ€§æ¨¡å¼
        
        Args:
            similarities: ç›¸é‚»å¥å­ç›¸ä¼¼åº¦åˆ—è¡¨
            sentences: å¥å­åˆ—è¡¨
        
        Returns:
            dict: è¯¦ç»†çš„è¿è´¯æ€§åˆ†æç»“æœ
        """
        if not similarities:
            return {
                "error": "æ— æ³•è®¡ç®—è¿è´¯æ€§æ¨¡å¼",
                "reason": "ç›¸ä¼¼åº¦åˆ—è¡¨ä¸ºç©º"
            }
        
        similarities_array = np.array(similarities)
        
        # åŸºæœ¬ç»Ÿè®¡
        stats = {
            "å¹³å‡è¿è´¯æ€§": float(np.mean(similarities_array)),
            "è¿è´¯æ€§æ ‡å‡†å·®": float(np.std(similarities_array)),
            "æœ€é«˜è¿è´¯æ€§": float(np.max(similarities_array)),
            "æœ€ä½è¿è´¯æ€§": float(np.min(similarities_array)),
            "è¿è´¯æ€§ä¸­ä½æ•°": float(np.median(similarities_array))
        }
        
        # è¯†åˆ«è¿è´¯æ€§æ–­ç‚¹ï¼ˆç›¸ä¼¼åº¦æ˜¾è‘—ä¸‹é™çš„ä½ç½®ï¼‰
        threshold = np.mean(similarities_array) - np.std(similarities_array)
        low_coherence_points = []
        
        for i, sim in enumerate(similarities):
            if sim < threshold:
                low_coherence_points.append({
                    "ä½ç½®": i + 1,
                    "å¥å­å¯¹": f"{sentences[i][:30]}... â†’ {sentences[i+1][:30]}...",
                    "ç›¸ä¼¼åº¦": round(sim, 3)
                })
        
        # è¯†åˆ«é«˜è¿è´¯æ€§æ®µè½
        high_threshold = np.mean(similarities_array) + 0.5 * np.std(similarities_array)
        high_coherence_segments = []
        
        current_segment = []
        for i, sim in enumerate(similarities):
            if sim > high_threshold:
                current_segment.append(i)
            else:
                if len(current_segment) >= 2:  # è‡³å°‘3ä¸ªè¿ç»­å¥å­
                    high_coherence_segments.append({
                        "èµ·å§‹å¥å­": current_segment[0] + 1,
                        "ç»“æŸå¥å­": current_segment[-1] + 2,
                        "é•¿åº¦": len(current_segment) + 1,
                        "å¹³å‡ç›¸ä¼¼åº¦": round(np.mean([similarities[j] for j in current_segment]), 3)
                    })
                current_segment = []
        
        # æ£€æŸ¥æœ€åä¸€ä¸ªæ®µè½
        if len(current_segment) >= 2:
            high_coherence_segments.append({
                "èµ·å§‹å¥å­": current_segment[0] + 1,
                "ç»“æŸå¥å­": current_segment[-1] + 2,
                "é•¿åº¦": len(current_segment) + 1,
                "å¹³å‡ç›¸ä¼¼åº¦": round(np.mean([similarities[j] for j in current_segment]), 3)
            })
        
        return {
            "åŸºæœ¬ç»Ÿè®¡": stats,
            "è¿è´¯æ€§æ–­ç‚¹": low_coherence_points,
            "é«˜è¿è´¯æ€§æ®µè½": high_coherence_segments,
            "å®¢è§‚æè¿°": self._describe_coherence_objectively(stats["å¹³å‡è¿è´¯æ€§"])
        }
    
    def _describe_coherence_objectively(self, avg_coherence):
        """
        å®¢è§‚æè¿°è¿è´¯æ€§ï¼Œä¸ç»™ä¸»è§‚è¯„çº§
        
        Args:
            avg_coherence: å¹³å‡è¿è´¯æ€§åˆ†æ•°
        
        Returns:
            dict: å®¢è§‚æè¿°
        """
        return {
            "åˆ†æ•°": round(avg_coherence, 4),
            "èŒƒå›´": "0-1ï¼ˆ1ä¸ºå®Œå…¨ç›¸ä¼¼ï¼‰",
            "è§£é‡Š": "åŸºäºsentence-transformersæ¨¡å‹è®¡ç®—çš„ç›¸é‚»å¥å­è¯­ä¹‰ç›¸ä¼¼åº¦å‡å€¼",
            "å‚è€ƒ": "å»ºè®®ä¸å…¶ä»–æ–‡æœ¬å¯¹æ¯”ï¼Œè€Œéç»å¯¹è¯„çº§"
        }
    
    def evaluate_story_coherence(self, story_data, include_details=True):
        """
        å®Œæ•´çš„æ•…äº‹è¿è´¯æ€§è¯„ä»·
        
        Args:
            story_data: åŸæ–‡æ•…äº‹æ•°æ®
            include_details: æ˜¯å¦åŒ…å«è¯¦ç»†åˆ†æ
        
        Returns:
            dict: è¿è´¯æ€§è¯„ä»·ç»“æœ
        """
        print(f"\nğŸ” å¼€å§‹HREDè¯­ä¹‰è¿è´¯æ€§åˆ†æ...")
        print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {self.model_name}")
        print(f"ğŸ“‹ åˆ†ææ–¹å¼: åŸæ–‡å¥å­è¿è´¯æ€§")
        
        # Step 1: æå–åŸæ–‡å¥å­
        sentences = self.extract_sentences_from_story(story_data)
        print(f"ğŸ“ æå–åˆ° {len(sentences)} ä¸ªæœ‰æ•ˆå¥å­")
        
        if len(sentences) < 2:
            return {
                "é”™è¯¯": "å¥å­æ•°é‡ä¸è¶³",
                "è¯¦æƒ…": f"éœ€è¦è‡³å°‘2ä¸ªå¥å­ï¼Œå½“å‰æœ‰{len(sentences)}ä¸ª",
                "å»ºè®®": "è¯·ç¡®ä¿è¾“å…¥åŒ…å«è¶³å¤Ÿçš„å¥å­"
            }
        
        # Step 2: è®¡ç®—å‘é‡è¡¨ç¤º
        embeddings = self.compute_embeddings(sentences)
        
        # Step 3: è®¡ç®—ç›¸é‚»ç›¸ä¼¼åº¦
        similarities = self.compute_adjacent_similarities(embeddings)
        print(f"ğŸ”— è®¡ç®—äº† {len(similarities)} ä¸ªç›¸é‚»å¥å­å¯¹çš„ç›¸ä¼¼åº¦")
        
        # Step 4: åŸºæœ¬è¯„ä»·ç»“æœ
        avg_coherence = np.mean(similarities) if similarities else 0
        
        result = {
            "HREDè¿è´¯æ€§è¯„ä»·": {
                "æ¨¡å‹åç§°": self.model_name,
                "åˆ†ææ–¹å¼": "åŸæ–‡å¥å­è¿è´¯æ€§",
                "å¥å­æ€»æ•°": len(sentences),
                "ç›¸é‚»å¯¹æ•°": len(similarities),
                "å¹³å‡è¿è´¯æ€§": round(avg_coherence, 4),
                "å®¢è§‚æè¿°": self._describe_coherence_objectively(avg_coherence)
            }
        }
        
        # Step 5: è¯¦ç»†åˆ†æï¼ˆå¯é€‰ï¼‰
        if include_details:
            print("ğŸ” è¿›è¡Œè¯¦ç»†è¿è´¯æ€§æ¨¡å¼åˆ†æ...")
            detailed_analysis = self.analyze_coherence_patterns(similarities, sentences)
            result["è¯¦ç»†åˆ†æ"] = detailed_analysis
            
            # æ·»åŠ é€å¯¹ç›¸ä¼¼åº¦ï¼ˆåªæ˜¾ç¤ºå‰20å¯¹ï¼Œé¿å…è¿‡é•¿ï¼‰
            result["é€å¯¹ç›¸ä¼¼åº¦ç¤ºä¾‹"] = []
            max_pairs = min(20, len(similarities))
            for i in range(max_pairs):
                result["é€å¯¹ç›¸ä¼¼åº¦ç¤ºä¾‹"].append({
                    "å¥å­å¯¹": f"å¥å­{i+1} â†’ å¥å­{i+2}",
                    "å¥å­1": sentences[i][:50] + "..." if len(sentences[i]) > 50 else sentences[i],
                    "å¥å­2": sentences[i+1][:50] + "..." if len(sentences[i+1]) > 50 else sentences[i+1],
                    "ç›¸ä¼¼åº¦": round(similarities[i], 4)
                })
            
            if len(similarities) > 20:
                result["é€å¯¹ç›¸ä¼¼åº¦ç¤ºä¾‹"].append({
                    "è¯´æ˜": f"ä»…æ˜¾ç¤ºå‰20å¯¹ï¼Œæ€»å…±æœ‰{len(similarities)}å¯¹ç›¸é‚»å¥å­"
                })
        
        print(f"âœ… HREDè¿è´¯æ€§åˆ†æå®Œæˆ")
        print(f"ğŸ“Š å¹³å‡è¿è´¯æ€§: {avg_coherence:.4f} (0-1èŒƒå›´ï¼Œè¶Šæ¥è¿‘1è¶Šè¿è´¯)")
        
        return result


def evaluate_story_coherence_from_file(version, story_file="story_updated.json", model_name="all-mpnet-base-v2"):
    """
    ä»åŸæ–‡æ•…äº‹æ–‡ä»¶è¯„ä»·è¯­ä¹‰è¿è´¯æ€§
    
    Args:
        version: ç‰ˆæœ¬æ–‡ä»¶å¤¹å
        story_file: åŸæ–‡æ•…äº‹æ–‡ä»¶å
        model_name: sentence-transformersæ¨¡å‹å
    
    Returns:
        dict: è¿è´¯æ€§è¯„ä»·ç»“æœ
    """
    # å°è¯•å¯¼å…¥output_dirï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    try:
        from src.constant import output_dir
    except ImportError:
        # æ¨æ–­outputç›®å½•
        possible_dirs = ["output", "outputs", "data/output", "results"]
        output_dir = None
        for dir_name in possible_dirs:
            dir_path = os.path.join(project_root, dir_name)
            if os.path.exists(dir_path):
                output_dir = dir_path
                break
        
        if not output_dir:
            output_dir = os.path.join(project_root, "output")
            print(f"âš ï¸ ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•: {output_dir}")
    
    print(f"\nğŸ” å¼€å§‹åŸæ–‡è¯­ä¹‰è¿è´¯æ€§è¯„ä»·ï¼š{version}")
    
    # è¯»å–åŸæ–‡æ•…äº‹æ•°æ®
    story_path = os.path.join(output_dir, version, story_file)
    if not os.path.exists(story_path):
        print(f"âš ï¸ æ•…äº‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{story_path}")
        print(f"ğŸ“ æŸ¥æ‰¾çš„è·¯å¾„ï¼š{story_path}")
        return None
    
    story_data = load_json(story_path)
    
    # åˆ›å»ºè¯„ä»·å™¨å¹¶åˆ†æ
    evaluator = HREDCoherenceEvaluator(model_name=model_name)
    coherence_result = evaluator.evaluate_story_coherence(story_data, include_details=True)
    
    # ä¿å­˜ç»“æœ
    output_filename = f"hred_coherence_analysis_{model_name.replace('/', '_')}.json"
    save_json(coherence_result, version, output_filename)
    
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")
    
    return coherence_result


def compare_coherence_models(version, story_file="story_updated.json"):
    """
    ä½¿ç”¨å¤šä¸ªæ¨¡å‹æ¯”è¾ƒè¿è´¯æ€§è¯„ä»·ç»“æœ
    
    Args:
        version: ç‰ˆæœ¬æ–‡ä»¶å¤¹å
        story_file: åŸæ–‡æ•…äº‹æ–‡ä»¶å
    
    Returns:
        dict: å¤šæ¨¡å‹æ¯”è¾ƒç»“æœ
    """
    models_to_test = [
        "all-mpnet-base-v2",           # æ¨èï¼šæ€§èƒ½æœ€å¥½
        "all-MiniLM-L6-v2",           # å¿«é€Ÿï¼šé€Ÿåº¦ä¼˜å…ˆ
        "paraphrase-mpnet-base-v2"     # é‡Šä¹‰ï¼šç†è§£ç›¸ä¼¼è¡¨è¾¾
    ]
    
    print(f"\nğŸ” å¤šæ¨¡å‹è¿è´¯æ€§è¯„ä»·æ¯”è¾ƒï¼š{version}")
    
    # å°è¯•å¯¼å…¥output_dir
    try:
        from src.constant import output_dir
    except ImportError:
        possible_dirs = ["output", "outputs", "data/output", "results"]
        output_dir = None
        for dir_name in possible_dirs:
            dir_path = os.path.join(project_root, dir_name)
            if os.path.exists(dir_path):
                output_dir = dir_path
                break
        if not output_dir:
            output_dir = os.path.join(project_root, "output")
    
    story_path = os.path.join(output_dir, version, story_file)
    if not os.path.exists(story_path):
        print(f"âš ï¸ æ•…äº‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{story_path}")
        return None
    
    story_data = load_json(story_path)
    
    # é¢„è®¡ç®—å¥å­æ•°é‡ç”¨äºæ˜¾ç¤º
    try:
        # å°è¯•ä½¿ç”¨é¡¹ç›®çš„splitå‡½æ•°
        try:
            from src.utils.utils import split_plot_into_sentences
        except ImportError:
            def split_plot_into_sentences(text):
                import re
                return re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)
        
        sentence_count = len([s for ch in story_data for s in split_plot_into_sentences(ch.get('plot', '')) if s.strip()])
    except:
        sentence_count = "æœªçŸ¥"
    
    comparison_results = {
        "ç‰ˆæœ¬": version,
        "åˆ†ææ–¹å¼": "åŸæ–‡å¥å­è¿è´¯æ€§",
        "å¥å­æ€»æ•°": sentence_count,
        "æ¨¡å‹æ¯”è¾ƒ": {}
    }
    
    for model_name in models_to_test:
        print(f"\n{'='*50}")
        print(f"ğŸ§® æµ‹è¯•æ¨¡å‹: {model_name}")
        
        try:
            evaluator = HREDCoherenceEvaluator(model_name=model_name)
            result = evaluator.evaluate_story_coherence(story_data, include_details=False)
            
            comparison_results["æ¨¡å‹æ¯”è¾ƒ"][model_name] = {
                "å¹³å‡è¿è´¯æ€§": result["HREDè¿è´¯æ€§è¯„ä»·"]["å¹³å‡è¿è´¯æ€§"],
                "å®¢è§‚æè¿°": result["HREDè¿è´¯æ€§è¯„ä»·"]["å®¢è§‚æè¿°"],
                "å¥å­æ€»æ•°": result["HREDè¿è´¯æ€§è¯„ä»·"]["å¥å­æ€»æ•°"],
                "çŠ¶æ€": "æˆåŠŸ"
            }
            
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹ {model_name} æµ‹è¯•å¤±è´¥: {e}")
            comparison_results["æ¨¡å‹æ¯”è¾ƒ"][model_name] = {
                "çŠ¶æ€": "å¤±è´¥",
                "é”™è¯¯": str(e)
            }
    
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    save_json(comparison_results, version, "hred_model_comparison.json")
    
    # æ‰“å°æ¯”è¾ƒæ‘˜è¦
    print(f"\n{'='*60}")
    print("ğŸ“Š åŸæ–‡å¥å­è¿è´¯æ€§æ¨¡å‹æ¯”è¾ƒæ‘˜è¦:")
    print("â”" * 60)
    print(f"{'æ¨¡å‹åç§°':<30} | {'è¿è´¯æ€§':<8} | {'çŠ¶æ€':<10}")
    print("â”" * 60)
    
    for model_name, result in comparison_results["æ¨¡å‹æ¯”è¾ƒ"].items():
        if result["çŠ¶æ€"] == "æˆåŠŸ":
            coherence = result["å¹³å‡è¿è´¯æ€§"]
            print(f"{model_name:<30} | {coherence:<8.4f} | {'æˆåŠŸ':<10}")
        else:
            print(f"{model_name:<30} | {'--':<8} | {'å¤±è´¥':<10}")
    
    print("â”" * 60)
    print("æ³¨ï¼šè¿è´¯æ€§åˆ†æ•°èŒƒå›´0-1ï¼Œè¶Šæ¥è¿‘1è¡¨ç¤ºç›¸é‚»å¥å­è¯­ä¹‰è¶Šç›¸ä¼¼")
    print("â”" * 60)
    
    return comparison_results


def add_coherence_to_story_evaluation(version, story_file="story_updated.json", model_name="all-mpnet-base-v2"):
    """
    å°†è¿è´¯æ€§åˆ†æé›†æˆåˆ°ç°æœ‰çš„æ•…äº‹è¯„ä»·ä¸­
    
    Args:
        version: ç‰ˆæœ¬æ–‡ä»¶å¤¹å
        story_file: åŸæ–‡æ•…äº‹æ–‡ä»¶å
        model_name: sentence-transformersæ¨¡å‹å
    
    Returns:
        dict: åŒ…å«è¿è´¯æ€§çš„å®Œæ•´è¯„ä»·ç»“æœ
    """
    # å°è¯•å¯¼å…¥output_dir
    try:
        from src.constant import output_dir
    except ImportError:
        possible_dirs = ["output", "outputs", "data/output", "results"]
        output_dir = None
        for dir_name in possible_dirs:
            dir_path = os.path.join(project_root, dir_name)
            if os.path.exists(dir_path):
                output_dir = dir_path
                break
        if not output_dir:
            output_dir = os.path.join(project_root, "output")
    
    print(f"\nğŸ” é›†æˆè¿è´¯æ€§åˆ†æåˆ°æ•…äº‹è¯„ä»·ï¼š{version}")
    
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰çš„ç»“æ„åˆ†æç»“æœ
    existing_files = [
        "story_structure_analysis_statistical.json",
        "story_structure_analysis_default.json", 
        "story_structure_analysis_fixed.json"
    ]
    
    structure_result = None
    for filename in existing_files:
        file_path = os.path.join(output_dir, version, filename)
        if os.path.exists(file_path):
            structure_result = load_json(file_path)
            print(f"ğŸ“‚ æ‰¾åˆ°ç°æœ‰ç»“æ„åˆ†æ: {filename}")
            break
    
    # 2. è¿›è¡Œè¿è´¯æ€§åˆ†æ
    coherence_result = evaluate_story_coherence_from_file(version, story_file, model_name)
    
    if not coherence_result:
        print("âš ï¸ è¿è´¯æ€§åˆ†æå¤±è´¥")
        return None
    
    # 3. åˆå¹¶ç»“æœ
    combined_result = {
        "ç‰ˆæœ¬": version,
        "è¯„ä»·æ—¶é—´": None,
        "è¿è´¯æ€§åˆ†æ": coherence_result
    }
    
    if structure_result:
        combined_result["ç»“æ„åˆ†æ"] = structure_result
        combined_result["è¯„ä»·æ¨¡å¼"] = structure_result.get("è¯„ä»·æ¨¡å¼", "æœªçŸ¥")
    
    # 4. ç”Ÿæˆç»¼åˆæ‘˜è¦
    coherence_score = coherence_result["HREDè¿è´¯æ€§è¯„ä»·"]["å¹³å‡è¿è´¯æ€§"]
    coherence_desc = coherence_result["HREDè¿è´¯æ€§è¯„ä»·"]["å®¢è§‚æè¿°"]
    
    summary = {
        "è¯­ä¹‰è¿è´¯æ€§": {
            "åˆ†æ•°": coherence_score,
            "æè¿°": coherence_desc
        }
    }
    
    if structure_result and "ç»“æ„åˆ†æ" in structure_result:
        struct_analysis = structure_result["ç»“æ„åˆ†æ"]
        summary["ç»“æ„å®Œæ•´æ€§"] = {
            "Papalampidi": struct_analysis.get("Papalampidiç»“æ„åˆ†æ", {}).get("è½¬æŠ˜ç‚¹å®Œæ•´æ€§", {}).get("TPè¦†ç›–ç‡", "æœªçŸ¥"),
            "LiåŠŸèƒ½": struct_analysis.get("LiåŠŸèƒ½åˆ†æ", {}).get("åŠŸèƒ½å¤šæ ·æ€§", "æœªçŸ¥")
        }
    
    combined_result["ç»¼åˆæ‘˜è¦"] = summary
    
    # 5. ä¿å­˜åˆå¹¶ç»“æœ
    save_json(combined_result, version, "complete_story_evaluation.json")
    
    # 6. æ‰“å°ç»¼åˆæ‘˜è¦
    print(f"\n{'='*50}")
    print("ğŸ“Š å®Œæ•´æ•…äº‹è¯„ä»·æ‘˜è¦:")
    print("â”" * 50)
    print(f"è¯­ä¹‰è¿è´¯æ€§: {coherence_score:.4f} (0-1èŒƒå›´)")
    if "ç»“æ„å®Œæ•´æ€§" in summary:
        print(f"ç»“æ„å®Œæ•´æ€§: TPè¦†ç›–ç‡{summary['ç»“æ„å®Œæ•´æ€§']['Papalampidi']}, åŠŸèƒ½å¤šæ ·æ€§{summary['ç»“æ„å®Œæ•´æ€§']['LiåŠŸèƒ½']}ç§")
    print("â”" * 50)
    print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: complete_story_evaluation.json")
    
    return combined_result


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="HREDåŸæ–‡å¥å­è¿è´¯æ€§è¯„ä»·å·¥å…·")
    parser.add_argument("--version", type=str, required=True, help="ç‰ˆæœ¬æ–‡ä»¶å¤¹å")
    parser.add_argument("--story-file", type=str, default="story_updated.json", 
                       help="åŸæ–‡æ•…äº‹æ–‡ä»¶å")
    parser.add_argument("--model", type=str, default="all-mpnet-base-v2", 
                       help="sentence-transformersæ¨¡å‹å")
    parser.add_argument("--compare-models", action="store_true", 
                       help="æ˜¯å¦æ¯”è¾ƒå¤šä¸ªæ¨¡å‹æ•ˆæœ")
    parser.add_argument("--integrate", action="store_true",
                       help="æ˜¯å¦é›†æˆåˆ°ç°æœ‰çš„æ•…äº‹è¯„ä»·ä¸­")
    
    args = parser.parse_args()
    
    if args.compare_models:
        compare_coherence_models(args.version, args.story_file)
    elif args.integrate:
        add_coherence_to_story_evaluation(args.version, args.story_file, args.model)
    else:
        evaluate_story_coherence_from_file(args.version, args.story_file, args.model)