#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æbaselineæ–‡ä»¶çš„å¤šæ ·æ€§
"""

import os
import sys
import json
import re
import pandas as pd
import numpy as np
from pathlib import Path
from advanced_diversity_analyzer import AdvancedDiversityAnalyzer

def analyze_single_baseline(file_path, output_name, tokenizer='simple'):
    """åˆ†æå•ä¸ªbaselineæ–‡ä»¶"""
    print(f"\nåˆ†æ {file_path}...")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•ç»“æ„
    temp_dir = Path(f"temp_baseline_{output_name}")
    temp_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºå­ç›®å½•ï¼ˆæ¨¡æ‹Ÿå‚æ•°é…ç½®ï¼‰
    # ä½¿ç”¨æ ‡å‡†çš„å‘½åæ ¼å¼ï¼Œè®©åˆ†æå™¨èƒ½æ­£ç¡®è§£æ
    sub_dir = temp_dir / f"baseline_{output_name}rewrite_linear_T1.0_s1"
    sub_dir.mkdir(exist_ok=True)
    
    # å¤åˆ¶æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
    import shutil
    target_file = sub_dir / "enhanced_story_dialogue_updated.md"
    shutil.copy2(file_path, target_file)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = f"diversity_results_baseline_{output_name}"
    
    try:
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = AdvancedDiversityAnalyzer(
            data_dir=str(temp_dir),
            output_dir=output_dir,
            window=1000,
            stride=500,
            bleu_sample_every=1,
            tokenizer=tokenizer,
            p_low=5,
            p_high=95,
            alpha_min=0.4,
            alpha_max=0.8
        )
        
        # è¿è¡Œåˆ†æ
        individual_results, group_results = analyzer.run_analysis()
        analyzer.save_results()
        
        print(f"âœ“ {output_name} åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_dir}")
        
        # è¿”å›ç»“æœ
        return individual_results, group_results, output_dir
        
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(temp_dir)

def compare_baselines():
    """æ¯”è¾ƒä¸¤ä¸ªbaselineçš„ç»“æœ"""
    print("=" * 80)
    print("Baselineæ–‡ä»¶å¤šæ ·æ€§åˆ†æ")
    print("=" * 80)
    
    # åˆ†æä¸¤ä¸ªbaselineæ–‡ä»¶
    sci_results, sci_group, sci_dir = analyze_single_baseline(
        "/Users/haha/Story/data/sci_baseline.md", 
        "sci"
    )
    
    normal_results, normal_group, normal_dir = analyze_single_baseline(
        "/Users/haha/Story/data/normal_baseline.md", 
        "normal"
    )
    
    # æå–ç»“æœæ•°æ®
    sci_key = list(sci_results.keys())[0]
    normal_key = list(normal_results.keys())[0]
    
    sci_data = sci_results[sci_key]
    normal_data = normal_results[normal_key]
    
    print("\n" + "=" * 80)
    print("Baselineå¯¹æ¯”åˆ†æç»“æœ")
    print("=" * 80)
    
    print("\nğŸ“Š åŸºæœ¬ç»Ÿè®¡")
    print("-" * 40)
    print(f"ç§‘å¹»baseline: {sci_data['total_words']} è¯, {sci_data['total_sentences']} å¥")
    print(f"ä¼ ç»Ÿbaseline: {normal_data['total_words']} è¯, {normal_data['total_sentences']} å¥")
    
    print("\nğŸ¯ å¤šæ ·æ€§æŒ‡æ ‡å¯¹æ¯”")
    print("-" * 40)
    print(f"ç§‘å¹»baseline:")
    print(f"  distinct_1: {sci_data['distinct_1']:.4f}")
    print(f"  distinct_2: {sci_data['distinct_2']:.4f}")
    print(f"  distinct_avg: {sci_data['distinct_avg']:.4f}")
    
    print(f"ä¼ ç»Ÿbaseline:")
    print(f"  distinct_1: {normal_data['distinct_1']:.4f}")
    print(f"  distinct_2: {normal_data['distinct_2']:.4f}")
    print(f"  distinct_avg: {normal_data['distinct_avg']:.4f}")
    
    print("\nğŸ“ˆ ç›¸å¯¹æ¯”è¾ƒ")
    print("-" * 40)
    
    # è®¡ç®—ç›¸å¯¹å·®å¼‚
    distinct_1_diff = (sci_data['distinct_1'] - normal_data['distinct_1']) / normal_data['distinct_1'] * 100
    distinct_2_diff = (sci_data['distinct_2'] - normal_data['distinct_2']) / normal_data['distinct_2'] * 100
    distinct_avg_diff = (sci_data['distinct_avg'] - normal_data['distinct_avg']) / normal_data['distinct_avg'] * 100
    
    print(f"ç§‘å¹»ç›¸å¯¹ä¼ ç»Ÿbaseline:")
    print(f"  distinct_1: {distinct_1_diff:+.1f}%")
    print(f"  distinct_2: {distinct_2_diff:+.1f}%")
    print(f"  distinct_avg: {distinct_avg_diff:+.1f}%")
    
    # è¯æ±‡å¯†åº¦æ¯”è¾ƒ
    sci_density = sci_data['total_words'] / sci_data['total_sentences']
    normal_density = normal_data['total_words'] / normal_data['total_sentences']
    density_diff = (sci_density - normal_density) / normal_density * 100
    
    print(f"\nè¯æ±‡å¯†åº¦:")
    print(f"  ç§‘å¹»: {sci_density:.1f} è¯/å¥")
    print(f"  ä¼ ç»Ÿ: {normal_density:.1f} è¯/å¥")
    print(f"  å·®å¼‚: {density_diff:+.1f}%")
    
    print("\nğŸ’¡ å…³é”®å‘ç°")
    print("-" * 40)
    
    if sci_data['distinct_avg'] > normal_data['distinct_avg']:
        print(f"â€¢ ç§‘å¹»baselineè¯æ±‡å¤šæ ·æ€§æ›´é«˜ ({sci_data['distinct_avg']:.4f} vs {normal_data['distinct_avg']:.4f})")
    else:
        print(f"â€¢ ä¼ ç»Ÿbaselineè¯æ±‡å¤šæ ·æ€§æ›´é«˜ ({normal_data['distinct_avg']:.4f} vs {sci_data['distinct_avg']:.4f})")
    
    if sci_data['total_words'] > normal_data['total_words']:
        print(f"â€¢ ç§‘å¹»baselineæ›´é•¿ ({sci_data['total_words']} vs {normal_data['total_words']} è¯)")
    else:
        print(f"â€¢ ä¼ ç»Ÿbaselineæ›´é•¿ ({normal_data['total_words']} vs {sci_data['total_words']} è¯)")
    
    # é£æ ¼åˆ†æ
    print(f"\nğŸ¨ é£æ ¼ç‰¹å¾")
    print("-" * 40)
    
    # è¯»å–æ–‡ä»¶å†…å®¹è¿›è¡Œç®€å•çš„é£æ ¼åˆ†æ
    with open("/Users/haha/Story/data/sci_baseline.md", 'r', encoding='utf-8') as f:
        sci_content = f.read().lower()
    
    with open("/Users/haha/Story/data/normal_baseline.md", 'r', encoding='utf-8') as f:
        normal_content = f.read().lower()
    
    # ç§‘å¹»è¯æ±‡
    sci_words = ['station', 'algorithm', 'data', 'network', 'drone', 'bio-signal', 'radiation']
    sci_count = sum(sci_content.count(word) for word in sci_words)
    
    # ä¼ ç»Ÿè¯æ±‡
    traditional_words = ['forest', 'cottage', 'grandmother', 'wolf', 'path', 'village', 'woods']
    trad_count = sum(normal_content.count(word) for word in traditional_words)
    
    print(f"ç§‘å¹»ç‰¹å¾è¯é¢‘: {sci_count}")
    print(f"ä¼ ç»Ÿç‰¹å¾è¯é¢‘: {trad_count}")
    
    print("\nğŸ“ ç»“æœæ–‡ä»¶")
    print("-" * 40)
    print(f"ç§‘å¹»baselineç»“æœ: {sci_dir}/")
    print(f"ä¼ ç»Ÿbaselineç»“æœ: {normal_dir}/")
    
    print("\n" + "=" * 80)
    
    return sci_data, normal_data

def create_baseline_summary():
    """åˆ›å»ºbaselineåˆ†ææ‘˜è¦"""
    print("åˆ›å»ºbaselineåˆ†ææ‘˜è¦...")
    
    # è¿è¡Œåˆ†æ
    sci_data, normal_data = compare_baselines()
    
    # åˆ›å»ºæ‘˜è¦æ•°æ®
    summary = {
        'sci_baseline': {
            'file': 'data/sci_baseline.md',
            'total_words': sci_data['total_words'],
            'total_sentences': sci_data['total_sentences'],
            'distinct_1': sci_data['distinct_1'],
            'distinct_2': sci_data['distinct_2'],
            'distinct_avg': sci_data['distinct_avg'],
            'word_per_sentence': sci_data['total_words'] / sci_data['total_sentences']
        },
        'normal_baseline': {
            'file': 'data/normal_baseline.md',
            'total_words': normal_data['total_words'],
            'total_sentences': normal_data['total_sentences'],
            'distinct_1': normal_data['distinct_1'],
            'distinct_2': normal_data['distinct_2'],
            'distinct_avg': normal_data['distinct_avg'],
            'word_per_sentence': normal_data['total_words'] / normal_data['total_sentences']
        }
    }
    
    # ä¿å­˜æ‘˜è¦
    with open('baseline_analysis_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print("æ‘˜è¦å·²ä¿å­˜åˆ°: baseline_analysis_summary.json")

if __name__ == "__main__":
    create_baseline_summary()
