import os
import argparse
from src.constant import output_dir
from src.utils.utils import save_md, save_json, load_json, extract_plot_list
from src.generation.outline_generator import generate_outline
from src.generation.chapter_reorder import reorder_chapters
from src.generation.generate_characters import generate_characters_v1
from src.generation.expand_story import expand_story_v1
from src.compile_story import compile_full_story_by_sentence
from src.enhance_story import enhance_story_with_transitions, polish_dialogues_in_story
from src.generation.dialogue_inserter import analyze_dialogue_insertions, run_dialogue_insertion, analyze_dialogue_insertions_v2
from src.utils.utils import extract_behavior_llm, convert_dialogue_dict_to_list
from src.sync.plot_sync_manager import sync_plot_and_dialogue_from_behavior
from src.sync.auto_propagate_plot_update import auto_propagate_plot_update
from src.analysis.character_state_tracker import run_character_state_tracker
from src.utils.logger import append_log, build_log_record, build_simple_log, init_log_path
from src.version_namer import build_version_name 
from src.generation.narrative_analyzer import analyze_narrative_structure, enhance_summaries_with_narrative
from src.analysis.performance_analyzer import PerformanceAnalyzer


def ensure_output_dir(version):
    folder = os.path.join(output_dir, version)
    os.makedirs(folder, exist_ok=True)
    return folder

def step_file(version, filename):
    return os.path.join(output_dir, version, filename)

def main( 
    version="test",
    reorder_mode="linear",
    use_cache=False,
    topic="å°çº¢å¸½",
    style="ç§‘å¹»æ”¹å†™",
    behavior_model="gpt-4.1",
    temperature=0.7,
    seed=1,
    generation_mode="traditional",
    user_description=None,
    file_content=None
):
    #  è‡ªåŠ¨æ„å»ºç‰ˆæœ¬åç§°
    if version == "test":
        if generation_mode == "traditional":
            version = build_version_name(
                topic=topic,
                style=style,
                temperature=temperature,
                seed=seed,
                order_mode=reorder_mode
            )
        else:  # description_based æ¨¡å¼
            import hashlib
            import datetime
            # åŸºäºç”¨æˆ·æè¿°ç”Ÿæˆç®€çŸ­æ ‡è¯†
            desc_hash = hashlib.md5(user_description.encode('utf-8')).hexdigest()[:8] if user_description else "nodesc"
            timestamp = datetime.datetime.now().strftime("%m%d_%H%M")
            version = f"custom_{desc_hash}_{reorder_mode}_T{temperature}_s{seed}_{timestamp}"

    print(f"\nå¼€å§‹è¿è¡Œ main_pipeline")
    print(f"ç‰ˆæœ¬: {version}ï¼Œé¡ºåºæ¨¡å¼: {reorder_mode}ï¼Œä½¿ç”¨ç¼“å­˜: {use_cache}")
    if generation_mode == "traditional":
        print(f"ç”Ÿæˆæ¨¡å¼: ä¼ ç»Ÿæ¨¡å¼ï¼Œé¢˜æ: {topic}ï¼Œé£æ ¼: {style}")
    else:
        print(f"ç”Ÿæˆæ¨¡å¼: è‡ªå®šä¹‰æè¿°æ¨¡å¼")
        print(f"ç”¨æˆ·æè¿°: {user_description[:100]}..." if user_description and len(user_description) > 100 else f"ç”¨æˆ·æè¿°: {user_description}")
        print(f"æ–‡ä»¶å†…å®¹: {'å·²æä¾›' if file_content else 'æ— '}")
    print()
    folder = ensure_output_dir(version)
    role_state = {}

    # åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨
    experiment_config = {
        'topic': topic,
        'style': style,
        'temperature': temperature,
        'seed': seed,
        'behavior_model': behavior_model,
        'generation_mode': generation_mode,
        'reorder_mode': reorder_mode,
        'use_cache': use_cache,
        'user_description': user_description,
        'file_content': file_content is not None
    }
    
    performance_analyzer = PerformanceAnalyzer(
        task_name=version, 
        experiment_config=experiment_config
    )
    performance_analyzer.start_total_timing()
    print("æ€§èƒ½åˆ†æå™¨å·²å¯åŠ¨")

    plot_log_path = init_log_path(folder, "plot")
    dialogue_log_path = init_log_path(folder, "dialogue")

    # Step 1: Outline Generation
    performance_analyzer.start_stage("outline_generation", {"topic": topic, "style": style, "generation_mode": generation_mode})
    
    if generation_mode == "traditional":
        # ä¼ ç»Ÿæ¨¡å¼ï¼šä½¿ç”¨å…±äº«çš„ outline ç¼“å­˜
        outline_base_path = os.path.join(output_dir, "reference_outline", f"{topic}_{style}_T{temperature}_s{seed}_outline.json")
        os.makedirs(os.path.dirname(outline_base_path), exist_ok=True)

        if os.path.exists(outline_base_path) and use_cache:
            outline = load_json(outline_base_path)
            performance_analyzer.record_cache_usage("outline_generation", True, outline_base_path)
            print(f"å·²åŠ è½½å…±äº« outlineï¼š{outline_base_path}")
        else:
            performance_analyzer.record_cache_usage("outline_generation", False, outline_base_path)
            outline = generate_outline(
                topic=topic, 
                style=style, 
                custom_instruction="",
                generation_mode="traditional",
                performance_analyzer=performance_analyzer
            )
            save_json(outline, "reference_outline", f"{topic}_{style}_T{temperature}_s{seed}_outline.json")
            print(f"ç”Ÿæˆå¹¶ä¿å­˜å…±äº« outlineï¼š{outline_base_path}")
    else:
        # æè¿°æ¨¡å¼ï¼šæ¯æ¬¡éƒ½é‡æ–°ç”Ÿæˆï¼ˆä¸ä½¿ç”¨å…±äº«ç¼“å­˜ï¼‰
        outline_path = step_file(version, "test_outline.json")
        if os.path.exists(outline_path) and use_cache:
            outline = load_json(outline_path)
            performance_analyzer.record_cache_usage("outline_generation", True, outline_path)
            print(f"å·²åŠ è½½è‡ªå®šä¹‰ outlineï¼š{outline_path}")
        else:
            performance_analyzer.record_cache_usage("outline_generation", False, outline_path)
            outline = generate_outline(
                topic=topic,  # è¿™é‡Œå¯èƒ½ä¸ºç©ºï¼Œä½†generate_outlineä¼šå¤„ç†
                style=style,  # è¿™é‡Œå¯èƒ½ä¸ºç©ºï¼Œä½†generate_outlineä¼šå¤„ç†
                custom_instruction="",
                generation_mode="description_based",
                user_description=user_description,
                file_content=file_content,
                performance_analyzer=performance_analyzer
            )
            save_json(outline, version, "test_outline.json")
            print(f"ç”Ÿæˆè‡ªå®šä¹‰ outline å®Œæˆ")
    
    performance_analyzer.end_stage("outline_generation", outline)


    # Step 2: ç« èŠ‚é¡ºåºå¤„ç†
    performance_analyzer.start_stage("chapter_reorder", {"reorder_mode": reorder_mode, "chapters": len(outline)})
    
    if reorder_mode == "linear":
        reorder_outline_raw = outline
        save_json(outline, version, "test_outline.json")
        print("ä½¿ç”¨ linear é¡ºåºï¼ˆç›´æ¥æ¥è‡ª outlineï¼‰")


    elif reorder_mode == "nonlinear":
        save_json(outline, version, "test_outline_linear.json")
        reorder_path = os.path.join(output_dir, "reference_reorder", f"{topic}_{style}_T{temperature}_s{seed}_nonlinear.json")
        os.makedirs(os.path.dirname(reorder_path), exist_ok=True)

        if os.path.exists(reorder_path):
            reorder_outline_raw = load_json(reorder_path)
            print(f"å·²åŠ è½½ cached nonlinear é¡ºåºï¼š{reorder_path}")
        else:
            # Step 2.1: ç« èŠ‚é‡æ’
            reorder_outline_raw = reorder_chapters(outline, mode="nonlinear", performance_analyzer=performance_analyzer)
            
            # æ£€æŸ¥é‡æ’æ˜¯å¦æˆåŠŸ
            if not any("new_order" in ch for ch in reorder_outline_raw):
                print("LLM é‡æ’å¤±è´¥ï¼šæœªæ£€æµ‹åˆ°ä»»ä½• new_order å­—æ®µï¼Œå›é€€ä¸ºåŸå§‹é¡ºåº")
                reorder_mode = "linear"  # å›é€€åˆ°çº¿æ€§æ¨¡å¼
                reorder_outline_raw = outline
            else:
                print("reorder_chapters æˆåŠŸç”Ÿæˆéçº¿æ€§é¡ºåº")
                
                # Step 2.2: å™è¿°ç»“æ„åˆ†æ
                print("å¼€å§‹å™è¿°ç»“æ„åˆ†æ...")
                reorder_outline_raw = analyze_narrative_structure(
                    reorder_outline_raw, outline, topic=topic, style=style, performance_analyzer=performance_analyzer
                )
                
                # æ˜¾ç¤ºåˆ†æç»“æœ
                print("ğŸ“– å™è¿°ç»“æ„åˆ†æç»“æœï¼š")
                for ch in reorder_outline_raw:
                    role = ch.get('narrative_role', 'æœªåˆ†æ')
                    print(f"  {ch['chapter_id']}: {role}")

            save_json(reorder_outline_raw, "reference_reorder", f"{topic}_{style}_T{temperature}_s{seed}_nonlinear.json")
            print(f"ç”Ÿæˆ nonlinear é¡ºåºå¹¶ç¼“å­˜ï¼š{reorder_path}")

        # æ·»åŠ æ—¥å¿—è®°å½•
        reorder_log_path = init_log_path(folder, "reorder")
        reorder_log = build_simple_log(
            module="chapter_reorder_with_narrative",
            task_name=version,
            input_data={"outline": outline, "reorder_mode": reorder_mode},
            output_data={"reorder_result": reorder_outline_raw, "narrative_mode": reorder_mode}
        )
        append_log(reorder_log_path, reorder_log)

    # ç»Ÿä¸€ç»“æ„ï¼šè¡¥å…¨ summary å­—æ®µï¼Œä¿ç•™å™è¿°åˆ†æå­—æ®µ
    reorder_outline = []
    for reordered_ch in reorder_outline_raw:
        match = next((x for x in outline if x["chapter_id"] == reordered_ch["chapter_id"]), None)
        if match:
            merged = {
                "chapter_id": reordered_ch["chapter_id"],
                "title": reordered_ch["title"],
                "summary": match.get("summary", "")
            }
            
            # ä¿ç•™é‡æ’å’Œå™è¿°åˆ†æç›¸å…³å­—æ®µ
            narrative_fields = ["new_order", "narrative_role", "narrative_instruction", "transition_hint"]
            for field in narrative_fields:
                if field in reordered_ch:
                    merged[field] = reordered_ch[field]
            
            reorder_outline.append(merged)

    save_json(reorder_outline, version, "test_reorder_outline.json")

    # æ˜¾ç¤ºæœ€ç»ˆç»“æ„
    if reorder_mode == "nonlinear":
        print("ç« èŠ‚é¡ºåºå¤„ç†å®Œæˆï¼ˆå·²ä¿ç•™ summary å’Œå™è¿°æŒ‡å¯¼ï¼‰")
        print("ğŸ­ æœ€ç»ˆç« èŠ‚ç»“æ„ï¼š")
        for idx, ch in enumerate(reorder_outline):
            role = ch.get('narrative_role', 'çº¿æ€§å™è¿°')
            orig_pos = next((i+1 for i, x in enumerate(outline) if x["chapter_id"] == ch["chapter_id"]), "?")
            print(f"  {idx+1}. {ch['chapter_id']} (åŸç¬¬{orig_pos}ç« ) - {role}")
    else:
        print("ç« èŠ‚é¡ºåºå¤„ç†å®Œæˆï¼ˆå·²ä¿ç•™ summaryï¼‰")
    
    performance_analyzer.end_stage("chapter_reorder", reorder_outline)

    # Step 3: Character Generation
    performance_analyzer.start_stage("character_generation", {"chapters": len(reorder_outline)})
    
    character_path = step_file(version, "characters.json")
    if use_cache and os.path.exists(character_path):
        characters = load_json(character_path)
        performance_analyzer.record_cache_usage("character_generation", True, character_path)
        print("å·²åŠ è½½è§’è‰²è®¾å®š")
    else:
        performance_analyzer.record_cache_usage("character_generation", False, character_path)
        characters = generate_characters_v1(reorder_outline, performance_analyzer=performance_analyzer)
        save_json(characters, version, "characters.json")
        print("ç”Ÿæˆè§’è‰²è®¾å®šå®Œæˆ")
    
    performance_analyzer.end_stage("character_generation", characters)

    # Step 4: Story Expansion
    performance_analyzer.start_stage("story_expansion", {"chapters": len(reorder_outline), "characters": len(characters)})
    
    plot_path = step_file(version, "story.json")
    if use_cache and os.path.exists(plot_path):
        story = load_json(plot_path)
        performance_analyzer.record_cache_usage("story_expansion", True, plot_path)
        print("å·²åŠ è½½æ•…äº‹å†…å®¹")
    else:
        performance_analyzer.record_cache_usage("story_expansion", False, plot_path)
        story = expand_story_v1(reorder_outline, characters, custom_instruction="", performance_analyzer=performance_analyzer)
        for idx, ch in enumerate(story):
            ch.setdefault("chapter_id", reorder_outline[idx]["chapter_id"])
            ch.setdefault("title", reorder_outline[idx]["title"])

            log = build_log_record(
                module="expand_story", step="plot",
                task_name=version, chapter_id=ch["chapter_id"],
                model=behavior_model, 
                input_data={"outline": reorder_outline[idx]},
                output_data={"plot": ch["plot"]},
                temperature=temperature, 
                seed=seed
            )
            append_log(plot_log_path, log)

        save_json(story, version, "story.json")
        print("æ•…äº‹å†…å®¹ç”Ÿæˆå®Œæˆ")
    
    performance_analyzer.end_stage("story_expansion", story)

    # Step 5 & 6: æ–°ç‰ˆå¯¹è¯ç”Ÿæˆï¼ˆå¥å­çº§åˆ†æ + ç« èŠ‚çº§å…¼å®¹ï¼‰
    performance_analyzer.start_stage("dialogue_generation", {"story_chapters": len(story), "characters": len(characters)})
    
    chapter_results, sentence_results, behavior_timeline = analyze_dialogue_insertions_v2(story, characters, performance_analyzer=performance_analyzer)

    # ä¿å­˜ä¸‰ç§æ ¼å¼çš„æ•°æ®
    save_json(chapter_results, version, "dialogue_marks.json")        # å…¼å®¹æ ¼å¼
    save_json(sentence_results, version, "sentence_dialogues.json")    # å¥å­çº§è¯¦ç»†åˆ†æ
    save_json(behavior_timeline, version, "behavior_timeline_raw.json")  # åŸå§‹behavioræ•°æ®

    # å¦‚æœéœ€è¦syncï¼Œç”¨ç« èŠ‚çº§
    if len(story) == len(chapter_results):
        story, chapter_results_updated, revision_log = sync_plot_and_dialogue_from_behavior(
            story, chapter_results, characters, model=behavior_model)
    else:
        chapter_results_updated = chapter_results  # ğŸ”´ æ·»åŠ è¿™è¡Œ
        revision_log = []

    # è®°å½•æ—¥å¿—ï¼ˆç”¨ç« èŠ‚çº§ä¿æŒå…¼å®¹ï¼‰
    for ch, dlg in zip(story, chapter_results_updated):  # ğŸ”´ ä½¿ç”¨ chapter_results_updated
        log = build_log_record(
            module="dialogue_inserter", step="dialogue",
            task_name=version, chapter_id=ch["chapter_id"],
            model=behavior_model,
            input_data={"plot": ch["plot"]},
            output_data={"dialogue": dlg["dialogue"]},
            temperature=temperature, seed=seed
        )
        append_log(dialogue_log_path, log)
 

    # Step 6.5: æ–°ç‰ˆbehaviorä¿å­˜ï¼ˆå·²åœ¨v2ä¸­æå–ï¼‰
    # ç»„ç»‡è§’è‰²å¼§çº¿
    character_arcs = {}
    for item in behavior_timeline:
        char = item["character"]
        if char not in character_arcs:
            character_arcs[char] = []
        character_arcs[char].append({
            "chapter": item["chapter_id"],
            "sentence": item["sentence_index"],
            "behavior": item["behavior"],
            "scene": item["scene_context"][:30] + "..." if len(item["scene_context"]) > 30 else item["scene_context"]
        })

    # ç”Ÿæˆå®Œæ•´çš„behavior_trace
    behavior_trace = {
        "timeline": behavior_timeline,
        "character_arcs": character_arcs,
        "statistics": {
            "total_dialogue_moments": len(behavior_timeline),
            "characters_behavior_count": {char: len(arcs) for char, arcs in character_arcs.items()}
        },
        "legacy_behaviors": [f"{item['character']}ï¼š{item['behavior']}" for item in behavior_timeline]
    }

    save_json(behavior_trace, version, "behavior_trace.json")

    # å…¼å®¹role_state
    role_state = {}
    for item in behavior_timeline:
        role = item["character"]
        behavior = item["behavior"]
        role_state.setdefault(role, [])
        if behavior not in role_state[role]:
            role_state[role].append(behavior)

    print("æ–°ç‰ˆbehavior traceç”Ÿæˆå®Œæˆ")
    
    performance_analyzer.end_stage("dialogue_generation", {
        "chapter_results": len(chapter_results_updated),
        "sentence_results": len(sentence_results),
        "behavior_timeline": len(behavior_timeline)
    })


    # Step 7: ä¿å­˜è¾“å‡º
    # save_json(role_state, version, "role_state.json")
    save_json(story, version, "story_updated.json")
    save_json(sentence_results, version, "dialogue_updated.json") 
    save_json(revision_log, version, "revision_log.json")

    compiled_updated = compile_full_story_by_sentence(story, sentence_results)
    save_md(compiled_updated, os.path.join(folder, "novel_story.md")) 
    print("novel_story.md å·²ç”Ÿæˆ")

    # Step 8: Enhancement
    performance_analyzer.start_stage("story_enhancement", {"compiled_story_length": len(compiled_updated)})
    
    enhance_story_with_transitions(task_name=version, input_story_file="story_updated.json")
    polish_dialogues_in_story(task_name=version, input_dialogue_file="dialogue_updated.json")
    print("å¢å¼ºç‰ˆæœ¬å·²å®Œæˆ")
    
    performance_analyzer.end_stage("story_enhancement", {"enhanced": True})

    def generate_role_state_from_behavior_trace(behavior_trace_data):
        """ä»behavior_traceç›´æ¥ç”Ÿæˆrole_stateï¼Œé¿å…é‡å¤LLMè°ƒç”¨å’Œé”™è¯¯æ˜ å°„"""
        from collections import defaultdict
        
        timeline = behavior_trace_data.get("timeline", [])
        role_state_by_chapter = defaultdict(lambda: defaultdict(set))
        
        for item in timeline:
            chapter_id = item["chapter_id"]
            character = item["character"]
            behavior = item["behavior"]
            role_state_by_chapter[chapter_id][character].add(behavior)
        
        # è½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼
        result = {}
        for chapter_id in sorted(role_state_by_chapter.keys()):
            result[chapter_id] = {}
            for character, behaviors in role_state_by_chapter[chapter_id].items():
                result[chapter_id][character] = sorted(list(behaviors))
        
        return result

    # ä»å·²æœ‰çš„æ­£ç¡®æ•°æ®ç”Ÿæˆ role_state
    correct_role_state = generate_role_state_from_behavior_trace(behavior_trace)
    save_json(correct_role_state, version, "role_state.json")
    print(f"ä» behavior_trace ç”Ÿæˆæ­£ç¡®çš„ role_state.json ({len(correct_role_state)} ä¸ªç« èŠ‚)")
    # run_character_state_tracker(version=version, dialogue_file="dialogue_updated.json", model=behavior_model)
    
    # ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š
    print("\næ­£åœ¨ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š...")
    performance_analyzer.analyze_text_features(story, sentence_results, characters)
    
    # è®¡ç®—å†…å­˜vsè§’è‰²æ•°é‡çš„å…³ç³»
    memory_per_character = performance_analyzer.calculate_memory_per_character(characters)
    if memory_per_character:
        print(f"å†…å­˜æ•ˆç‡: {memory_per_character.get('peak_memory_per_character', 0):.2f} MB/è§’è‰²")
    
    performance_report_path = performance_analyzer.save_report(folder, f"performance_analysis_{version}.json")
    
    # æ˜¾ç¤ºæ€§èƒ½æ‘˜è¦
    complexity_metrics = performance_analyzer.calculate_complexity_metrics()
    total_time = performance_analyzer.get_total_time()
    
    print(f"\næ€§èƒ½æ‘˜è¦:")
    print(f"   æ€»æ‰§è¡Œæ—¶é—´: {performance_analyzer._format_duration(total_time)}")
    print(f"   ç”Ÿæˆæ–‡æœ¬: {performance_analyzer.text_features.get('total_word_count', 0)} å­—")
    print(f"   ç”Ÿæˆæ•ˆç‡: {complexity_metrics.get('efficiency_metrics', {}).get('words_per_second', 0):.2f} å­—/ç§’")
    print(f"   å³°å€¼å†…å­˜ä½¿ç”¨: {performance_analyzer.peak_memory_usage:.2f} MB")
    print(f"   æ€»APIæˆæœ¬: ${performance_analyzer.total_api_cost:.4f}")
    print(f"   æ€»Tokenæ¶ˆè€—: {performance_analyzer.total_tokens:,}")
    print(f"   æ—¶é—´å¤æ‚åº¦ä¼°ç®—: {performance_analyzer._estimate_complexity_class()}")
    
    # æ˜¾ç¤ºå„é˜¶æ®µè€—æ—¶
    print(f"\nå„é˜¶æ®µè€—æ—¶:")
    stage_percentages = performance_analyzer._calculate_stage_percentages()
    for stage, duration in performance_analyzer.stage_times.items():
        percentage = stage_percentages.get(stage, 0)
        # æ˜¾ç¤ºè¯¥é˜¶æ®µçš„å†…å­˜å¢é•¿
        memory_increase = performance_analyzer.stage_memory_usage.get(f"{stage}_increase", 0)
        memory_info = f", å†…å­˜+{memory_increase:.2f}MB" if memory_increase > 0 else ""
        print(f"   {stage}: {performance_analyzer._format_duration(duration)} ({percentage:.1f}%){memory_info}")
    
    # æ˜¾ç¤ºAPIæˆæœ¬åˆ†è§£
    if performance_analyzer.token_consumption:
        print(f"\nAPIæˆæœ¬åˆ†è§£:")
        for stage, tokens in performance_analyzer.token_consumption.items():
            print(f"   {stage}: ${tokens['total_cost']:.4f} ({tokens['total_tokens']:,} tokens, {tokens['api_calls']} æ¬¡è°ƒç”¨)")
    
    print(f"\nå…¨éƒ¨æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼ç»“æœä¿å­˜åœ¨ï¼š{folder}")
    print(f"æ€§èƒ½åˆ†ææŠ¥å‘Š: {performance_report_path}\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--version", type=str, default="test", help="ç‰ˆæœ¬åç§°")
    parser.add_argument("--reorder", type=str, default="linear", choices=["linear", "nonlinear"], help="ç« èŠ‚é¡ºåºæ§åˆ¶")
    parser.add_argument("--no-cache", action="store_true", help="æ˜¯å¦å¿½ç•¥å·²æœ‰ç»“æœ")
    parser.add_argument("--topic", type=str, default="å°çº¢å¸½", help="æ•…äº‹é¢˜æ")
    parser.add_argument("--style", type=str, default="ç§‘å¹»æ”¹å†™", help="æ•…äº‹é£æ ¼")
    parser.add_argument("--behavior-model", type=str, default="gpt-4.1", help="è¡Œä¸ºè¯†åˆ«æ¨¡å‹")
    parser.add_argument("--temperature", type=float, default=0.7, help="LLM temperature")
    parser.add_argument("--seed", type=int, default=42, help="ç”Ÿæˆéšæœºç§å­")
    
    # æ–°å¢å‚æ•°ï¼šæ”¯æŒæè¿°æ¨¡å¼
    parser.add_argument("--generation-mode", type=str, default="traditional", 
                       choices=["traditional", "description_based"], help="ç”Ÿæˆæ¨¡å¼ï¼štraditionalæˆ–description_based")
    parser.add_argument("--user-description", type=str, default=None, help="ç”¨æˆ·æ•…äº‹æè¿°ï¼ˆdescription_basedæ¨¡å¼ä½¿ç”¨ï¼‰")
    parser.add_argument("--file-content", type=str, default=None, help="å‚è€ƒæ–‡ä»¶å†…å®¹ï¼ˆå¯é€‰ï¼‰")

    args = parser.parse_args()

    main(
        version=args.version,
        reorder_mode=args.reorder,
        use_cache=not args.no_cache,
        topic=args.topic,
        style=args.style,
        behavior_model=args.behavior_model,
        temperature=args.temperature,
        seed=args.seed,
        generation_mode=args.generation_mode,
        user_description=args.user_description,
        file_content=args.file_content
    )

