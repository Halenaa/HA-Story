#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¨³å¥ç»„åˆ«æ•ˆåº”åˆ†æ - ä½¿ç”¨Cluster-Robustæ ‡å‡†è¯¯
Robust Group Effects Analysis - Using Cluster-Robust Standard Errors

ç”±äºæ··åˆæ•ˆåº”æ¨¡å‹æ”¶æ•›é—®é¢˜ï¼Œæ”¹ç”¨OLS + cluster-robustæ ‡å‡†è¯¯è¿›è¡Œåˆ†æ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ä¿®æ­£å‡½æ•°
import fix_analysis_functions as fix

def robust_group_effects_ols(long_table_df):
    """
    ä½¿ç”¨OLS + cluster-robustæ ‡å‡†è¯¯åˆ†æç»„åˆ«æ•ˆåº”
    """
    try:
        import statsmodels.api as sm
        from statsmodels.formula.api import ols
    except ImportError:
        print("âŒ statsmodelsæœªå®‰è£…")
        return None
    
    print("ğŸ”§ ç¨³å¥ç»„åˆ«æ•ˆåº”åˆ†æ (OLS + Cluster-Robust SE)")
    print("-" * 60)
    
    # 1. æ•°æ®å‡†å¤‡
    rater_groups = fix.identify_group_membership(long_table_df)
    
    model_data = long_table_df[long_table_df['dimension'] == 'Overall Quality'].copy()
    model_data['group_type'] = model_data['rater_id'].map(
        lambda x: rater_groups.get(x, {}).get('group_type', 'unknown')
    )
    
    # æ ‡å‡†åŒ–è¯„åˆ† (within-rater)
    model_data['score_z'] = model_data.groupby('rater_id')['score'].transform(
        lambda x: (x - x.mean()) / x.std() if x.std() > 0 else x - x.mean()
    )
    
    # è¿‡æ»¤å®éªŒé…ç½®
    exp_data = model_data[model_data['config'] != 'baseline'].copy()
    
    print(f"ğŸ“Š æ•°æ®æ¦‚å†µ:")
    print(f"   è§‚å¯Ÿæ•°: {len(exp_data)}")
    print(f"   å‚ä¸è€…: {exp_data['rater_id'].nunique()}")
    print(f"   æ·±åº¦è¯„ä»·: {len(exp_data[exp_data['group_type']=='deep'])}")
    print(f"   æµ…åº¦è¯„ä»·: {len(exp_data[exp_data['group_type']=='shallow'])}")
    
    results = {}
    
    # 2. æ¨¡å‹1: ä»…configæ•ˆåº”
    print(f"\nğŸ”§ æ¨¡å‹1: Configæ•ˆåº”æ¨¡å‹")
    try:
        model1 = ols('score_z ~ C(config)', data=exp_data).fit(cov_type='cluster', 
                                                               cov_kwds={'groups': exp_data['rater_id']})
        
        print(f"   RÂ²: {model1.rsquared:.4f}")
        print(f"   Fç»Ÿè®¡é‡: {model1.fvalue:.3f} (p={model1.f_pvalue:.4f})")
        
        results['model1'] = model1
        
    except Exception as e:
        print(f"   âŒ æ¨¡å‹1æ‹Ÿåˆå¤±è´¥: {e}")
        results['model1'] = None
    
    # 3. æ¨¡å‹2: Config + ç»„åˆ«æ•ˆåº”
    print(f"\nğŸ”§ æ¨¡å‹2: Config + ç»„åˆ«æ•ˆåº”æ¨¡å‹")
    try:
        model2 = ols('score_z ~ C(config) + C(group_type)', data=exp_data).fit(cov_type='cluster',
                                                                               cov_kwds={'groups': exp_data['rater_id']})
        
        print(f"   RÂ²: {model2.rsquared:.4f}")
        print(f"   Fç»Ÿè®¡é‡: {model2.fvalue:.3f} (p={model2.f_pvalue:.4f})")
        
        # æ£€éªŒç»„åˆ«æ•ˆåº”æ˜¯å¦æ˜¾è‘—
        group_coef = model2.params.get('C(group_type)[T.shallow]', None)
        if group_coef is not None:
            group_p = model2.pvalues.get('C(group_type)[T.shallow]', 1.0)
            print(f"   ç»„åˆ«æ•ˆåº”: {group_coef:+.4f} (p={group_p:.4f})")
        
        results['model2'] = model2
        
    except Exception as e:
        print(f"   âŒ æ¨¡å‹2æ‹Ÿåˆå¤±è´¥: {e}")
        results['model2'] = None
    
    # 4. æ¨¡å‹3: Config Ã— ç»„åˆ«äº¤äº’
    print(f"\nğŸ”§ æ¨¡å‹3: Config Ã— ç»„åˆ«äº¤äº’æ¨¡å‹")
    try:
        model3 = ols('score_z ~ C(config) * C(group_type)', data=exp_data).fit(cov_type='cluster',
                                                                               cov_kwds={'groups': exp_data['rater_id']})
        
        print(f"   RÂ²: {model3.rsquared:.4f}")
        print(f"   Fç»Ÿè®¡é‡: {model3.fvalue:.3f} (p={model3.f_pvalue:.4f})")
        
        results['model3'] = model3
        
    except Exception as e:
        print(f"   âŒ æ¨¡å‹3æ‹Ÿåˆå¤±è´¥: {e}")
        results['model3'] = None
    
    # 5. æ¨¡å‹æ¯”è¾ƒ
    valid_models = {name: model for name, model in results.items() if model is not None}
    
    if len(valid_models) > 1:
        print(f"\nğŸ“Š æ¨¡å‹æ¯”è¾ƒ:")
        print("æ¨¡å‹\\t\\t\\tRÂ²\\t\\tAIC\\t\\tFç»Ÿè®¡é‡\\tpå€¼")
        print("-" * 60)
        
        best_r2 = 0
        best_model_name = None
        
        for name, model in valid_models.items():
            print(f"{name:<15}\\t{model.rsquared:.4f}\\t\\t{model.aic:.1f}\\t\\t{model.fvalue:.3f}\\t\\t{model.f_pvalue:.4f}")
            
            if model.rsquared > best_r2:
                best_r2 = model.rsquared
                best_model_name = name
        
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} (RÂ² = {best_r2:.4f})")
        best_model = valid_models[best_model_name]
        
    else:
        best_model = list(valid_models.values())[0] if valid_models else None
        best_model_name = list(valid_models.keys())[0] if valid_models else "æ— "
    
    # 6. è¯¦ç»†åˆ†ææœ€ä½³æ¨¡å‹
    if best_model is not None:
        print(f"\nğŸ“‹ {best_model_name} è¯¦ç»†ç»“æœ (Cluster-Robust SE):")
        print("-" * 60)
        
        print("ç³»æ•°\\t\\t\\t\\t\\tä¼°è®¡å€¼\\t\\tæ ‡å‡†è¯¯\\t\\ttå€¼\\t\\tPå€¼\\t\\tæ˜¾è‘—æ€§")
        print("-" * 90)
        
        for param, coef in best_model.params.items():
            se = best_model.bse[param]
            t_val = best_model.tvalues[param]
            p_val = best_model.pvalues[param]
            
            # æ˜¾è‘—æ€§æ ‡è®°
            if p_val < 0.001:
                sig = "***"
            elif p_val < 0.01:
                sig = "**"
            elif p_val < 0.05:
                sig = "*"
            elif p_val < 0.1:
                sig = "."
            else:
                sig = ""
            
            print(f"{param:<25}\\t\\t{coef:8.4f}\\t{se:8.4f}\\t{t_val:8.3f}\\t{p_val:8.4f}\\t\\t{sig}")
    
    return results, exp_data

def detailed_group_comparison(long_table_df):
    """
    è¯¦ç»†çš„ç»„åˆ«æ¯”è¾ƒåˆ†æ
    """
    print(f"\nğŸ” è¯¦ç»†ç»„åˆ«æ¯”è¾ƒåˆ†æ")
    print("-" * 60)
    
    rater_groups = fix.identify_group_membership(long_table_df)
    
    overall_data = long_table_df[long_table_df['dimension'] == 'Overall Quality'].copy()
    overall_data['group_type'] = overall_data['rater_id'].map(
        lambda x: rater_groups.get(x, {}).get('group_type', 'unknown')
    )
    
    # 1. æè¿°æ€§ç»Ÿè®¡
    print("ğŸ“Š å„ç»„åˆ«è¯„åˆ†æè¿°ç»Ÿè®¡:")
    group_desc = overall_data.groupby('group_type')['score'].agg(['count', 'mean', 'std', 'min', 'max']).round(3)
    print(group_desc)
    
    # 2. é…ç½®çº§åˆ«çš„ç»„åˆ«å·®å¼‚
    print(f"\nğŸ“Š å„é…ç½®çš„ç»„åˆ«å·®å¼‚:")
    print("Config\\t\\t\\tæ·±åº¦ç»„\\t\\tæµ…åº¦ç»„\\t\\tå·®å€¼\\t\\tCohen's d")
    print("-" * 70)
    
    comparison_results = []
    
    for config in overall_data['config'].unique():
        config_data = overall_data[overall_data['config'] == config]
        
        deep_scores = config_data[config_data['group_type'] == 'deep']['score']
        shallow_scores = config_data[config_data['group_type'] == 'shallow']['score']
        
        if len(deep_scores) > 0 and len(shallow_scores) > 0:
            deep_mean = deep_scores.mean()
            shallow_mean = shallow_scores.mean()
            diff = shallow_mean - deep_mean
            
            # Cohen's d
            pooled_std = np.sqrt(((len(deep_scores)-1)*deep_scores.var() + 
                                 (len(shallow_scores)-1)*shallow_scores.var()) / 
                                (len(deep_scores)+len(shallow_scores)-2))
            cohens_d = (shallow_mean - deep_mean) / pooled_std if pooled_std > 0 else 0
            
            print(f"{config:<15}\\t\\t{deep_mean:.2f}Â±{deep_scores.std():.2f}\\t{shallow_mean:.2f}Â±{shallow_scores.std():.2f}\\t{diff:+.2f}\\t\\t{cohens_d:+.3f}")
            
            comparison_results.append({
                'config': config,
                'deep_mean': deep_mean,
                'deep_std': deep_scores.std(),
                'deep_n': len(deep_scores),
                'shallow_mean': shallow_mean,
                'shallow_std': shallow_scores.std(),
                'shallow_n': len(shallow_scores),
                'difference': diff,
                'cohens_d': cohens_d
            })
        elif len(deep_scores) > 0:
            print(f"{config:<15}\\t\\t{deep_scores.mean():.2f}Â±{deep_scores.std():.2f}\\t   N/A\\t\\t   N/A\\t\\t   N/A")
    
    # 3. æ€»ä½“ç»„åˆ«æ•ˆåº”çš„å‡è®¾æ£€éªŒ
    print(f"\nğŸ§ª æ€»ä½“ç»„åˆ«æ•ˆåº”æ£€éªŒ:")
    
    exp_data = overall_data[overall_data['config'] != 'baseline']
    deep_all = exp_data[exp_data['group_type'] == 'deep']['score']
    shallow_all = exp_data[exp_data['group_type'] == 'shallow']['score']
    
    if len(deep_all) > 0 and len(shallow_all) > 0:
        from scipy.stats import mannwhitneyu, ttest_ind, levene
        
        # Leveneæ–¹å·®é½æ€§æ£€éªŒ
        levene_stat, levene_p = levene(deep_all, shallow_all)
        print(f"   Leveneæ–¹å·®é½æ€§æ£€éªŒ: F={levene_stat:.3f}, p={levene_p:.4f}")
        
        # é€‰æ‹©åˆé€‚çš„tæ£€éªŒ
        equal_var = levene_p > 0.05
        t_stat, t_p = ttest_ind(deep_all, shallow_all, equal_var=equal_var)
        test_type = "ç­‰æ–¹å·®" if equal_var else "ä¸ç­‰æ–¹å·®"
        print(f"   ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ({test_type}): t={t_stat:.3f}, p={t_p:.4f}")
        
        # éå‚æ•°æ£€éªŒ
        u_stat, u_p = mannwhitneyu(deep_all, shallow_all, alternative='two-sided')
        print(f"   Mann-Whitney Uæ£€éªŒ: U={u_stat:.2f}, p={u_p:.4f}")
        
        # æ•ˆåº”é‡
        pooled_std = np.sqrt(((len(deep_all)-1)*deep_all.var() + 
                             (len(shallow_all)-1)*shallow_all.var()) / 
                            (len(deep_all)+len(shallow_all)-2))
        overall_cohens_d = (shallow_all.mean() - deep_all.mean()) / pooled_std
        
        # ç½®ä¿¡åŒºé—´ (åŸºäºtåˆ†å¸ƒ)
        se_diff = pooled_std * np.sqrt(1/len(deep_all) + 1/len(shallow_all))
        df = len(deep_all) + len(shallow_all) - 2
        from scipy.stats import t
        t_critical = t.ppf(0.975, df)
        ci_lower = (shallow_all.mean() - deep_all.mean()) - t_critical * se_diff
        ci_upper = (shallow_all.mean() - deep_all.mean()) + t_critical * se_diff
        
        print(f"   æ•´ä½“æ•ˆåº”é‡(Cohen's d): {overall_cohens_d:+.3f}")
        print(f"   å‡å€¼å·®å¼‚95%CI: [{ci_lower:+.3f}, {ci_upper:+.3f}]")
        
        # è§£é‡Šæ•ˆåº”
        if abs(overall_cohens_d) < 0.2:
            effect_interpretation = "å¾®å°æ•ˆåº”"
        elif abs(overall_cohens_d) < 0.5:
            effect_interpretation = "å°æ•ˆåº”"
        elif abs(overall_cohens_d) < 0.8:
            effect_interpretation = "ä¸­ç­‰æ•ˆåº”"
        else:
            effect_interpretation = "å¤§æ•ˆåº”"
        
        direction = "æµ…åº¦ç»„è¯„åˆ†æ›´é«˜" if overall_cohens_d > 0 else "æ·±åº¦ç»„è¯„åˆ†æ›´é«˜"
        print(f"   â†’ {direction}ï¼Œ{effect_interpretation}")
    
    return pd.DataFrame(comparison_results)

def create_group_effects_visualization(group_comparison, exp_data):
    """
    åˆ›å»ºç»„åˆ«æ•ˆåº”å¯è§†åŒ–
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    print(f"\nğŸ“Š åˆ›å»ºç»„åˆ«æ•ˆåº”å¯è§†åŒ–")
    
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. ç»„åˆ«å·®å¼‚ç®±çº¿å›¾
    sns.boxplot(data=exp_data, x='config', y='score', hue='group_type', ax=axes[0,0])
    axes[0,0].set_title('å„é…ç½®çš„ç»„åˆ«è¯„åˆ†å·®å¼‚')
    axes[0,0].set_xlabel('é…ç½®')
    axes[0,0].set_ylabel('è¯„åˆ†')
    axes[0,0].tick_params(axis='x', rotation=45)
    axes[0,0].legend(title='ç»„åˆ«ç±»å‹')
    
    # 2. æ•ˆåº”é‡çƒ­å›¾
    if len(group_comparison) > 0:
        pivot_data = group_comparison.pivot_table(index=['config'], values=['cohens_d'], aggfunc='first')
        pivot_data = pivot_data.fillna(0)
        
        sns.heatmap(pivot_data, annot=True, cmap='RdBu_r', center=0, ax=axes[0,1],
                    cbar_kws={'label': "Cohen's d"})
        axes[0,1].set_title('ç»„åˆ«æ•ˆåº”é‡çƒ­å›¾')
        axes[0,1].set_xlabel('')
    
    # 3. å‡å€¼å·®å¼‚æ•£ç‚¹å›¾
    if len(group_comparison) > 0:
        axes[1,0].scatter(group_comparison['deep_mean'], group_comparison['shallow_mean'], 
                         s=100, alpha=0.7)
        
        # æ·»åŠ é…ç½®æ ‡ç­¾
        for _, row in group_comparison.iterrows():
            axes[1,0].annotate(row['config'], 
                              (row['deep_mean'], row['shallow_mean']),
                              xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        # æ·»åŠ å¯¹è§’çº¿
        min_val = min(group_comparison['deep_mean'].min(), group_comparison['shallow_mean'].min())
        max_val = max(group_comparison['deep_mean'].max(), group_comparison['shallow_mean'].max())
        axes[1,0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5, label='ç›¸ç­‰çº¿')
        
        axes[1,0].set_xlabel('æ·±åº¦ç»„å‡å€¼')
        axes[1,0].set_ylabel('æµ…åº¦ç»„å‡å€¼')
        axes[1,0].set_title('æ·±åº¦vsæµ…åº¦ç»„å‡å€¼æ¯”è¾ƒ')
        axes[1,0].legend()
    
    # 4. ç»„åˆ«åˆ†å¸ƒç›´æ–¹å›¾
    deep_scores = exp_data[exp_data['group_type'] == 'deep']['score']
    shallow_scores = exp_data[exp_data['group_type'] == 'shallow']['score']
    
    axes[1,1].hist(deep_scores, alpha=0.6, label=f'æ·±åº¦ç»„ (n={len(deep_scores)})', bins=15, color='blue')
    axes[1,1].hist(shallow_scores, alpha=0.6, label=f'æµ…åº¦ç»„ (n={len(shallow_scores)})', bins=8, color='red')
    
    axes[1,1].axvline(deep_scores.mean(), color='blue', linestyle='--', alpha=0.8, 
                     label=f'æ·±åº¦ç»„å‡å€¼: {deep_scores.mean():.2f}')
    axes[1,1].axvline(shallow_scores.mean(), color='red', linestyle='--', alpha=0.8,
                     label=f'æµ…åº¦ç»„å‡å€¼: {shallow_scores.mean():.2f}')
    
    axes[1,1].set_xlabel('è¯„åˆ†')
    axes[1,1].set_ylabel('é¢‘æ¬¡')
    axes[1,1].set_title('ç»„åˆ«è¯„åˆ†åˆ†å¸ƒ')
    axes[1,1].legend()
    
    plt.tight_layout()
    plt.savefig('group_effects_analysis.png', dpi=300, bbox_inches='tight')
    print("âœ… ç»„åˆ«æ•ˆåº”å›¾è¡¨å·²ä¿å­˜: group_effects_analysis.png")
    plt.show()

def export_group_analysis_results(model_results, group_comparison):
    """å¯¼å‡ºç»„åˆ«åˆ†æç»“æœ"""
    output_dir = Path('data/processed_corrected')
    output_dir.mkdir(exist_ok=True)
    
    try:
        # å¯¼å‡ºç»„åˆ«æ¯”è¾ƒç»“æœ
        if group_comparison is not None and len(group_comparison) > 0:
            group_comparison.to_csv(output_dir / 'group_effects_detailed.csv', index=False)
        
        # å¯¼å‡ºæ¨¡å‹ç»“æœæ‘˜è¦
        model_summaries = []
        for name, model in model_results.items():
            if model is not None:
                summary = {
                    'model_name': name,
                    'r_squared': model.rsquared,
                    'adj_r_squared': model.rsquared_adj,
                    'aic': model.aic,
                    'f_statistic': model.fvalue,
                    'f_pvalue': model.f_pvalue,
                    'n_obs': model.nobs
                }
                model_summaries.append(summary)
        
        if model_summaries:
            pd.DataFrame(model_summaries).to_csv(output_dir / 'robust_models_summary.csv', index=False)
        
        print(f"âœ… ç»„åˆ«åˆ†æç»“æœå·²å¯¼å‡ºåˆ° {output_dir}/")
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

def main():
    """ä¸»åˆ†ææµç¨‹"""
    print("ğŸ”§ ç¨³å¥ç»„åˆ«æ•ˆåº”åˆ†æ")
    print("="*60)
    
    # åŠ è½½æ•°æ®
    try:
        long_table = pd.read_csv('data/processed/human_ratings_long.csv')
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {long_table.shape}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # 1. ç¨³å¥ç»„åˆ«æ•ˆåº”OLSåˆ†æ
    model_results, exp_data = robust_group_effects_ols(long_table)
    
    # 2. è¯¦ç»†ç»„åˆ«æ¯”è¾ƒ
    group_comparison = detailed_group_comparison(long_table)
    
    # 3. å¯è§†åŒ–
    if model_results and exp_data is not None:
        create_group_effects_visualization(group_comparison, exp_data)
    
    # 4. å¯¼å‡ºç»“æœ
    export_group_analysis_results(model_results, group_comparison)
    
    print(f"\n" + "="*60)
    print("ğŸ‰ ç¨³å¥ç»„åˆ«æ•ˆåº”åˆ†æå®Œæˆï¼")
    print("="*60)
    
    print(f"\nğŸ”‘ æ ¸å¿ƒå‘ç°:")
    if len(group_comparison) > 0:
        avg_effect = group_comparison['cohens_d'].mean()
        print(f"   å¹³å‡æ•ˆåº”é‡(Cohen's d): {avg_effect:+.3f}")
        
        significant_configs = group_comparison[np.abs(group_comparison['cohens_d']) > 0.2]
        if len(significant_configs) > 0:
            print(f"   æœ‰æ˜æ˜¾ç»„åˆ«å·®å¼‚çš„é…ç½®: {significant_configs['config'].tolist()}")
        else:
            print(f"   æ‰€æœ‰é…ç½®çš„ç»„åˆ«å·®å¼‚éƒ½å¾ˆå° (|d| < 0.2)")
    
    return model_results, group_comparison, exp_data

if __name__ == "__main__":
    model_results, group_comparison, exp_data = main()
