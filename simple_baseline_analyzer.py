#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€å•çš„baselineæ–‡ä»¶å¤šæ ·æ€§åˆ†æ
"""

import re
import json
from pathlib import Path
import numpy as np

class SimpleBaselineAnalyzer:
    def __init__(self, window=1000, stride=500):
        self.window = window
        self.stride = stride
    
    def tokenize_text(self, text):
        """ç®€å•åˆ†è¯"""
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'#+\s*.*?\n', '', text)  # ç§»é™¤æ ‡é¢˜
        text = re.sub(r'[*_`]', '', text)  # ç§»é™¤markdownæ ¼å¼
        text = re.sub(r'\n+', ' ', text)  # åˆå¹¶æ¢è¡Œ
        
        # ç®€å•åˆ†è¯
        tokens = re.findall(r'\b\w+\b', text.lower())
        return [token for token in tokens if len(token) > 1]
    
    def calculate_distinct_with_window(self, tokens):
        """ä½¿ç”¨æ»‘åŠ¨çª—å£è®¡ç®—distinctåˆ†æ•°"""
        if len(tokens) < 10:
            return 0.0, 0.0
        
        if len(tokens) <= self.window:
            # å¦‚æœæ–‡æœ¬çŸ­äºçª—å£ï¼Œç›´æ¥è®¡ç®—
            uniq_1 = len(set(tokens))
            uniq_2 = len(set(zip(tokens[:-1], tokens[1:]))) if len(tokens) > 1 else 0
            
            d1 = uniq_1 / (len(tokens) + 1)
            d2 = uniq_2 / (max(len(tokens) - 1, 1) + 1)
            return d1, d2
        
        # æ»‘åŠ¨çª—å£è®¡ç®—
        d1_scores = []
        d2_scores = []
        
        # ç¡®ä¿è¦†ç›–å°¾éƒ¨
        last_start = max(0, len(tokens) - self.window)
        starts = list(range(0, len(tokens) - self.window + 1, self.stride))
        if not starts or starts[-1] != last_start:
            starts.append(last_start)
        
        for i in starts:
            window_tokens = tokens[i:i + self.window]
            
            uniq_1 = len(set(window_tokens))
            uniq_2 = len(set(zip(window_tokens[:-1], window_tokens[1:]))) if len(window_tokens) > 1 else 0
            
            d1 = uniq_1 / (len(window_tokens) + 1)
            d2 = uniq_2 / (max(len(window_tokens) - 1, 1) + 1)
            
            d1_scores.append(d1)
            d2_scores.append(d2)
        
        return np.mean(d1_scores), np.mean(d2_scores)
    
    def analyze_file(self, file_path):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ†è¯
        tokens = self.tokenize_text(content)
        
        if len(tokens) < 10:
            return {
                'file_path': str(file_path),
                'total_words': len(tokens),
                'total_sentences': 0,
                'distinct_1': 0.0,
                'distinct_2': 0.0,
                'distinct_avg': 0.0,
                'error': 'Too few tokens'
            }
        
        # è®¡ç®—distinctåˆ†æ•°
        d1, d2 = self.calculate_distinct_with_window(tokens)
        distinct_avg = 0.5 * d1 + 0.5 * d2
        
        # è®¡ç®—å¥å­æ•°ï¼ˆç®€å•ä¼°ç®—ï¼‰
        sentences = len(re.split(r'[.!?]+', content))
        
        return {
            'file_path': str(file_path),
            'total_words': len(tokens),
            'total_sentences': sentences,
            'distinct_1': float(d1),
            'distinct_2': float(d2),
            'distinct_avg': float(distinct_avg)
        }

def analyze_baselines():
    """åˆ†æä¸¤ä¸ªbaselineæ–‡ä»¶"""
    print("=" * 80)
    print("Baselineæ–‡ä»¶å¤šæ ·æ€§åˆ†æ")
    print("=" * 80)
    
    analyzer = SimpleBaselineAnalyzer()
    
    # åˆ†æä¸¤ä¸ªæ–‡ä»¶
    sci_result = analyzer.analyze_file("/Users/haha/Story/data/sci_baseline.md")
    normal_result = analyzer.analyze_file("/Users/haha/Story/data/normal_baseline.md")
    
    print("\nğŸ“Š åŸºæœ¬ç»Ÿè®¡")
    print("-" * 40)
    print(f"ç§‘å¹»baseline: {sci_result['total_words']} è¯, {sci_result['total_sentences']} å¥")
    print(f"ä¼ ç»Ÿbaseline: {normal_result['total_words']} è¯, {normal_result['total_sentences']} å¥")
    
    print("\nğŸ¯ å¤šæ ·æ€§æŒ‡æ ‡å¯¹æ¯”")
    print("-" * 40)
    print(f"ç§‘å¹»baseline:")
    print(f"  distinct_1: {sci_result['distinct_1']:.4f}")
    print(f"  distinct_2: {sci_result['distinct_2']:.4f}")
    print(f"  distinct_avg: {sci_result['distinct_avg']:.4f}")
    
    print(f"ä¼ ç»Ÿbaseline:")
    print(f"  distinct_1: {normal_result['distinct_1']:.4f}")
    print(f"  distinct_2: {normal_result['distinct_2']:.4f}")
    print(f"  distinct_avg: {normal_result['distinct_avg']:.4f}")
    
    print("\nğŸ“ˆ ç›¸å¯¹æ¯”è¾ƒ")
    print("-" * 40)
    
    # è®¡ç®—ç›¸å¯¹å·®å¼‚
    if normal_result['distinct_1'] > 0:
        distinct_1_diff = (sci_result['distinct_1'] - normal_result['distinct_1']) / normal_result['distinct_1'] * 100
        distinct_2_diff = (sci_result['distinct_2'] - normal_result['distinct_2']) / normal_result['distinct_2'] * 100
        distinct_avg_diff = (sci_result['distinct_avg'] - normal_result['distinct_avg']) / normal_result['distinct_avg'] * 100
        
        print(f"ç§‘å¹»ç›¸å¯¹ä¼ ç»Ÿbaseline:")
        print(f"  distinct_1: {distinct_1_diff:+.1f}%")
        print(f"  distinct_2: {distinct_2_diff:+.1f}%")
        print(f"  distinct_avg: {distinct_avg_diff:+.1f}%")
    
    # è¯æ±‡å¯†åº¦æ¯”è¾ƒ
    if sci_result['total_sentences'] > 0 and normal_result['total_sentences'] > 0:
        sci_density = sci_result['total_words'] / sci_result['total_sentences']
        normal_density = normal_result['total_words'] / normal_result['total_sentences']
        density_diff = (sci_density - normal_density) / normal_density * 100
        
        print(f"\nè¯æ±‡å¯†åº¦:")
        print(f"  ç§‘å¹»: {sci_density:.1f} è¯/å¥")
        print(f"  ä¼ ç»Ÿ: {normal_density:.1f} è¯/å¥")
        print(f"  å·®å¼‚: {density_diff:+.1f}%")
    
    print("\nğŸ’¡ å…³é”®å‘ç°")
    print("-" * 40)
    
    if sci_result['distinct_avg'] > normal_result['distinct_avg']:
        print(f"â€¢ ç§‘å¹»baselineè¯æ±‡å¤šæ ·æ€§æ›´é«˜ ({sci_result['distinct_avg']:.4f} vs {normal_result['distinct_avg']:.4f})")
    else:
        print(f"â€¢ ä¼ ç»Ÿbaselineè¯æ±‡å¤šæ ·æ€§æ›´é«˜ ({normal_result['distinct_avg']:.4f} vs {sci_result['distinct_avg']:.4f})")
    
    if sci_result['total_words'] > normal_result['total_words']:
        print(f"â€¢ ç§‘å¹»baselineæ›´é•¿ ({sci_result['total_words']} vs {normal_result['total_words']} è¯)")
    else:
        print(f"â€¢ ä¼ ç»Ÿbaselineæ›´é•¿ ({normal_result['total_words']} vs {sci_result['total_words']} è¯)")
    
    # é£æ ¼åˆ†æ
    print(f"\nğŸ¨ é£æ ¼ç‰¹å¾åˆ†æ")
    print("-" * 40)
    
    # è¯»å–æ–‡ä»¶å†…å®¹è¿›è¡Œç®€å•çš„é£æ ¼åˆ†æ
    with open("/Users/haha/Story/data/sci_baseline.md", 'r', encoding='utf-8') as f:
        sci_content = f.read().lower()
    
    with open("/Users/haha/Story/data/normal_baseline.md", 'r', encoding='utf-8') as f:
        normal_content = f.read().lower()
    
    # ç§‘å¹»è¯æ±‡
    sci_words = ['station', 'algorithm', 'data', 'network', 'drone', 'bio-signal', 'radiation', 'quantum', 'cyber', 'tech']
    sci_count = sum(sci_content.count(word) for word in sci_words)
    
    # ä¼ ç»Ÿè¯æ±‡
    traditional_words = ['forest', 'cottage', 'grandmother', 'wolf', 'path', 'village', 'woods', 'tree', 'flower', 'bird']
    trad_count_in_normal = sum(normal_content.count(word) for word in traditional_words)
    trad_count_in_sci = sum(sci_content.count(word) for word in traditional_words)
    
    print(f"ç§‘å¹»ç‰¹å¾è¯åœ¨ç§‘å¹»baselineä¸­: {sci_count} æ¬¡")
    print(f"ä¼ ç»Ÿç‰¹å¾è¯åœ¨ä¼ ç»Ÿbaselineä¸­: {trad_count_in_normal} æ¬¡")
    print(f"ä¼ ç»Ÿç‰¹å¾è¯åœ¨ç§‘å¹»baselineä¸­: {trad_count_in_sci} æ¬¡")
    
    # ä¿å­˜ç»“æœ
    results = {
        'sci_baseline': sci_result,
        'normal_baseline': normal_result,
        'comparison': {
            'sci_higher_diversity': sci_result['distinct_avg'] > normal_result['distinct_avg'],
            'sci_longer': sci_result['total_words'] > normal_result['total_words'],
            'sci_feature_words': sci_count,
            'traditional_words_in_normal': trad_count_in_normal,
            'traditional_words_in_sci': trad_count_in_sci
        }
    }
    
    with open('baseline_analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: baseline_analysis_results.json")
    print("=" * 80)
    
    return results

if __name__ == "__main__":
    analyze_baselines()
